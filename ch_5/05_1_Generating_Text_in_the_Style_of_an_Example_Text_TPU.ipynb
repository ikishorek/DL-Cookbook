{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.1 Generating Text in the Style of an Example Text - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "t0fqVfPuRO8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "colab 에서 gutenberg 패키지를 사용하기 위해서는, libdb5.3-dev 선행 설치가 필요하다."
      ]
    },
    {
      "metadata": {
        "id": "PGLS_ZOe5Vnf",
        "colab_type": "code",
        "outputId": "440f2daf-f17f-4aca-a8e6-8a22041d0d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "!sudo apt install libdb5.3-dev"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  db5.3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libdb5.3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 762 kB of archives.\n",
            "After this operation, 3,146 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdb5.3-dev amd64 5.3.28-13.1ubuntu1 [762 kB]\n",
            "Fetched 762 kB in 1s (877 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdb5.3-dev.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../libdb5.3-dev_5.3.28-13.1ubuntu1_amd64.deb ...\n",
            "Unpacking libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n",
            "Setting up libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiCi2V2G3bUq",
        "colab_type": "code",
        "outputId": "baf0746c-9586-4d7a-8ce0-8fdc467c3475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install gutenberg"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (40.8.0)\n",
            "Collecting gutenberg\n",
            "  Downloading https://files.pythonhosted.org/packages/14/b1/6e99867c38e70d46366966a0a861c580377f38312cf9dbad38b82ed1823d/Gutenberg-0.7.0.tar.gz\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/fc/ebfbd4de236b493f9ece156f816c21df0ae87ccc22604c5f9b664efef1b9/bsddb3-6.2.6.tar.gz (239kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 22.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (0.16.0)\n",
            "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/a2/bc580a51ac1f9680aa04da4b6e96d499903d6e606d2f78f02e73527799da/rdflib_sqlalchemy-0.3.8-py3-none-any.whl\n",
            "Collecting rdflib>=4.2.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (2.18.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (40.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (1.11.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.1.4 in /usr/local/lib/python3.6/dist-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.18)\n",
            "Collecting alembic>=0.8.8 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bb/ec1e21f2e303689ad2170eb47fc67df9ad4199ade6759a99474c4d3535c8/alembic-1.0.8.tar.gz (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.2.0->gutenberg) (2.3.1)\n",
            "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2.6)\n",
            "Collecting Mako (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f3/67579bb486517c0d49547f9697e36582cd19dafb5df9e687ed8e22de57fa/Mako-1.0.7.tar.gz (564kB)\n",
            "\u001b[K    100% |████████████████████████████████| 573kB 25.8MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3 (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.1.1)\n",
            "Building wheels for collected packages: gutenberg, bsddb3, alembic, Mako\n",
            "  Building wheel for gutenberg (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8e/cd/75/4bc6f16541a1b7a69b02168da567695b2271c23ac4a0a0a453\n",
            "  Building wheel for bsddb3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/11/b8/b3/fa84db10bf8c563e4ba1a72837a0946d123f12adb34b164bf5\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/bc/74/834fa0c75c4ae6d6718db5e65187d508623ee291dead032156\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/35/25/dbcb848832ccb1a4b4ad23f529badfd3bce9bf88017f7ca510\n",
            "Successfully built gutenberg bsddb3 alembic Mako\n",
            "Installing collected packages: bsddb3, isodate, rdflib, Mako, python-editor, alembic, rdflib-sqlalchemy, gutenberg\n",
            "Successfully installed Mako-1.0.7 alembic-1.0.8 bsddb3-6.2.6 gutenberg-0.7.0 isodate-0.6.0 python-editor-1.0.4 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_WCvUA0zQGpd",
        "colab_type": "code",
        "outputId": "a6965658-4bd6-40f1-a58f-5c884c4f1cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dmqnH2hoSjjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "시간 절약을 위해, gutenberg db file 들을 미리 다운로드 받아놓고, google drive 를 mount 하여 연결한다.\n",
        "\n",
        "* gutenberg db file 다운로드: https://drive.google.com/drive/folders/1Y6nMqJ-srDfflTuoniVdEzdWfkrShe4T\n",
        "\n",
        "*  gutenberg db file  기본 경로 : ~/gutenberg_data/metadata/metadata.db\n",
        "\n",
        "*  기학습된 모델 파일 경로 : /gutenberg_data/models/\n",
        "\n",
        "colab 상에서는, /content/gutenberg_data/metadata/metadata.db 에 위치하면 됨"
      ]
    },
    {
      "metadata": {
        "id": "fVrbBx3IQvqa",
        "colab_type": "code",
        "outputId": "9f6a77e5-e9f6-4946-ac83-fc601036fba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lx3_jzJdR-W4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unmount 를 원할 시 실행\n",
        "\n",
        "#!fusermount -u gdrive\n",
        "#!rmdir gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xgwi_TVgTWwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "/content/gdrive 에 google drive 를 mount 하고,\n",
        "\n",
        " 구글 드라이브 상의 /gutenberg/gutenberg_data 를 \n",
        " /content/gutenberg_data 위치로 symbolic link 연결한다\n"
      ]
    },
    {
      "metadata": {
        "id": "gZcDcY0xWPXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/gutenberg/gutenberg_data /content/gutenberg_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36ZYgHb_VjKx",
        "colab_type": "code",
        "outputId": "39af9c98-89b7-43f4-8158-18c10b3f3e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al /content/gutenberg_data/metadata/metadata.db"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3697408\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  contexts\n",
            "-rw------- 1 root root 909377536 Mar  2 03:05 'c^o^s^p^'\n",
            "-rw------- 1 root root 942211072 Mar  2 03:05 'c^p^o^s^'\n",
            "-rw------- 1 root root 915693568 Mar  2 03:06 'c^s^p^o^'\n",
            "-rw------- 1 root root    999424 Mar  2 03:05  __db.001\n",
            "-rw------- 1 root root   1441792 Mar  2 03:06  __db.002\n",
            "-rw------- 1 root root  65544192 Mar  2 03:06  __db.003\n",
            "-rw------- 1 root root 285048832 Mar  2 03:06  i2k\n",
            "-rw------- 1 root root 665780224 Mar  2 03:06  k2i\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  namespace\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  prefix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rU6lpK_33ayM",
        "colab_type": "code",
        "outputId": "683b45f4-964e-49e8-95bf-4e1965535a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    GUTENBERG = True\n",
        "    from gutenberg.acquire import load_etext\n",
        "    from gutenberg.query import get_etexts, get_metadata\n",
        "    from gutenberg.acquire import get_metadata_cache\n",
        "    from gutenberg.acquire.text import UnknownDownloadUriException\n",
        "    from gutenberg.cleanup import strip_headers\n",
        "    from gutenberg._domain_model.exceptions import CacheAlreadyExistsException\n",
        "except ImportError:\n",
        "    GUTENBERG = False\n",
        "    print('Gutenberg is not installed. See instructions at https://pypi.python.org/pypi/Gutenberg')\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import tensorflow.keras.callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "import scipy.misc\n",
        "import json\n",
        "\n",
        "import os, sys\n",
        "import re\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "try:\n",
        "    from io import BytesIO\n",
        "except ImportError:\n",
        "    from StringIO import StringIO as BytesIO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3u4J8SIPTuKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "미리 db 데이터를 넣어 놓아도,  cache.populate() 시 메타 데이타 땡겨오는데 시간이 꽤 걸린다.\n",
        "\n",
        "꼼수가 있는데, 애로 코드블럭을 run 하고 바로 stop 하면 메타데이타(get_metadata) 검색은 잘 안되지만,\n",
        "\n",
        "\n",
        "실제 텍스트 데이터 로드(load_etext) 는 잘 실행된다."
      ]
    },
    {
      "metadata": {
        "id": "fnVGqiNJ3ayP",
        "colab_type": "code",
        "outputId": "cf1a1c36-642d-4553-8f2d-4e48394e96e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1184
        }
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    cache = get_metadata_cache()\n",
        "    try:\n",
        "        cache.populate()\n",
        "    except CacheAlreadyExistsException as e:\n",
        "        print(e)\n",
        "        pass"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-70d9b2c78a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metadata_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCacheAlreadyExistsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gutenberg/acquire/metadata.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_metadata_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_metadata_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gutenberg/acquire/metadata.py\u001b[0m in \u001b[0;36m_iter_metadata_triples\u001b[0;34m(cls, metadata_archive_path)\u001b[0m\n\u001b[1;32m    166\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mdisable_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                         \u001b[0mextracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_is_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rdflib/graph.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, publicID, format, location, file, data, **args)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rdflib/plugins/parsers/rdfxml.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, sink, **args)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;31m# content_handler.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# self._parser.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cont_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetDocumentLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpatLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mxmlreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# bpo-30264: Close the source on error to not leak resources:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/xml/sax/xmlreader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data, isFinal)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# document. When feeding chunks, they are not normally final -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# except when invoked from close.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misFinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAXParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m../Modules/pyexpat.c\u001b[0m in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mend_element_ns\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    370\u001b[0m                                           AttributesNSImpl(newattrs, qnames))\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mend_element_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qz2ZG4om3ayR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    for text_id in get_etexts('author', 'Shakespeare, William'):\n",
        "        print(text_id, list(get_metadata('title', text_id))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ic7JF1RMWpQp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "서적 ID를 기반으로 실제 서적 데이터를 땡겨온다.\n",
        "\n",
        "\n",
        "100 : 세익스피어 전집(희곡)\n",
        "\n",
        "2981 : 카사노바 회고집(에세이)\n",
        "\n",
        "[9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398] : clarissa (여러 편의 편지 형태)\n",
        "\n",
        "\n",
        "학습 텍스트는 원본에서 목차 등 불필요한 텍스트를 발라내게 된다. (text.split 부분)"
      ]
    },
    {
      "metadata": {
        "id": "8n00slgP6YGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shakespeare_id = 100\n",
        "casanova_id = 2981 \n",
        "clarissa_ids = [9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398]\n",
        "\n",
        "def load_etext_from(ids, filter_func):\n",
        "  etext = '\\n'.join([filter_func(strip_headers(load_etext(id))) \\\n",
        "                     for id in ids])\n",
        "  return etext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h43THvuk-F84",
        "colab_type": "code",
        "outputId": "b8fbcb01-1d7f-4e18-ff69-8c21aed83274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "cell_type": "code",
      "source": [
        "shakespeare = load_etext_from([shakespeare_id], lambda text: text.split('\\nTHE END', 1)[-1])\n",
        "print(len(shakespeare))\n",
        "shakespeare[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5528070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\nALL’S WELL THAT ENDS WELL\\n\\n\\nby William Shakespeare\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Rossillon. A room in the Countess’s palace.\\nScene II. Paris. A room in the King’s palace.\\nScene III. Rossillon. A Room in the Palace.\\n\\n\\nACT II\\nScene I. Paris. A room in the King’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Paris. The King’s palace.\\nScene IV. Paris. The King’s palace.\\nScene V. Another room in the same.\\n\\n\\nACT III\\nScene I. Florence. A room in the Duke’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Florence. Before the Duke’s palace.\\nScene IV. Rossillon. A room in the Countess’s palace.\\nScene V. Without the walls of Florence.\\nScene VI. Camp before Florence.\\nScene VII. Florence. A room in the Widow’s house.\\n\\n\\nACT IV\\nScene I. Without the Florentine camp.\\nScene II. Florence. A room in the Widow’s house.\\nScene III. The Florentine camp.\\nScene IV. Florence. A room in the Widow’s house.\\nScene V. Rossillon. A room in the Countess’s palace.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "nvAMCBz1Ci9Q",
        "colab_type": "code",
        "outputId": "69eefbeb-129c-4f38-963e-79147d30df20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "cell_type": "code",
      "source": [
        "casanova = load_etext_from([casanova_id], lambda text: text.split('\\nCASANOVA AT DUX', 1)[-1]) # from main contents\n",
        "print(len(casanova))\n",
        "casanova[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6685264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n An Unpublished Chapter of History, By Arthur Symons\\n\\n I\\n The Memoirs of Casanova, though they have enjoyed the popularity of a bad reputation, have never had justice done to them by serious students of literature, of life, and of history. One English writer, indeed, Mr. Havelock Ellis, has realised that ‘there are few more delightful books in the world,’ and he has analysed them in an essay on Casanova, published in Affirmations, with extreme care and remarkable subtlety. But this essay stands alone, at all events in English, as an attempt to take Casanova seriously, to show him in his relation to his time, and in his relation to human problems. And yet these Memoirs are perhaps the most valuable document which we possess on the society of the eighteenth century; they are the history of a unique life, a unique personality, one of the greatest of autobiographies; as a record of adventures, they are more entertaining than Gil Blas, or Monte Cristo, or any of the imaginary travels, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Zbur6dJQCjH6",
        "colab_type": "code",
        "outputId": "6abf4a91-ec03-4128-eead-37b2e4788e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "cell_type": "code",
      "source": [
        "clarissa = load_etext_from(clarissa_ids, lambda text: text.split('\\nTHE HISTORY OF CLARISSA HARLOWE', 1)[-1]) # from main contents\n",
        "print(len(clarissa))\n",
        "clarissa[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5173348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\nLETTER I\\n\\nMISS ANNA HOWE, TO MISS CLARISSA HARLOWE JAN 10.\\n\\n\\nI am extremely concerned, my dearest friend, for the disturbances that\\nhave happened in your family. I know how it must hurt you to become\\nthe subject of the public talk: and yet, upon an occasion so generally\\nknown, it is impossible but that whatever relates to a young lady, whose\\ndistinguished merits have made her the public care, should engage every\\nbody's attention. I long to have the particulars from yourself; and of\\nthe usage I am told you receive upon an accident you could not help; and\\nin which, as far as I can learn, the sufferer was the aggressor.\\n\\nMr. Diggs, the surgeon, whom I sent for at the first hearing of the\\nrencounter, to inquire, for your sake, how your brother was, told me,\\nthat there was no danger from the wound, if there were none from the\\nfever; which it seems has been increased by the perturbation of his\\nspirits.\\n\\nMr. Wyerley drank tea with us yesterday; and though he is far from being\\npartial to \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "jhMW0xeVg2G9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_global_chars_index(etexts):\n",
        "  chars = list(sorted(set(\"\\n\".join(etexts))))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "gchar, gchar_to_idx = get_global_chars_index([shakespeare,casanova,clarissa])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2smFtvyOXVtS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "모델 저장에 쓸 파일명, char 임베딩 목록, chunk_size 등 메타데이타를 dict 로 묶어둔다."
      ]
    },
    {
      "metadata": {
        "id": "s0nqz37H3ayW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_chars_index(etext):\n",
        "  chars = list(sorted(set(etext)))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "def generate_meta_from(etext, model_name, chunk_size=160):\n",
        "  etext_meta = {}\n",
        "  etext_meta['model_name'] = model_name\n",
        "  etext_meta['char'], etext_meta['char_to_idx'] = gchar, gchar_to_idx\n",
        "  etext_meta['chunk_size'] = chunk_size\n",
        "  return etext_meta\n",
        "\n",
        "\n",
        "shakespeare_meta = generate_meta_from(shakespeare, 'shakespeare')\n",
        "casanova_meta = generate_meta_from(casanova, 'casanova')\n",
        "clarissa_meta = generate_meta_from(clarissa, 'clarissa')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YwM_iyMCXjv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "메서드 순서대로\n",
        "\n",
        "1. 텍스트 문체 흉내를 위한 rnn 모델 \n",
        "\n",
        "2. TPU 전용 모델 컨버팅 (tf.contrib.tpu.keras_to_tpu_model)\n",
        "\n",
        "3. chunk_size 별 임베딩 텍스트를 짤라주는 메서드\n",
        "\n",
        "이다. TPU 지원을 위해서 keras 기반 클래스나 메서드 등을 tf.keras 로 변환할 필요가 있다.\n",
        "\n",
        "(초기 import 문에도 관련 패키치명 수정이 있음)"
      ]
    },
    {
      "metadata": {
        "id": "pIG2LwgW3ayY",
        "colab_type": "code",
        "outputId": "ec5df6ba-bc23-4e42-f019-07795b45a8b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# TPU 사용하려면 input shape 가 static 으로 지정 되어 잇어야 함(None 값 설정 불가)\n",
        "def char_rnn_model(chunk_size, num_chars, num_layers, num_nodes=512, dropout=0.1):\n",
        "    input = Input(shape=(chunk_size, num_chars), name='input')\n",
        "    prev = input\n",
        "    for i in range(num_layers):\n",
        "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
        "        if dropout:\n",
        "            prev = Dropout(dropout)(lstm)\n",
        "        else:\n",
        "            prev = lstm\n",
        "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
        "    model = Model(inputs=[input], outputs=[dense])\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_tpu_model(compiled_model, tpu_address):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(compiled_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)))\n",
        "  \n",
        "  \n",
        "def data_generator(all_text, char_to_idx, batch_size, chunk_size):\n",
        "    X = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
        "    y = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
        "    while True:\n",
        "        for row in range(batch_size):\n",
        "            idx = random.randrange(len(all_text) - chunk_size - 1)\n",
        "            chunk = np.zeros((chunk_size + 1, len(char_to_idx)))\n",
        "            for i in range(chunk_size + 1):\n",
        "                chunk[i, char_to_idx[all_text[idx + i]]] = 1\n",
        "            X[row, :, :] = chunk[:chunk_size]\n",
        "            y[row, :, :] = chunk[1:]\n",
        "        yield X, y\n",
        "\n",
        "#next(data_generator(training_text, char_to_idx, 4, chunk_size=CHUNK_SIZE))  \n",
        "\n",
        "# TPU 관련 설정\n",
        "try:\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\n",
        "  TPU_ADDRESS = 'grpc://' + device_name\n",
        "  print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "except KeyError:\n",
        "  print('TPU not found')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at: grpc://10.6.87.10:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lma2zbdWYkjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TPU 모델을 생성하는 시점에 TPUClusterResolver 가 사용가능 TPU 들을 스캔하여 보여준다.\n",
        "\n",
        "특이 사항은, TPU 를 이용한 학습을 위해서는 데이터 총량을 제외한 모든 dimension 이 static 이어야 한다.\n",
        "\n",
        "그래서 chunk_size 로 명시 지정하여 학습 및 weight 저장 후, 로드 시 \n",
        "(inference 버젼 참고)\n",
        "\n",
        "\n",
        "예시)\n",
        "\n",
        "순수 keras  : input (InputLayer)           (None, None, 98) \n",
        "\n",
        "tf TPU 구동가능 버젼  : input (InputLayer)           (None, 160, 98) -> 여기서 160 은 chunk size"
      ]
    },
    {
      "metadata": {
        "id": "qzcioWym3ayb",
        "colab_type": "code",
        "outputId": "a0ba3d1a-9173-4e53-c6f4-0bc61dbf4a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1720
        }
      },
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 160\n",
        "\n",
        "shakespeare_model = get_tpu_model(char_rnn_model(shakespeare_meta['chunk_size'], len(shakespeare_meta['char']), num_layers=2, num_nodes=640, dropout=0), TPU_ADDRESS)\n",
        "casanova_model = get_tpu_model(char_rnn_model(casanova_meta['chunk_size'], len(casanova_meta['char']), num_layers=2, num_nodes=640, dropout=0), TPU_ADDRESS)\n",
        "clarissa_model = get_tpu_model(char_rnn_model(clarissa_meta['chunk_size'], len(clarissa_meta['char']), num_layers=2, num_nodes=640, dropout=0), TPU_ADDRESS)\n",
        "\n",
        "print(shakespeare_model.summary())\n",
        "print(casanova_model.summary())\n",
        "print(clarissa_model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.53.174.26:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16506049000455623698)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14863639230791593516)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9978608036878501689)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 666614101093242485)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 810507544065214660)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16604396502633453459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16866579200758548712)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13221784050982576097)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4994108979603102006)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7505486045209192366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 440577656874674054)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.53.174.26:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16506049000455623698)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14863639230791593516)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9978608036878501689)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 666614101093242485)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 810507544065214660)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16604396502633453459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16866579200758548712)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13221784050982576097)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4994108979603102006)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7505486045209192366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 440577656874674054)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.53.174.26:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16506049000455623698)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14863639230791593516)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9978608036878501689)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 666614101093242485)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 810507544065214660)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16604396502633453459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16866579200758548712)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13221784050982576097)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4994108979603102006)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7505486045209192366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 440577656874674054)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 106)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1912320   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 160, 106)          67946     \n",
            "=================================================================\n",
            "Total params: 5,259,626\n",
            "Trainable params: 5,259,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 106)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1912320   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 160, 106)          67946     \n",
            "=================================================================\n",
            "Total params: 5,259,626\n",
            "Trainable params: 5,259,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 106)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1912320   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 160, 106)          67946     \n",
            "=================================================================\n",
            "Total params: 5,259,626\n",
            "Trainable params: 5,259,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cB1nx-iPY8rd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "EarlyStopping 의 경우 특정 조건이 발생하면 학습을 조기종료한다.\n",
        "\n",
        "아래의 경우 loss 가 min_delta 보다 적게 감소하는 경우가 10(patience) epoch 회 이상이면 학습을 조기 종료한다.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OfEsY1yS3ayf",
        "colab_type": "code",
        "outputId": "8850137a-2432-44af-f638-fd7987af16f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1550
        }
      },
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=10,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "clarissa_model.fit_generator(\n",
        "    data_generator(clarissa, clarissa_meta['char_to_idx'], batch_size=BATCH_SIZE, chunk_size=clarissa_meta['chunk_size']),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(clarissa) / (BATCH_SIZE * CHUNK_SIZE)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(32, 160, 106), dtype=tf.float32, name='input_10'), TensorSpec(shape=(32, 160, 106), dtype=tf.float32, name='time_distributed_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.616089105606079 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 116s - loss: 3.6432 - acc: 0.1455\n",
            "Epoch 2/40\n",
            " - 98s - loss: 3.2029 - acc: 0.1509\n",
            "Epoch 3/40\n",
            " - 98s - loss: 3.1497 - acc: 0.1682\n",
            "Epoch 4/40\n",
            " - 98s - loss: 3.1714 - acc: 0.1663\n",
            "Epoch 5/40\n",
            " - 99s - loss: 2.2526 - acc: 0.3758\n",
            "Epoch 6/40\n",
            " - 97s - loss: 1.4917 - acc: 0.5800\n",
            "Epoch 7/40\n",
            " - 98s - loss: 1.3678 - acc: 0.6120\n",
            "Epoch 8/40\n",
            " - 99s - loss: 1.3495 - acc: 0.6217\n",
            "Epoch 9/40\n",
            " - 99s - loss: 1.3178 - acc: 0.6315\n",
            "Epoch 10/40\n",
            " - 99s - loss: 1.2811 - acc: 0.6414\n",
            "Epoch 11/40\n",
            " - 98s - loss: 1.2359 - acc: 0.6512\n",
            "Epoch 12/40\n",
            " - 100s - loss: 1.2306 - acc: 0.6554\n",
            "Epoch 13/40\n",
            " - 99s - loss: 1.1990 - acc: 0.6630\n",
            "Epoch 14/40\n",
            " - 99s - loss: 1.2048 - acc: 0.6648\n",
            "Epoch 15/40\n",
            " - 98s - loss: 1.2033 - acc: 0.6668\n",
            "Epoch 16/40\n",
            " - 98s - loss: 1.1693 - acc: 0.6738\n",
            "Epoch 17/40\n",
            " - 98s - loss: 1.1613 - acc: 0.6776\n",
            "Epoch 18/40\n",
            " - 99s - loss: 1.1527 - acc: 0.6803\n",
            "Epoch 19/40\n",
            " - 99s - loss: 1.1471 - acc: 0.6829\n",
            "Epoch 20/40\n",
            " - 99s - loss: 1.1627 - acc: 0.6822\n",
            "Epoch 21/40\n",
            " - 99s - loss: 1.1204 - acc: 0.6898\n",
            "Epoch 22/40\n",
            " - 98s - loss: 1.1315 - acc: 0.6904\n",
            "Epoch 23/40\n",
            " - 99s - loss: 1.0899 - acc: 0.6974\n",
            "Epoch 24/40\n",
            " - 99s - loss: 1.1037 - acc: 0.6968\n",
            "Epoch 25/40\n",
            " - 100s - loss: 1.0932 - acc: 0.7008\n",
            "Epoch 26/40\n",
            " - 99s - loss: 1.0958 - acc: 0.7008\n",
            "Epoch 27/40\n",
            " - 97s - loss: 1.0870 - acc: 0.7038\n",
            "Epoch 28/40\n",
            " - 100s - loss: 1.0463 - acc: 0.7109\n",
            "Epoch 29/40\n",
            " - 98s - loss: 1.0713 - acc: 0.7090\n",
            "Epoch 30/40\n",
            " - 99s - loss: 1.0726 - acc: 0.7100\n",
            "Epoch 31/40\n",
            " - 98s - loss: 1.0754 - acc: 0.7110\n",
            "Epoch 32/40\n",
            " - 99s - loss: 1.0453 - acc: 0.7158\n",
            "Epoch 33/40\n",
            " - 98s - loss: 1.0443 - acc: 0.7177\n",
            "Epoch 34/40\n",
            " - 99s - loss: 1.0610 - acc: 0.7168\n",
            "Epoch 35/40\n",
            " - 99s - loss: 1.0548 - acc: 0.7180\n",
            "Epoch 36/40\n",
            " - 100s - loss: 1.0296 - acc: 0.7230\n",
            "Epoch 37/40\n",
            " - 98s - loss: 1.0348 - acc: 0.7237\n",
            "Epoch 38/40\n",
            " - 98s - loss: 1.0411 - acc: 0.7230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1a578f4ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "vDRdW2YNeclq",
        "colab_type": "code",
        "outputId": "3f180809-328e-486e-90ed-3114111b32f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1476
        }
      },
      "cell_type": "code",
      "source": [
        "shakespeare_model.fit_generator(\n",
        "    data_generator(shakespeare, shakespeare_meta['char_to_idx'], batch_size=BATCH_SIZE, chunk_size=shakespeare_meta['chunk_size']),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(shakespeare) / (BATCH_SIZE * CHUNK_SIZE)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " - 105s - loss: 1.0684 - acc: 0.7115\n",
            "Epoch 2/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-75b101b04fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshakespeare\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mCHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m     ], infeed_dict)\n\u001b[0m\u001b[1;32m   1270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wPSSkKZ_ecyu",
        "colab_type": "code",
        "outputId": "98c03312-e3a1-435b-9a3e-1c47a58415ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "cell_type": "code",
      "source": [
        "casanova_model.fit_generator(\n",
        "    data_generator(casanova, casanova_meta['char_to_idx'], batch_size=BATCH_SIZE, chunk_size=casanova_meta['chunk_size']),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(casanova) / (BATCH_SIZE * CHUNK_SIZE)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " - 132s - loss: 3.4652 - acc: 0.1735\n",
            "Epoch 2/40\n",
            " - 126s - loss: 3.0848 - acc: 0.1828\n",
            "Epoch 3/40\n",
            " - 126s - loss: 1.8494 - acc: 0.4757\n",
            "Epoch 4/40\n",
            " - 126s - loss: 1.3283 - acc: 0.6224\n",
            "Epoch 5/40\n",
            " - 127s - loss: 1.2681 - acc: 0.6442\n",
            "Epoch 6/40\n",
            " - 127s - loss: 1.2187 - acc: 0.6578\n",
            "Epoch 7/40\n",
            " - 127s - loss: 1.2266 - acc: 0.6613\n",
            "Epoch 8/40\n",
            " - 126s - loss: 1.1650 - acc: 0.6736\n",
            "Epoch 9/40\n",
            " - 126s - loss: 1.1401 - acc: 0.6800\n",
            "Epoch 10/40\n",
            " - 126s - loss: 1.1465 - acc: 0.6818\n",
            "Epoch 11/40\n",
            " - 126s - loss: 1.1300 - acc: 0.6864\n",
            "Epoch 12/40\n",
            " - 128s - loss: 1.1075 - acc: 0.6919\n",
            "Epoch 13/40\n",
            " - 126s - loss: 1.1195 - acc: 0.6922\n",
            "Epoch 14/40\n",
            " - 126s - loss: 1.1057 - acc: 0.6953\n",
            "Epoch 15/40\n",
            " - 126s - loss: 1.0775 - acc: 0.7008\n",
            "Epoch 16/40\n",
            " - 127s - loss: 1.0899 - acc: 0.7007\n",
            "Epoch 17/40\n",
            " - 126s - loss: 1.0699 - acc: 0.7048\n",
            "Epoch 18/40\n",
            " - 126s - loss: 1.0823 - acc: 0.7051\n",
            "Epoch 19/40\n",
            " - 126s - loss: 1.0915 - acc: 0.7047\n",
            "Epoch 20/40\n",
            " - 126s - loss: 1.0655 - acc: 0.7095\n",
            "Epoch 21/40\n",
            " - 128s - loss: 1.0686 - acc: 0.7102\n",
            "Epoch 22/40\n",
            " - 126s - loss: 1.0417 - acc: 0.7150\n",
            "Epoch 23/40\n",
            " - 127s - loss: 1.0551 - acc: 0.7141\n",
            "Epoch 24/40\n",
            " - 127s - loss: 1.0333 - acc: 0.7184\n",
            "Epoch 25/40\n",
            " - 126s - loss: 1.0292 - acc: 0.7206\n",
            "Epoch 26/40\n",
            " - 127s - loss: 1.0264 - acc: 0.7218\n",
            "Epoch 27/40\n",
            " - 127s - loss: 1.0064 - acc: 0.7254\n",
            "Epoch 28/40\n",
            " - 126s - loss: 1.0375 - acc: 0.7222\n",
            "Epoch 29/40\n",
            " - 125s - loss: 1.0008 - acc: 0.7278\n",
            "Epoch 30/40\n",
            " - 127s - loss: 1.0177 - acc: 0.7265\n",
            "Epoch 31/40\n",
            " - 127s - loss: 1.0193 - acc: 0.7273\n",
            "Epoch 32/40\n",
            " - 127s - loss: 1.0041 - acc: 0.7295\n",
            "Epoch 33/40\n",
            " - 128s - loss: 1.0209 - acc: 0.7288\n",
            "Epoch 34/40\n",
            " - 127s - loss: 1.0034 - acc: 0.7318\n",
            "Epoch 35/40\n",
            " - 127s - loss: 0.9803 - acc: 0.7355\n",
            "Epoch 36/40\n",
            " - 127s - loss: 0.9715 - acc: 0.7381\n",
            "Epoch 37/40\n",
            " - 128s - loss: 1.0042 - acc: 0.7339\n",
            "Epoch 38/40\n",
            " - 127s - loss: 0.9770 - acc: 0.7382\n",
            "Epoch 39/40\n",
            " - 126s - loss: 0.9924 - acc: 0.7369\n",
            "Epoch 40/40\n",
            " - 126s - loss: 0.9637 - acc: 0.7417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1a448fd9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "jwHq27ck3ayi",
        "colab_type": "code",
        "outputId": "85ec4513-605d-4f76-93f8-ec0dcabd7056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "def save_model(basepath, model, model_meta):\n",
        "  base_file_path = basepath + '/' + model_meta['model_name']\n",
        "  with open(base_file_path + '.json', 'w') as fout:\n",
        "      json.dump({\n",
        "          'chars': ''.join(model_meta['char']),\n",
        "          'char_to_idx': model_meta['char_to_idx'] ,\n",
        "          'chunk_size': CHUNK_SIZE,\n",
        "      }, fout)\n",
        "  model.save(base_file_path +'.h5')\n",
        "  model.save_weights(base_file_path + '_weights.h5')\n",
        "  \n",
        "basepath = '/content/gutenberg_data/models/rel/'\n",
        "\n",
        "save_model(basepath, clarissa_model, clarissa_meta)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j07es-FnhLlt",
        "colab_type": "code",
        "outputId": "29901927-9af7-462b-fd9a-6f5a69dc6f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "save_model(basepath, shakespeare_model, shakespeare_meta)\n",
        "save_model(basepath, casanova_model, casanova_meta)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gWAzFaQo2Ywz",
        "colab_type": "code",
        "outputId": "d75db027-1965-4797-8a0b-a471481c1169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al /content/gutenberg_data/models/rel"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 123384\n",
            "-rw------- 1 root root 21057072 Mar 13 07:57 casanova.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 casanova.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:57 casanova_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 04:23 clarissa.h5\n",
            "-rw------- 1 root root     1293 Mar 13 04:23 clarissa.json\n",
            "-rw------- 1 root root 21055728 Mar 13 04:24 clarissa_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 07:56 shakespeare.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 shakespeare.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:56 shakespeare_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8YXsk5bki9qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "고정 chunk_size 로 학습후 모델의 weight을 저장한다.\n",
        "\n",
        "(mount 된 google drive 내부(/content/gutenberg_data/ 하위)에 저장하여 다른 notebook 에서 활용 가능하도록 하였다.)\n",
        "\n",
        "그 후 모델을 load 할 시 chunk_size=None 로 새로운 rnn 을 생성 후 load_weights 로 weight 만 엎어치는 방식을 사용 하였다."
      ]
    },
    {
      "metadata": {
        "id": "33BNza2wwZHX",
        "colab_type": "code",
        "outputId": "e6b3001d-e42b-44b8-daa3-9d4004d16e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# prediction from saved weights\n",
        "\n",
        "def load_model(basepath, model_meta):\n",
        "  model = char_rnn_model(None, len(model_meta['char']), num_layers=2, num_nodes=640, dropout=0) \n",
        "  model.load_weights(basepath + \"\" + model_meta['model_name'] + '_weights.h5')\n",
        "  return model\n",
        "  \n",
        "\n",
        "prediction_model = load_model(basepath, clarissa_meta)\n",
        "prediction_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 106)         0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1912320   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, None, 106)         67946     \n",
            "=================================================================\n",
            "Total params: 5,259,626\n",
            "Trainable params: 5,259,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cwp7eIu-iso1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "clarissa 모델로 diversity 만 변경하여 문장을 한번씩 생성해 보았다.\n",
        "\n",
        "하기 결과에는 눈에 띄진 않으나 실험적으로는,\n",
        "\n",
        "diversity 가 1에 가까울수록 다양하면서 아무말(과 철자 오류) 하는 느낌이고\n",
        "diversity=None 일 경우 좀 정돈된 말을 하지만 비슷비슷한 말을 반복하는 경향이 있다.\n",
        "\n",
        "amount 를 크게 잡으면(문장을 길게 생성하면) 눈에 보이게 된다.\n"
      ]
    },
    {
      "metadata": {
        "id": "2ffQ8tgF3ayj",
        "colab_type": "code",
        "outputId": "bae9ee6b-c42f-425c-9cf5-99c88571f9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_output(model, training_text, model_meta, start_index=None, diversity=None, amount=400):\n",
        "    if start_index is None:\n",
        "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
        "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
        "    yield generated + '#'\n",
        "    for i in range(amount):\n",
        "        x = np.zeros((1, len(generated), len(model_meta['char'])))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, model_meta['char_to_idx'][char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        if diversity is None:\n",
        "            next_index = np.argmax(preds[len(generated) - 1])\n",
        "        else:\n",
        "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            next_index = np.argmax(probas)     \n",
        "        next_char = model_meta['char'][next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "    return generated\n",
        "\n",
        "#for ch in generate_output(sp_prediction_model, training_text):\n",
        "for ch in generate_output(prediction_model, clarissa, model_meta=clarissa_meta, diversity=0.5, amount=1500):\n",
        "    sys.stdout.write(ch)\n",
        "print()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ght have been attended with bad consequences,) no\n",
            "two brothers have a more cordial esteem for each other.  You know, Mr.\n",
            "Lovelace, that there is a consent, as I# may call it, on the contents of this\n",
            "dear creature, and the company of a domestic nature.  What a figure would the\n",
            "surmounted day before him!  The subject is a very vile person of her fortune.  On the\n",
            "contrary, she seemed to be a very wicked fellow, and a sudden flower and affectionate\n",
            "heart.\n",
            "\n",
            "Having disordered the man who had the honour to know the family, and to find\n",
            "him out, and made him set out in her company, and the more respectfully, and who had\n",
            "more difficult to regulate his motions by her parents.  And, as to\n",
            "mo, of a spirit so proper, and shall be my choice.  I will deposit this\n",
            "and unfriendly interest and apprehension that my love for her not\n",
            "lest favour to her.\n",
            "\n",
            "To the fellow has drawn her out of the course of this day's visit in fear of me; and that\n",
            "the result was this: 'that he could not help thinking the steels of the world, and to\n",
            "a pleasant place for your side of the question: and that will be sometimes\n",
            "that you are not to be expressed in it; I cannot but say.\n",
            "\n",
            "We approach you, Madam.  But it is my opinion, that the fellows are set upon you, will or\n",
            "not.\n",
            "\n",
            "I shall see this morning, as if it were any thing in the woman's justice.\n",
            "\n",
            "There is a sort of a fair will and politeness; but it is not worth while to give me a more\n",
            "particular inquiry. I have no suspicion that I am convinced, if she had not some\n",
            "heart of her dealing for all that must be my consent to me.\n",
            "\n",
            "All the manner of my wig and my sister's measures to renew their minds to my last, that I may not\n",
            "be s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrsOPjPcy1R_",
        "colab_type": "code",
        "outputId": "e8d63a98-af18-4760-dda7-7b2992bcc05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_model, clarissa, model_meta=clarissa_meta, amount=1500):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The doctor\n",
            "resolves to write to her father.  Her intense, yet cheerful devotion.\n",
            "\n",
            "LETTER LV.  Clarissa to Miss Howe.--\n",
            "A letter full of pious reflections, and g#lorying in them: and then she spoke\n",
            "this with so much spirit as I have set up an inclination to receive bendford from me.  I was\n",
            "afraid that her own mother would be glad to have the world to have the\n",
            "least thought of marrying in the manner I have so soon after for. But why should I be thought as\n",
            "freely as I have done?--Yet she could not but say, that if I were to be the more\n",
            "considerable that he was not a little too much to be displeased with my friends,\n",
            "will be all the miserable provise that were to be met with at the time.  And when I had\n",
            "done, that I was not able to preserve the first parents of a lady so much to be\n",
            "provided for an expedient which they were so set as to find him out.  They had\n",
            "sent for them to the farthest part of my cousin's letters.  I was silent.\n",
            "I was still silent.\n",
            "\n",
            "Tell me not of poor Belton.  I have no doubt that the settlements are so much afraid\n",
            "of the execution of the world as Mrs. Smith and his wife.  I think it is in the\n",
            "principal end.\n",
            "\n",
            "So down she flung.  I shall not be able to account for in a manner despondent with\n",
            "her.\n",
            "\n",
            "I wish you may, Madam, to consider what is much: you have not been used to call a horse,\n",
            "may be the better for the favours you have declared of me.\n",
            "\n",
            "I shall send this short letter to you; and that you will be pleased to mark\n",
            "that, because I have not suffered in the world to meet with a man who has not a friend in the world that\n",
            "thou mayest observe for him than the more for the faults of others of the ladies of his\n",
            "family, and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "myqaQTHNfauW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "파이썬 코드를 학습시켜 코드를 짜는 rnn 을 구현한 내용이다.\n",
        "\n",
        ".py 확장자의 코드를 전체 풀스캔하고 주석(#) 를 삭제하여 학습 데이터화한다."
      ]
    },
    {
      "metadata": {
        "id": "v3bn5bTy3aym",
        "colab_type": "code",
        "outputId": "316efee4-ac9b-4f9a-a63e-ab3db95fac14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def find_python(rootdir):\n",
        "    matches = []\n",
        "    for root, dirnames, filenames in os.walk(rootdir):\n",
        "        for fn in filenames:\n",
        "            if fn.endswith('.py'):\n",
        "                matches.append(os.path.join(root, fn))\n",
        "\n",
        "    return matches \n",
        "\n",
        "srcs = find_python(random.__file__.rsplit('/', 1)[0])\n",
        "srcs += find_python(os.path.join(sys.executable.rsplit('/', 2)[0], 'lib'))\n",
        "len(srcs)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2093"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "zYoPHqHY3ayp",
        "colab_type": "code",
        "outputId": "12c4cd62-5892-4741-c1e7-66ee2d336f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def replacer(value):\n",
        "    value = ''.join(ch for ch in value if ord(ch) < 127)\n",
        "    if not ' ' in value:\n",
        "        return value\n",
        "    if sum(1 for ch in value if ch.isalpha()) > 6:\n",
        "        return 'MSG'\n",
        "    return value\n",
        "\n",
        "\n",
        "def replace_literals(st):\n",
        "    res = []\n",
        "    start_text = start_quote = i = 0\n",
        "    quote = ''\n",
        "    while i < len(st):\n",
        "        if quote:\n",
        "            if st[i: i + len(quote)] == quote:\n",
        "                quote = ''\n",
        "                start_text = i\n",
        "                res.append(replacer(st[start_quote: i]))\n",
        "        elif st[i] in '\"\\'':\n",
        "            quote = st[i]\n",
        "            if i < len(st) - 2 and st[i + 1] == st[i + 2] == quote:\n",
        "                quote = 3 * quote\n",
        "            start_quote = i + len(quote)\n",
        "            res.append(st[start_text: start_quote])\n",
        "        if st[i] == '\\n' and len(quote) == 1:\n",
        "            start_text = i\n",
        "            res.append(quote)\n",
        "            quote = ''\n",
        "        if st[i] == '\\\\':\n",
        "            i += 1\n",
        "        i += 1\n",
        "    return ''.join(res) + st[start_text:]\n",
        "\n",
        "#replace_literals('print(\"hel\\\\\"lo\")') + replace_literals(\"print('hel\\\\'lo world')\")\n",
        "replace_literals('this = \"wrong\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this = \"\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "CWAhUnZm3ayr",
        "colab_type": "code",
        "outputId": "581edd7c-26ef-4128-9775-b66da1cd9b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "COMMENT_RE = re.compile('#.*')\n",
        "python_code = []\n",
        "for fn in srcs:\n",
        "    try:\n",
        "        with open(fn, 'r') as fin:\n",
        "            src = fin.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print('Could not read %s' % fn)\n",
        "    src = replace_literals(src)\n",
        "    src = COMMENT_RE.sub('', src)\n",
        "    python_code.append(src)\n",
        "\n",
        "python_code = '\\n\\n\\n'.join(python_code)\n",
        "len(python_code)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not read /usr/lib/python2.7/shlex.py\n",
            "Could not read /usr/lib/python2.7/heapq.py\n",
            "Could not read /usr/lib/python2.7/tarfile.py\n",
            "Could not read /usr/lib/python2.7/distutils/command/bdist_msi.py\n",
            "Could not read /usr/lib/python2.7/sqlite3/dbapi2.py\n",
            "Could not read /usr/lib/python2.7/sqlite3/__init__.py\n",
            "Could not read /usr/lib/python2.7/encodings/punycode.py\n",
            "Could not read /usr/lib/python2.7/encodings/string_escape.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19837429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "tyi1IXsO3ayv",
        "colab_type": "code",
        "outputId": "a3b7f65b-8667-42ad-fbef-bc591a6dd22e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "py_chars = list(sorted(set(python_code)))\n",
        "py_char_to_idx = {ch: idx for idx, ch in enumerate(py_chars)}\n",
        "len(py_chars)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "nA6VanAV3ayz",
        "colab_type": "code",
        "outputId": "970ed633-4a4f-4d32-b157-0c32504665c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = char_rnn_model(160, len(py_chars), num_layers=2, num_nodes=640, dropout=0)\n",
        "py_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 97)           0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 160, 97)           62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8WaPmJK3NCcl",
        "colab_type": "code",
        "outputId": "d97142be-b907-4674-b334-7f79cae5901b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = tf.contrib.tpu.keras_to_tpu_model(py_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.6.87.10:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 9809245123808874855)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11472922592050379328)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8280241242590232291)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 17167163830427851441)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12141304152876421246)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3768097874428642633)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 10495415504762893544)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 17877701681081841105)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16853638606168624587)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 2915169999020945906)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 11240999428394713033)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TtdXhdIk3ay2",
        "colab_type": "code",
        "outputId": "73c26882-bb90-4bea-fe03-39f7d2eb1ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "early = tensorflow.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=7,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "py_model.fit_generator(\n",
        "    data_generator(python_code, py_char_to_idx, batch_size=BATCH_SIZE, chunk_size=160),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(python_code) / (BATCH_SIZE * 160)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(32, 160, 97), dtype=tf.float32, name='input_50'), TensorSpec(shape=(32, 160, 97), dtype=tf.float32, name='time_distributed_2_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.185345649719238 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 393s - loss: 3.0369 - acc: 0.3639\n",
            "Epoch 2/40\n",
            " - 363s - loss: 0.9788 - acc: 0.7623\n",
            "Epoch 3/40\n",
            " - 363s - loss: 0.8148 - acc: 0.8092\n",
            "Epoch 4/40\n",
            " - 363s - loss: 0.7529 - acc: 0.8265\n",
            "Epoch 5/40\n",
            " - 364s - loss: 0.7306 - acc: 0.8348\n",
            "Epoch 6/40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xslhrK9sUKPM",
        "colab_type": "code",
        "outputId": "b3916e4c-d134-4627-844c-13a4efd4f592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gutenberg_data/models/rel/py_code.json', 'w') as fout:\n",
        "    json.dump({\n",
        "        'chars': ''.join(py_chars),\n",
        "        'char_to_idx': py_char_to_idx,\n",
        "        'chunk_size': 160,\n",
        "    }, fout)\n",
        "py_model.save('/content/gutenberg_data/models/rel/py_code.h5')\n",
        "py_model.save_weights('/content/gutenberg_data/models/rel/py_code_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cu4hlM_TgFXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "상기 TPU 예제와 동일하게, \n",
        "\n",
        "고정 chunk_size 모델의 weight을 저장, chunk_size=None 로  rnn 을 생성 후 저장된 weight 만 로드 하였다.\n",
        "\n",
        "\n",
        "현재 본 colab 상에 python 라이브러리가 몇개 없어서인지 결과는 생각보다는 별로다."
      ]
    },
    {
      "metadata": {
        "id": "S365Jny5rBrF",
        "colab_type": "code",
        "outputId": "21f1db82-f5db-4c74-e332-5f48f5eb7f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "prediction_model = char_rnn_model(None, len(py_chars), num_layers=2, num_nodes=640, dropout=0) \n",
        "prediction_model.load_weights('/content/gutenberg_data/models/02/py_code_weights.h5')\n",
        "prediction_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 97)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_13 (TimeDis (None, None, 97)          62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lKQ9LYD_3ay5",
        "colab_type": "code",
        "outputId": "49b89e4d-3800-4d1e-ec0b-58bdd431f0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3412
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_code(model, start_with='\\ndef ', end_with='\\n\\n', diversity=1.0):\n",
        "    generated = start_with\n",
        "    yield generated\n",
        "    for i in range(2000):\n",
        "        x = np.zeros((1, len(generated), len(py_chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, py_char_to_idx[char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        \n",
        "        preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        next_index = np.argmax(probas)        \n",
        "        next_char = py_chars[next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "        if generated.endswith(end_with):\n",
        "            break\n",
        "\n",
        "st = ''\n",
        "for i in range(20):\n",
        "    for ch in generate_code(prediction_model):\n",
        "        sys.stdout.write(ch)\n",
        "        st += ch\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "def itemgee2_array(*xerr = hashlib.m = heap[action]\n",
            "    alias_gc                                       \n",
            "    raise AssertionError(\"MSG\"\n",
            "GE = byte_inde = re.compile(r'[a-zA-M]\")\n",
            "except Impo =errormsg:\n",
            "    standardMset = {}\n",
            "\n",
            "\n",
            "\n",
            "def masterchr - cnf={MSGt\n",
            "    def window(name):\n",
            "        \"\"\"MSG\"\"\"'%s' % iConn and '    SinNodetUs els ' + nazename\n",
            "    else:\n",
            "        names = names.split(\".\")\n",
            "    rematedihten = 1\n",
            "\n",
            "\n",
            "\n",
            "def sav i 2):\n",
            "retur sorted(x for x in x + 1):\n",
            "    rc = sre (int(word))*1000)\n",
            "    return t.lower()\n",
            "\n",
            "\n",
            "\n",
            "def _rmdefault_t prog.1set_pa <Peerhas(p as Ftpcp2, p2c2(peermcos, s, end decode_q.CRED_ERROR)\n",
            "\n",
            "\n",
            "\n",
            "def r <= 1:\n",
            "    reason = b'H' not in mode\n",
            "    file_or_future.cail(result or [] i = is_finalizer(cha modname)\n",
            "fro(q, data):\n",
            "    attr, va_w_items = html.entities()\n",
            "    if default_header_map is not None and cpu_noop():\n",
            "        new_comps.append(tosize)\n",
            "        code = locale_aliaf\n",
            "        loadfile_l = filename\n",
            "    else:\n",
            "        compiler_type = 'mi */x + theye' | th)\n",
            "        target_vexup = ''.join([test+6, test])\n",
            "    d = {\"MimeFolume\"), -tagl, r\"M\n",
            "    ~opcod modeL =r (modelimitrrict + r\"(-.%s%s\" % (x, y)) feetureed = ge .deno = pensize\n",
            "    names . bindingroup_bytes = len(byte_lin\n",
            "    for i                unicode_filterfj[\"escape] = _de except Att = ehlowedError\n",
            "\n",
            "\n",
            "\n",
            "def _c is not None:\n",
            "    _platform_cache.clear()\n",
            "\n",
            "\n",
            "\n",
            "def top():\n",
            "    for tok in w.handle\n",
            "logid() as value:\n",
            "        yield\n",
            "        tup = action for opt, value in options.items()r\\n'\n",
            "  MIMENon = '\\n'\n",
            "n       \n",
            "    else:\n",
            "            item = '\n",
            "\n",
            "\n",
            "\n",
            "def datetime_result(date):\n",
            "    \"\"\"MSG\"\"\"\n",
            "        missing = []\n",
            "        if match:\n",
            "\n",
            "\n",
            "\n",
            "def urlsafe_t S_lsG(url.replace(data, size) + b'\\n')\n",
            "\n",
            "\n",
            "\n",
            "def _list_from_string(s[-1]):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    \n",
            "    \n",
            "    dummy = f\n",
            "    send_down(*possibilities, n, Man=Nonop)\n",
            "    if not deep oo n == 0:ib is not None:\n",
            "      url ==  \n",
            "       args = cookie\n",
            "    reposing   root is not \\\n",
            "          hisname and n == 1 or  'n' in func.__name__\n",
            "                        and s('.(%s)' % \n",
            "                 _new_value' Any <<len(starts) - 1):\n",
            "      or ''\n",
            "        return tuple([_nute)] == e ):\n",
            "      __)\n",
            "   nametpd_type = typecapt\n",
            "    _inter.__Encoding__\"abildentialnamed = 0\n",
            "\n",
            "\n",
            "\n",
            "def class_(*argsByta0, **kw): % (self.currentLineItem,\n",
            "                     self.sca   (len(self.color) + \" \" + command)\n",
            "        self.commands[bu)] = None\n",
            "        return self.undobufferentries(*str(content))\n",
            "\n",
            "\n",
            "\n",
            "def _parse_fla <<=5D(data):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    f.seek(pos)\n",
            "    dfe =r in zip(fdict for f in r_name(body)_i = b''.join(chars)\n",
            "    for dat in data:\n",
            "        i = b'0'*\"\\\\\"\n",
            "        flag = '&'\n",
            "        dumparm = can=os.read(address,swafl_filters)\n",
            "        return buf +\n",
            "            .firstweekday)\n",
            "def ListCompbyx = newparams\n",
            "\n",
            "\n",
            "\n",
            "def splitext(p):\n",
            "    p = os.path.normpath(path)\n",
            "    words = []\n",
            "    ph = bytes(writing)\n",
            "raw_data_menu = ctypes.we Sequence(SMTP)\n",
            "SetProxyType = type([]).vumtext.__no_type_check__()\n",
            "\n",
            "\n",
            "\n",
            "def expr'Mor2ms, None):\n",
            "    \"\"\"MSG\"\"\"\n",
            " templatenamd = \"tuple\" % dict(\n",
            "        ppr for p in groupind long               \n",
            "    return pred\n",
            "\n",
            "\n",
            "\n",
            "def _irsdecoded_words(a, b):\n",
            "    \"MSG\"\n",
            "    a tagged_a()  \n",
            "    return base and b\"\" or nr\n",
            "\n",
            "\n",
            "\n",
            "def  parsed_entities(src, name, path, fallbach=factory=False):\n",
            "    \"\"\"MS'\"({'norm\n",
            "        if s == b'e':\n",
            "  ftper = b'0R;'\n",
            "    else:\n",
            "        raise ValueError(msg % top 'rlue)\n",
            "\n",
            "\n",
            "\n",
            "def swapcolo2aX(name, lst=0):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    \n",
            "    if not condtype.__dict__:\n",
            "        _type(u %s %s\" get   format(text))\n",
            "    return t.\n",
            "\n",
            "\n",
            "\n",
            "def py_match(random, sendmailbox):\n",
            "    \"\"\"MSG\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "def _remove_ugvariate(filename, fname, ctx=None):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    if not isinstlist:Menubutton(MAPversion, \n",
            "    di':  \n",
            "    def tk_ver =0  = \"\"error\"\":\n",
            "        \"\"\"MSG\"\"\"\n",
            "    try:\n",
            "        j = file.read(2)\n",
            "        if len(t) !=                params = buf[148024]\n",
            "    \n",
            " o = -alphanum\n",
            "    reo[-1] = quoted\n",
            "    e = [4*5])\n",
            "    for i not in pat:\n",
            "        return p[:0]\n",
            "    return col\n",
            "\n",
            "\n",
            "\n",
            "def _posixsubprocess_shutdown(\n",
            "    \n",
            "    ):\n",
            "    \n",
            "    \n",
            "    max_workers = {}\n",
            "    stashed = None\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kft-cxL23azP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}