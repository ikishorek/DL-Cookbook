{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.1 Generating Text in the Style of an Example Text - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "t0fqVfPuRO8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "colab 에서 gutenberg 패키지를 사용하기 위해서는, libdb5.3-dev 선행 설치가 필요하다."
      ]
    },
    {
      "metadata": {
        "id": "PGLS_ZOe5Vnf",
        "colab_type": "code",
        "outputId": "281cf8b7-44b8-435e-fad7-09c6c96ac0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "cell_type": "code",
      "source": [
        "!sudo apt install libdb5.3-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  db5.3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libdb5.3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 762 kB of archives.\n",
            "After this operation, 3,146 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdb5.3-dev amd64 5.3.28-13.1ubuntu1 [762 kB]\n",
            "Fetched 762 kB in 1s (942 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdb5.3-dev.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../libdb5.3-dev_5.3.28-13.1ubuntu1_amd64.deb ...\n",
            "Unpacking libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n",
            "Setting up libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiCi2V2G3bUq",
        "colab_type": "code",
        "outputId": "f463ddf3-c294-4a68-cb5b-3a5c83ce9742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install gutenberg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (40.8.0)\n",
            "Collecting gutenberg\n",
            "  Downloading https://files.pythonhosted.org/packages/14/b1/6e99867c38e70d46366966a0a861c580377f38312cf9dbad38b82ed1823d/Gutenberg-0.7.0.tar.gz\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/fc/ebfbd4de236b493f9ece156f816c21df0ae87ccc22604c5f9b664efef1b9/bsddb3-6.2.6.tar.gz (239kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (0.16.0)\n",
            "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/a2/bc580a51ac1f9680aa04da4b6e96d499903d6e606d2f78f02e73527799da/rdflib_sqlalchemy-0.3.8-py3-none-any.whl\n",
            "Collecting rdflib>=4.2.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 24.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (2.18.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (40.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (1.11.0)\n",
            "Collecting alembic>=0.8.8 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/06/f1ae8393463c26f3dafa21eebac611088da02a26e1f1e23bd75fee2dbffe/alembic-1.0.7.tar.gz (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.1.4 in /usr/local/lib/python3.6/dist-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.18)\n",
            "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.2.0->gutenberg) (2.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (1.22)\n",
            "Collecting Mako (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f3/67579bb486517c0d49547f9697e36582cd19dafb5df9e687ed8e22de57fa/Mako-1.0.7.tar.gz (564kB)\n",
            "\u001b[K    100% |████████████████████████████████| 573kB 23.7MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3 (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.1.1)\n",
            "Building wheels for collected packages: gutenberg, bsddb3, alembic, Mako\n",
            "  Building wheel for gutenberg (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8e/cd/75/4bc6f16541a1b7a69b02168da567695b2271c23ac4a0a0a453\n",
            "  Building wheel for bsddb3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/11/b8/b3/fa84db10bf8c563e4ba1a72837a0946d123f12adb34b164bf5\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f9/71/46/604b8a4f0a04b513f5799c974b556c1de19a70fde41d25672b\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/35/25/dbcb848832ccb1a4b4ad23f529badfd3bce9bf88017f7ca510\n",
            "Successfully built gutenberg bsddb3 alembic Mako\n",
            "Installing collected packages: bsddb3, isodate, rdflib, Mako, python-editor, alembic, rdflib-sqlalchemy, gutenberg\n",
            "Successfully installed Mako-1.0.7 alembic-1.0.7 bsddb3-6.2.6 gutenberg-0.7.0 isodate-0.6.0 python-editor-1.0.4 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_WCvUA0zQGpd",
        "colab_type": "code",
        "outputId": "68cbf8af-0ae4-4ab0-d13d-7e2dc4a114a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dmqnH2hoSjjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "시간 절약을 위해, gutenberg db file 들을 미리 다운로드 받아놓고, google drive 를 mount 하여 연결한다.\n",
        "\n",
        "* gutenberg db file 다운로드: https://drive.google.com/drive/folders/1Y6nMqJ-srDfflTuoniVdEzdWfkrShe4T\n",
        "\n",
        "*  gutenberg db file  기본 경로 : ~/gutenberg_data/metadata/metadata.db\n",
        "\n",
        "*  기학습된 모델 파일 경로 : /gutenberg_data/models/\n",
        "\n",
        "colab 상에서는, /content/gutenberg_data/metadata/metadata.db 에 위치하면 됨"
      ]
    },
    {
      "metadata": {
        "id": "fVrbBx3IQvqa",
        "colab_type": "code",
        "outputId": "2345b03f-64f7-4cc4-e2d4-c511d52bfc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lx3_jzJdR-W4",
        "colab_type": "code",
        "outputId": "abe89b2a-2d4c-437d-ba83-54e1330b2b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# unmount 를 원할 시 실행\n",
        "\n",
        "#!fusermount -u gdrive\n",
        "#!rmdir gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fusermount: failed to unmount /content/gutenberg_data: Invalid argument\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xgwi_TVgTWwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "/content/gdrive 에 google drive 를 mount 하고,\n",
        "\n",
        " 구글 드라이브 상의 /gutenberg/gutenberg_data 를 \n",
        " /content/gutenberg_data 위치로 symbolic link 연결한다\n"
      ]
    },
    {
      "metadata": {
        "id": "gZcDcY0xWPXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/gutenberg/gutenberg_data /content/gutenberg_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36ZYgHb_VjKx",
        "colab_type": "code",
        "outputId": "33821d77-d5e7-4c94-9a5d-29aa728f37ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al /content/gutenberg_data/metadata/metadata.db"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3697408\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  contexts\n",
            "-rw------- 1 root root 909377536 Mar  2 03:05 'c^o^s^p^'\n",
            "-rw------- 1 root root 942211072 Mar  2 03:05 'c^p^o^s^'\n",
            "-rw------- 1 root root 915693568 Mar  2 03:06 'c^s^p^o^'\n",
            "-rw------- 1 root root    999424 Mar  2 03:05  __db.001\n",
            "-rw------- 1 root root   1441792 Mar  2 03:06  __db.002\n",
            "-rw------- 1 root root  65544192 Mar  2 03:06  __db.003\n",
            "-rw------- 1 root root 285048832 Mar  2 03:06  i2k\n",
            "-rw------- 1 root root 665780224 Mar  2 03:06  k2i\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  namespace\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  prefix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rU6lpK_33ayM",
        "colab_type": "code",
        "outputId": "098dcd17-adbf-4a08-c1ec-ee259730d6f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    GUTENBERG = True\n",
        "    from gutenberg.acquire import load_etext\n",
        "    from gutenberg.query import get_etexts, get_metadata\n",
        "    from gutenberg.acquire import get_metadata_cache\n",
        "    from gutenberg.acquire.text import UnknownDownloadUriException\n",
        "    from gutenberg.cleanup import strip_headers\n",
        "    from gutenberg._domain_model.exceptions import CacheAlreadyExistsException\n",
        "except ImportError:\n",
        "    GUTENBERG = False\n",
        "    print('Gutenberg is not installed. See instructions at https://pypi.python.org/pypi/Gutenberg')\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import tensorflow.keras.callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "import scipy.misc\n",
        "import json\n",
        "\n",
        "import os, sys\n",
        "import re\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "try:\n",
        "    from io import BytesIO\n",
        "except ImportError:\n",
        "    from StringIO import StringIO as BytesIO"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3u4J8SIPTuKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "미리 db 데이터를 넣어 놓아도,  cache.populate() 시 메타 데이타 땡겨오는데 시간이 꽤 걸린다.\n",
        "\n",
        "꼼수가 있는데, 아래 코드블럭을 run 하고 바로 stop 하면 메타데이타(get_metadata) 검색은 잘 안되지만,\n",
        "\n",
        "\n",
        "실제 텍스트 데이터 로드(load_etext) 는 잘 실행된다."
      ]
    },
    {
      "metadata": {
        "id": "fnVGqiNJ3ayP",
        "colab_type": "code",
        "outputId": "efe07a71-42ac-48e5-97c4-9dfafdb77261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    cache = get_metadata_cache()\n",
        "    try:\n",
        "        cache.populate()\n",
        "    except CacheAlreadyExistsException as e:\n",
        "        print(e)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "location: /root/gutenberg_data/metadata/metadata.db\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qz2ZG4om3ayR",
        "colab_type": "code",
        "outputId": "8c188bd8-cf02-45f9-b894-e2e3676f43c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2117
        }
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    for text_id in get_etexts('author', 'Shakespeare, William'):\n",
        "        print(text_id, list(get_metadata('title', text_id))[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1536 The Life of Timon of Athens\n",
            "1537 Pericles, Prince of Tyre\n",
            "1538 Cymbeline\n",
            "1539 The Winter's Tale\n",
            "1540 The Tempest\n",
            "1541 The Life of Henry the Eighth\n",
            "1543 A Lover's Complaint\n",
            "1544 The Passionate Pilgrim\n",
            "1545 The Passionate Pilgrim\n",
            "1546 Sonnets on Sundry Notes of Music\n",
            "1528 The History of Troilus and Cressida\n",
            "1529 All's Well That Ends Well\n",
            "1041 Shakespeare's Sonnets\n",
            "1530 Measure for Measure\n",
            "1045 Venus and Adonis\n",
            "1531 Othello, the Moor of Venice\n",
            "1533 Macbeth\n",
            "10281 Antony's Address over the Body of Caesar\r\n",
            "From Julius Caesar\n",
            "12842 A Fairy Tale in Two Acts Taken from Shakespeare (1763)\n",
            "13868 Macbeth\n",
            "15418 Ang Sintang Dalisay ni Julieta at Romeo\n",
            "15942 Antoine et Cléopâtre\n",
            "1100 The First Part of Henry the Sixth\n",
            "1101 The Second Part of King Henry the Sixth\n",
            "1102 The Third Part of King Henry the Sixth\n",
            "1103 King Richard III\n",
            "1104 The Comedy of Errors\n",
            "1105 The Sonnets\n",
            "1106 The Tragedy of Titus Andronicus\n",
            "1107 The Taming of the Shrew\n",
            "1108 The Two Gentlemen of Verona\n",
            "1109 Love's Labour's Lost\n",
            "1110 King John\n",
            "1111 King Richard the Second\n",
            "1112 The Tragedy of Romeo and Juliet\n",
            "1113 A Midsummer Night's Dream\n",
            "1114 The Merchant of Venice\n",
            "1115 The First Part of King Henry the Fourth\n",
            "1116 The Merry Wives of Windsor\n",
            "1117 Second Part of King Henry IV\n",
            "1118 Much Ado about Nothing\n",
            "1119 The Life of King Henry the Fifth\n",
            "1120 The Tragedy of Julius Caesar\n",
            "1121 As You Like It\n",
            "1122 The Tragedy of Hamlet, Prince of Denmark\n",
            "1123 Twelfth Night; Or, What You Will\n",
            "100 The Complete Works of William Shakespeare\n",
            "1124 The History of Troilus and Cressida\n",
            "1125 All's Well That Ends Well\n",
            "1126 Measure for Measure\n",
            "1127 The Tragedy of Othello, Moor of Venice\n",
            "1128 The Tragedy of King Lear\n",
            "16490 Kuningas Lear\n",
            "1129 The Tragedy of Macbeth\n",
            "1130 The Tragedy of Antony and Cleopatra\n",
            "1131 The Tragedy of Coriolanus\n",
            "1132 The Life of Timon of Athens\n",
            "1133 Cymbeline\n",
            "1134 The Winter's Tale\n",
            "1135 The Tempest\n",
            "1136 King Henry the Eighth\n",
            "1137 A Lover's Complaint\n",
            "17529 Othello\n",
            "17046 Les alegres comares de Windsor\n",
            "15032 Hamlet\n",
            "15071 La Tempête\n",
            "16618 Antonius ja Cleopatra\n",
            "16128 Le Jour des Rois\n",
            "15632 Hamlet\n",
            "15643 Romeo ja Julia\n",
            "12578 Shakespeare's play of the Merchant of Venice\r\n",
            "Arranged for Representation at the Princess's Theatre, with Historical and Explanatory Notes by Charles Kean, F.S.A.\n",
            "16710 Les Deux Gentilshommes de Vérone\n",
            "10606 The Tragedie of Hamlet, Prince of Denmark\n",
            "A Study with the Text of the Folio of 1623\n",
            "1430 Beautiful Stories from Shakespeare\n",
            "12719 Sonnet # 29\n",
            "12720 Sonnet #40\n",
            "12721 Sonnet #55\n",
            "12722 Sonnet #100\n",
            "12723 Sonnet #106\n",
            "12724 Sonnet #116\n",
            "15303 Coriolan\n",
            "1500 King Henry VI, First Part\n",
            "1501 History of King Henry the Sixth, Second Part\n",
            "1502 The History of King Henry the Sixth, Third Part\n",
            "1503 The Tragedy of King Richard III\n",
            "1504 The Comedy of Errors\n",
            "1505 The Rape of Lucrece\n",
            "1506 The Rape of Lucrece\n",
            "1507 The Tragedy of Titus Andronicus\n",
            "1508 The Taming of the Shrew\n",
            "1509 The Two Gentlemen of Verona\n",
            "15846 Beaucoup de Bruit pour Rien\n",
            "15847 Jules César\n",
            "15848 La Comédie des Méprises\n",
            "15849 Timon d'Athènes\n",
            "1510 Love's Labour's Lost\n",
            "1511 King John\n",
            "1512 The Tragedy of King Richard the Second\n",
            "1513 Romeo and Juliet\n",
            "1514 A Midsummer Night's Dream\n",
            "1515 The Merchant of Venice\n",
            "1516 King Henry IV, the First Part\n",
            "1517 The Merry Wives of Windsor\n",
            "1518 King Henry IV, Second Part\n",
            "1519 Much Ado about Nothing\n",
            "1520 Much Ado about Nothing\n",
            "1521 The Life of King Henry V\n",
            "1522 Julius Caesar\n",
            "1527 Twelfth Night; Or, What You Will\n",
            "1523 As You Like It\n",
            "1524 Hamlet, Prince of Denmark\n",
            "1525 The Phoenix and the Turtle\n",
            "1526 Twelfth Night; Or, What You Will\n",
            "1532 The Tragedy of King Lear\n",
            "16893 Macbeth\n",
            "1534 Antony and Cleopatra\n",
            "1535 The Tragedy of Coriolanus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ic7JF1RMWpQp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "서적 ID를 기반으로 실제 서적 데이터를 땡겨온다.\n",
        "\n",
        "\n",
        "100 : 세익스피어 전집(희곡)\n",
        "\n",
        "2981 : 카사노바 회고집(에세이)\n",
        "\n",
        "[9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398] : clarissa (여러 편의 편지 형태)\n",
        "\n",
        "\n",
        "학습 텍스트는 원본에서 목차 등 불필요한 텍스트를 발라내게 된다. (text.split 부분)"
      ]
    },
    {
      "metadata": {
        "id": "8n00slgP6YGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shakespeare_id = 100\n",
        "casanova_id = 2981 \n",
        "clarissa_ids = [9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398]\n",
        "\n",
        "def load_etext_from(ids, filter_func):\n",
        "  etext = '\\n'.join([filter_func(strip_headers(load_etext(id))) \\\n",
        "                     for id in ids])\n",
        "  return etext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h43THvuk-F84",
        "colab_type": "code",
        "outputId": "bc45bb22-f279-4227-8de2-9372ef764041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "shakespeare = load_etext_from([shakespeare_id], lambda text: text.split('\\nTHE END', 1)[-1])\n",
        "print(len(shakespeare))\n",
        "shakespeare[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5528070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\nALL’S WELL THAT ENDS WELL\\n\\n\\nby William Shakespeare\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Rossillon. A room in the Countess’s palace.\\nScene II. Paris. A room in the King’s palace.\\nScene III. Rossillon. A Room in the Palace.\\n\\n\\nACT II\\nScene I. Paris. A room in the King’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Paris. The King’s palace.\\nScene IV. Paris. The King’s palace.\\nScene V. Another room in the same.\\n\\n\\nACT III\\nScene I. Florence. A room in the Duke’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Florence. Before the Duke’s palace.\\nScene IV. Rossillon. A room in the Countess’s palace.\\nScene V. Without the walls of Florence.\\nScene VI. Camp before Florence.\\nScene VII. Florence. A room in the Widow’s house.\\n\\n\\nACT IV\\nScene I. Without the Florentine camp.\\nScene II. Florence. A room in the Widow’s house.\\nScene III. The Florentine camp.\\nScene IV. Florence. A room in the Widow’s house.\\nScene V. Rossillon. A room in the Countess’s palace.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "nvAMCBz1Ci9Q",
        "colab_type": "code",
        "outputId": "63c1427b-8092-49c0-d365-9ee3c4e5f44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "casanova = load_etext_from([casanova_id], lambda text: text.split('\\nCASANOVA AT DUX', 1)[-1]) # from main contents\n",
        "print(len(casanova))\n",
        "casanova[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6685264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n An Unpublished Chapter of History, By Arthur Symons\\n\\n I\\n The Memoirs of Casanova, though they have enjoyed the popularity of a bad reputation, have never had justice done to them by serious students of literature, of life, and of history. One English writer, indeed, Mr. Havelock Ellis, has realised that ‘there are few more delightful books in the world,’ and he has analysed them in an essay on Casanova, published in Affirmations, with extreme care and remarkable subtlety. But this essay stands alone, at all events in English, as an attempt to take Casanova seriously, to show him in his relation to his time, and in his relation to human problems. And yet these Memoirs are perhaps the most valuable document which we possess on the society of the eighteenth century; they are the history of a unique life, a unique personality, one of the greatest of autobiographies; as a record of adventures, they are more entertaining than Gil Blas, or Monte Cristo, or any of the imaginary travels, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "Zbur6dJQCjH6",
        "colab_type": "code",
        "outputId": "61e156ee-8989-46b4-c2b4-c15f509c0320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "clarissa = load_etext_from(clarissa_ids, lambda text: text.split('\\nTHE HISTORY OF CLARISSA HARLOWE', 1)[-1]) # from main contents\n",
        "print(len(clarissa))\n",
        "clarissa[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5173348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\nLETTER I\\n\\nMISS ANNA HOWE, TO MISS CLARISSA HARLOWE JAN 10.\\n\\n\\nI am extremely concerned, my dearest friend, for the disturbances that\\nhave happened in your family. I know how it must hurt you to become\\nthe subject of the public talk: and yet, upon an occasion so generally\\nknown, it is impossible but that whatever relates to a young lady, whose\\ndistinguished merits have made her the public care, should engage every\\nbody's attention. I long to have the particulars from yourself; and of\\nthe usage I am told you receive upon an accident you could not help; and\\nin which, as far as I can learn, the sufferer was the aggressor.\\n\\nMr. Diggs, the surgeon, whom I sent for at the first hearing of the\\nrencounter, to inquire, for your sake, how your brother was, told me,\\nthat there was no danger from the wound, if there were none from the\\nfever; which it seems has been increased by the perturbation of his\\nspirits.\\n\\nMr. Wyerley drank tea with us yesterday; and though he is far from being\\npartial to \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "2smFtvyOXVtS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "모델 저장에 쓸 파일명, char 임베딩 목록, chunk_size 등 메타데이타를 dict 로 묶어둔다."
      ]
    },
    {
      "metadata": {
        "id": "s0nqz37H3ayW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_chars_index(etext):\n",
        "  chars = list(sorted(set(etext)))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "def generate_meta_from(etext, model_name, chunk_size=160):\n",
        "  etext_meta = {}\n",
        "  etext_meta['model_name'] = model_name\n",
        "  etext_meta['char'], etext_meta['char_to_idx'] = get_chars_index(etext)\n",
        "  etext_meta['chunk_size'] = chunk_size\n",
        "  return etext_meta\n",
        "  \n",
        "shakespeare_meta = generate_meta_from(shakespeare, 'shakespeare')\n",
        "casanova_meta = generate_meta_from(casanova, 'casanova')\n",
        "clarissa_meta = generate_meta_from(clarissa, 'clarissa')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YwM_iyMCXjv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "메서드 순서대로\n",
        "\n",
        "1. 텍스트 문체 흉내를 위한 rnn 모델 \n",
        "\n",
        "2. TPU 전용 모델 컨버팅 (tf.contrib.tpu.keras_to_tpu_model)\n",
        "\n",
        "3. chunk_size 별 임베딩 텍스트를 짤라주는 메서드\n",
        "\n",
        "이다. TPU 지원을 위해서 keras 기반 클래스나 메서드 등을 tf.keras 로 변환할 필요가 있다.\n",
        "\n",
        "(초기 import 문에도 관련 패키치명 수정이 있음)"
      ]
    },
    {
      "metadata": {
        "id": "pIG2LwgW3ayY",
        "colab_type": "code",
        "outputId": "33f5945a-a68d-46e5-ead2-c33d5fbdcd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def char_rnn_model(chunk_size, num_chars, num_layers, num_nodes=512, dropout=0.1):\n",
        "    input = Input(shape=(chunk_size, num_chars), name='input')\n",
        "    prev = input\n",
        "    for i in range(num_layers):\n",
        "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
        "        if dropout:\n",
        "            prev = Dropout(dropout)(lstm)\n",
        "        else:\n",
        "            prev = lstm\n",
        "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
        "    model = Model(inputs=[input], outputs=[dense])\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_tpu_model(compiled_model, tpu_address):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(compiled_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)))\n",
        "  \n",
        "  \n",
        "def data_generator(all_text, char_to_idx, batch_size, chunk_size):\n",
        "    X = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
        "    y = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
        "    while True:\n",
        "        for row in range(batch_size):\n",
        "            idx = random.randrange(len(all_text) - chunk_size - 1)\n",
        "            chunk = np.zeros((chunk_size + 1, len(char_to_idx)))\n",
        "            for i in range(chunk_size + 1):\n",
        "                chunk[i, char_to_idx[all_text[idx + i]]] = 1\n",
        "            X[row, :, :] = chunk[:chunk_size]\n",
        "            y[row, :, :] = chunk[1:]\n",
        "        yield X, y\n",
        "\n",
        "#next(data_generator(training_text, char_to_idx, 4, chunk_size=CHUNK_SIZE))  \n",
        "\n",
        "# TPU 관련 설정\n",
        "try:\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\n",
        "  TPU_ADDRESS = 'grpc://' + device_name\n",
        "  print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "except KeyError:\n",
        "  print('TPU not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at: grpc://10.112.95.250:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lma2zbdWYkjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TPU 모델을 생성하는 시점에 TPUClusterResolver 가 사용가능 TPU 들을 스캔하여 보여준다.\n",
        "\n",
        "특이 사항은, TPU 를 이용한 학습을 위해서는 데이터 총량을 제외한 모든 dimension 이 static 이어야 한다.\n",
        "\n",
        "그래서 chunk_size 로 명시 지정하여 학습 및 weight 저장 후, 로드 시 \n",
        "(inference 버젼 참고)\n",
        "\n",
        "\n",
        "예시)\n",
        "\n",
        "순수 keras  : input (InputLayer)           (None, None, 98) \n",
        "\n",
        "tf TPU 구동가능 버젼  : input (InputLayer)           (None, 160, 98) -> 여기서 160 은 chunk size"
      ]
    },
    {
      "metadata": {
        "id": "qzcioWym3ayb",
        "colab_type": "code",
        "outputId": "a2c54cea-b909-42b1-9c21-7be130c795df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        }
      },
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 160\n",
        "\n",
        "clarissa_model = get_tpu_model(char_rnn_model(clarissa_meta['chunk_size'], len(clarissa_meta['char']), num_layers=2, num_nodes=640, dropout=0), TPU_ADDRESS)\n",
        "print(clarissa_model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.112.95.250:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 12284464179694512849)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17918052039150788383)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10839526694855656025)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3122826369630911206)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1959499061609068135)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13815618805356909138)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 10240957504948528292)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14422585991492223688)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17656305604262722399)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 572279282893032345)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6522941046677826350)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 98)           0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1891840   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 160, 98)           62818     \n",
            "=================================================================\n",
            "Total params: 5,234,018\n",
            "Trainable params: 5,234,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cB1nx-iPY8rd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "EarlyStopping 의 경우 특정 조건이 발생하면 학습을 조기종료한다.\n",
        "\n",
        "아래의 경우 loss 가 min_delta 보다 적게 감소하는 경우가 10(patience) epoch 회 이상이면 학습을 조기 종료한다.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OfEsY1yS3ayf",
        "colab_type": "code",
        "outputId": "43e5e5c8-c411-4347-8d1b-b41035e63a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1664
        }
      },
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=10,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "clarissa_model.fit_generator(\n",
        "    data_generator(clarissa, clarissa_meta['char_to_idx'], batch_size=BATCH_SIZE, chunk_size=clarissa_meta['chunk_size']),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(clarissa) / (BATCH_SIZE * CHUNK_SIZE)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(32, 160, 98), dtype=tf.float32, name='input_70'), TensorSpec(shape=(32, 160, 98), dtype=tf.float32, name='time_distributed_3_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.475114107131958 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 113s - loss: 3.5677 - acc: 0.1466\n",
            "Epoch 2/40\n",
            " - 95s - loss: 3.2032 - acc: 0.1596\n",
            "Epoch 3/40\n",
            " - 95s - loss: 2.1013 - acc: 0.4131\n",
            "Epoch 4/40\n",
            " - 94s - loss: 1.4314 - acc: 0.5931\n",
            "Epoch 5/40\n",
            " - 94s - loss: 1.3354 - acc: 0.6200\n",
            "Epoch 6/40\n",
            " - 94s - loss: 1.3195 - acc: 0.6301\n",
            "Epoch 7/40\n",
            " - 94s - loss: 1.2690 - acc: 0.6414\n",
            "Epoch 8/40\n",
            " - 95s - loss: 1.2437 - acc: 0.6497\n",
            "Epoch 9/40\n",
            " - 96s - loss: 1.2052 - acc: 0.6585\n",
            "Epoch 10/40\n",
            " - 94s - loss: 1.2107 - acc: 0.6615\n",
            "Epoch 11/40\n",
            " - 96s - loss: 1.1841 - acc: 0.6675\n",
            "Epoch 12/40\n",
            " - 94s - loss: 1.2029 - acc: 0.6676\n",
            "Epoch 13/40\n",
            " - 96s - loss: 1.1614 - acc: 0.6758\n",
            "Epoch 14/40\n",
            " - 95s - loss: 1.1563 - acc: 0.6788\n",
            "Epoch 15/40\n",
            " - 94s - loss: 1.1470 - acc: 0.6819\n",
            "Epoch 16/40\n",
            " - 94s - loss: 1.1365 - acc: 0.6854\n",
            "Epoch 17/40\n",
            " - 93s - loss: 1.1287 - acc: 0.6884\n",
            "Epoch 18/40\n",
            " - 95s - loss: 1.1165 - acc: 0.6915\n",
            "Epoch 19/40\n",
            " - 93s - loss: 1.1490 - acc: 0.6895\n",
            "Epoch 20/40\n",
            " - 93s - loss: 1.1644 - acc: 0.6886\n",
            "Epoch 21/40\n",
            " - 95s - loss: 1.0982 - acc: 0.6984\n",
            "Epoch 22/40\n",
            " - 95s - loss: 1.0973 - acc: 0.7012\n",
            "Epoch 23/40\n",
            " - 94s - loss: 1.0831 - acc: 0.7040\n",
            "Epoch 24/40\n",
            " - 94s - loss: 1.0817 - acc: 0.7065\n",
            "Epoch 25/40\n",
            " - 94s - loss: 1.0415 - acc: 0.7128\n",
            "Epoch 26/40\n",
            " - 95s - loss: 1.0618 - acc: 0.7119\n",
            "Epoch 27/40\n",
            " - 93s - loss: 1.0585 - acc: 0.7140\n",
            "Epoch 28/40\n",
            " - 95s - loss: 1.0198 - acc: 0.7206\n",
            "Epoch 29/40\n",
            " - 95s - loss: 1.0252 - acc: 0.7209\n",
            "Epoch 30/40\n",
            " - 94s - loss: 1.0342 - acc: 0.7216\n",
            "Epoch 31/40\n",
            " - 94s - loss: 1.0485 - acc: 0.7210\n",
            "Epoch 32/40\n",
            " - 94s - loss: 1.0159 - acc: 0.7260\n",
            "Epoch 33/40\n",
            " - 94s - loss: 1.0300 - acc: 0.7255\n",
            "Epoch 34/40\n",
            " - 95s - loss: 0.9848 - acc: 0.7328\n",
            "Epoch 35/40\n",
            " - 95s - loss: 0.9985 - acc: 0.7325\n",
            "Epoch 36/40\n",
            " - 94s - loss: 0.9846 - acc: 0.7351\n",
            "Epoch 37/40\n",
            " - 94s - loss: 0.9988 - acc: 0.7349\n",
            "Epoch 38/40\n",
            " - 94s - loss: 1.0268 - acc: 0.7320\n",
            "Epoch 39/40\n",
            " - 94s - loss: 0.9643 - acc: 0.7409\n",
            "Epoch 40/40\n",
            " - 95s - loss: 0.9984 - acc: 0.7381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0a03c9fcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "jwHq27ck3ayi",
        "colab_type": "code",
        "outputId": "b9c38644-fadb-442f-8f60-e47f6136166a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "cell_type": "code",
      "source": [
        "def save_model(basepath, model, model_meta):\n",
        "  base_file_path = basepath + '/' + model_meta['model_name']\n",
        "  with open(base_file_path + '.json', 'w') as fout:\n",
        "      json.dump({\n",
        "          'chars': ''.join(model_meta['char']),\n",
        "          'char_to_idx': model_meta['char_to_idx'] ,\n",
        "          'chunk_size': CHUNK_SIZE,\n",
        "      }, fout)\n",
        "  model.save(base_file_path +'.h5')\n",
        "  model.save_weights(base_file_path + '_weights.h5')\n",
        "  \n",
        "\n",
        "\n",
        "!ls -al /content/gutenberg_data/models/\n",
        "\n",
        "basepath = '/content/gutenberg_data/models/02/'\n",
        "\n",
        "save_model(basepath, clarissa_model, clarissa_meta)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwx------ 2 root root 4096 Mar  2 06:08 01\n",
            "drwx------ 2 root root 4096 Mar  3 03:19 02\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8YXsk5bki9qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "고정 chunk_size 로 학습후 모델의 weight을 저장한다.\n",
        "\n",
        "(mount 된 google drive 내부(/content/gutenberg_data/ 하위)에 저장하여 다른 notebook 에서 활용 가능하도록 하였다.)\n",
        "\n",
        "그 후 모델을 load 할 시 chunk_size=None 로 새로운 rnn 을 생성 후 load_weights 로 weight 만 엎어치는 방식을 사용 하였다."
      ]
    },
    {
      "metadata": {
        "id": "33BNza2wwZHX",
        "colab_type": "code",
        "outputId": "c57e5eae-dfbe-49fd-f9ba-c7f1f130b2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "# prediction from saved weights\n",
        "\n",
        "def load_model(basepath, model_meta):\n",
        "  model = char_rnn_model(None, len(model_meta['char']), num_layers=2, num_nodes=640, dropout=0) \n",
        "  model.load_weights(basepath + \"/\" + model_meta['model_name'] + '_weights.h5')\n",
        "  return model\n",
        "  \n",
        "\n",
        "prediction_model = load_model(basepath, clarissa_meta)\n",
        "prediction_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 98)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1891840   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, None, 98)          62818     \n",
            "=================================================================\n",
            "Total params: 5,234,018\n",
            "Trainable params: 5,234,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cwp7eIu-iso1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "clarissa 모델로 diversity 만 변경하여 문장을 한번씩 생성해 보았다.\n",
        "\n",
        "하기 결과에는 눈에 띄진 않으나 실험적으로는,\n",
        "\n",
        "diversity 가 1에 가까울수록 다양하면서 아무말(과 철자 오류) 하는 느낌이고\n",
        "diversity=None 일 경우 좀 정돈된 말을 하지만 비슷비슷한 말을 반복하는 경향이 있다.\n",
        "\n",
        "amount 를 크게 잡으면(문장을 길게 생성하면) 눈에 보이게 된다.\n"
      ]
    },
    {
      "metadata": {
        "id": "2ffQ8tgF3ayj",
        "colab_type": "code",
        "outputId": "bae9ee6b-c42f-425c-9cf5-99c88571f9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_output(model, training_text, model_meta, start_index=None, diversity=None, amount=400):\n",
        "    if start_index is None:\n",
        "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
        "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
        "    yield generated + '#'\n",
        "    for i in range(amount):\n",
        "        x = np.zeros((1, len(generated), len(model_meta['char'])))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, model_meta['char_to_idx'][char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        if diversity is None:\n",
        "            next_index = np.argmax(preds[len(generated) - 1])\n",
        "        else:\n",
        "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            next_index = np.argmax(probas)     \n",
        "        next_char = model_meta['char'][next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "    return generated\n",
        "\n",
        "#for ch in generate_output(sp_prediction_model, training_text):\n",
        "for ch in generate_output(prediction_model, clarissa, model_meta=clarissa_meta, diversity=0.5, amount=1500):\n",
        "    sys.stdout.write(ch)\n",
        "print()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ght have been attended with bad consequences,) no\n",
            "two brothers have a more cordial esteem for each other.  You know, Mr.\n",
            "Lovelace, that there is a consent, as I# may call it, on the contents of this\n",
            "dear creature, and the company of a domestic nature.  What a figure would the\n",
            "surmounted day before him!  The subject is a very vile person of her fortune.  On the\n",
            "contrary, she seemed to be a very wicked fellow, and a sudden flower and affectionate\n",
            "heart.\n",
            "\n",
            "Having disordered the man who had the honour to know the family, and to find\n",
            "him out, and made him set out in her company, and the more respectfully, and who had\n",
            "more difficult to regulate his motions by her parents.  And, as to\n",
            "mo, of a spirit so proper, and shall be my choice.  I will deposit this\n",
            "and unfriendly interest and apprehension that my love for her not\n",
            "lest favour to her.\n",
            "\n",
            "To the fellow has drawn her out of the course of this day's visit in fear of me; and that\n",
            "the result was this: 'that he could not help thinking the steels of the world, and to\n",
            "a pleasant place for your side of the question: and that will be sometimes\n",
            "that you are not to be expressed in it; I cannot but say.\n",
            "\n",
            "We approach you, Madam.  But it is my opinion, that the fellows are set upon you, will or\n",
            "not.\n",
            "\n",
            "I shall see this morning, as if it were any thing in the woman's justice.\n",
            "\n",
            "There is a sort of a fair will and politeness; but it is not worth while to give me a more\n",
            "particular inquiry. I have no suspicion that I am convinced, if she had not some\n",
            "heart of her dealing for all that must be my consent to me.\n",
            "\n",
            "All the manner of my wig and my sister's measures to renew their minds to my last, that I may not\n",
            "be s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrsOPjPcy1R_",
        "colab_type": "code",
        "outputId": "e8d63a98-af18-4760-dda7-7b2992bcc05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_model, clarissa, model_meta=clarissa_meta, amount=1500):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The doctor\n",
            "resolves to write to her father.  Her intense, yet cheerful devotion.\n",
            "\n",
            "LETTER LV.  Clarissa to Miss Howe.--\n",
            "A letter full of pious reflections, and g#lorying in them: and then she spoke\n",
            "this with so much spirit as I have set up an inclination to receive bendford from me.  I was\n",
            "afraid that her own mother would be glad to have the world to have the\n",
            "least thought of marrying in the manner I have so soon after for. But why should I be thought as\n",
            "freely as I have done?--Yet she could not but say, that if I were to be the more\n",
            "considerable that he was not a little too much to be displeased with my friends,\n",
            "will be all the miserable provise that were to be met with at the time.  And when I had\n",
            "done, that I was not able to preserve the first parents of a lady so much to be\n",
            "provided for an expedient which they were so set as to find him out.  They had\n",
            "sent for them to the farthest part of my cousin's letters.  I was silent.\n",
            "I was still silent.\n",
            "\n",
            "Tell me not of poor Belton.  I have no doubt that the settlements are so much afraid\n",
            "of the execution of the world as Mrs. Smith and his wife.  I think it is in the\n",
            "principal end.\n",
            "\n",
            "So down she flung.  I shall not be able to account for in a manner despondent with\n",
            "her.\n",
            "\n",
            "I wish you may, Madam, to consider what is much: you have not been used to call a horse,\n",
            "may be the better for the favours you have declared of me.\n",
            "\n",
            "I shall send this short letter to you; and that you will be pleased to mark\n",
            "that, because I have not suffered in the world to meet with a man who has not a friend in the world that\n",
            "thou mayest observe for him than the more for the faults of others of the ladies of his\n",
            "family, and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "myqaQTHNfauW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "파이썬 코드를 학습시켜 코드를 짜는 rnn 을 구현한 내용이다.\n",
        "\n",
        ".py 확장자의 코드를 전체 풀스캔하고 주석(#) 를 삭제하여 학습 데이터화한다."
      ]
    },
    {
      "metadata": {
        "id": "v3bn5bTy3aym",
        "colab_type": "code",
        "outputId": "9ecb021f-d93c-42bb-caeb-1d785aed5e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def find_python(rootdir):\n",
        "    matches = []\n",
        "    for root, dirnames, filenames in os.walk(rootdir):\n",
        "        for fn in filenames:\n",
        "            if fn.endswith('.py'):\n",
        "                matches.append(os.path.join(root, fn))\n",
        "\n",
        "    return matches\n",
        "#  + find_python(os.path.join(sys.executable.rsplit('/', 2)[0], 'lib'))\n",
        "srcs = find_python(random.__file__.rsplit('/', 1)[0])\n",
        "len(srcs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "metadata": {
        "id": "zYoPHqHY3ayp",
        "colab_type": "code",
        "outputId": "ac347931-6b49-471b-954f-769579106abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def replacer(value):\n",
        "    value = ''.join(ch for ch in value if ord(ch) < 127)\n",
        "    if not ' ' in value:\n",
        "        return value\n",
        "    if sum(1 for ch in value if ch.isalpha()) > 6:\n",
        "        return 'MSG'\n",
        "    return value\n",
        "\n",
        "\n",
        "def replace_literals(st):\n",
        "    res = []\n",
        "    start_text = start_quote = i = 0\n",
        "    quote = ''\n",
        "    while i < len(st):\n",
        "        if quote:\n",
        "            if st[i: i + len(quote)] == quote:\n",
        "                quote = ''\n",
        "                start_text = i\n",
        "                res.append(replacer(st[start_quote: i]))\n",
        "        elif st[i] in '\"\\'':\n",
        "            quote = st[i]\n",
        "            if i < len(st) - 2 and st[i + 1] == st[i + 2] == quote:\n",
        "                quote = 3 * quote\n",
        "            start_quote = i + len(quote)\n",
        "            res.append(st[start_text: start_quote])\n",
        "        if st[i] == '\\n' and len(quote) == 1:\n",
        "            start_text = i\n",
        "            res.append(quote)\n",
        "            quote = ''\n",
        "        if st[i] == '\\\\':\n",
        "            i += 1\n",
        "        i += 1\n",
        "    return ''.join(res) + st[start_text:]\n",
        "\n",
        "#replace_literals('print(\"hel\\\\\"lo\")') + replace_literals(\"print('hel\\\\'lo world')\")\n",
        "replace_literals('this = \"wrong\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this = \"\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "CWAhUnZm3ayr",
        "colab_type": "code",
        "outputId": "be5ca381-958c-4e41-b597-9b800157df47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "COMMENT_RE = re.compile('#.*')\n",
        "python_code = []\n",
        "for fn in srcs:\n",
        "    try:\n",
        "        with open(fn, 'r') as fin:\n",
        "            src = fin.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print('Could not read %s' % fn)\n",
        "    src = replace_literals(src)\n",
        "    src = COMMENT_RE.sub('', src)\n",
        "    python_code.append(src)\n",
        "\n",
        "python_code = '\\n\\n\\n'.join(python_code)\n",
        "len(python_code)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6328011"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "tyi1IXsO3ayv",
        "colab_type": "code",
        "outputId": "75f307cf-850d-4766-f7aa-b45dae68c388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "py_chars = list(sorted(set(python_code)))\n",
        "py_char_to_idx = {ch: idx for idx, ch in enumerate(py_chars)}\n",
        "len(py_chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "nA6VanAV3ayz",
        "colab_type": "code",
        "outputId": "6f703f0a-034a-4879-c16d-a9dfa298fdfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = char_rnn_model(160, len(py_chars), num_layers=2, num_nodes=640, dropout=0)\n",
        "py_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 97)           0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 160, 97)           62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8WaPmJK3NCcl",
        "colab_type": "code",
        "outputId": "58b0c24e-4445-48a5-aa61-747fd19178b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = tf.contrib.tpu.keras_to_tpu_model(py_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.65.67.106:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11502498185262799519)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11588889170251908459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15484636259604477500)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6188258309789351709)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17618968535966954990)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1368695216754022451)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 896554020254126233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12401001441761719750)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5652057927253700336)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17090350113839668390)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 17711765655739407088)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TtdXhdIk3ay2",
        "colab_type": "code",
        "outputId": "f19f3f7b-a0a2-49c1-83e5-34ae8437440c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1559
        }
      },
      "cell_type": "code",
      "source": [
        "early = tensorflow.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=10,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "py_model.fit_generator(\n",
        "    data_generator(python_code, py_char_to_idx, batch_size=BATCH_SIZE, chunk_size=160),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(python_code) / (BATCH_SIZE * 160)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_50'), TensorSpec(shape=(32, 160, 97), dtype=tf.float32, name='input_140'), TensorSpec(shape=(32, 160, 97), dtype=tf.float32, name='time_distributed_6_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.36404800415039 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 150s - loss: 3.2331 - acc: 0.3467\n",
            "Epoch 2/40\n",
            " - 115s - loss: 2.8069 - acc: 0.3642\n",
            "Epoch 3/40\n",
            " - 115s - loss: 3.0248 - acc: 0.3544\n",
            "Epoch 4/40\n",
            " - 114s - loss: 3.0548 - acc: 0.3566\n",
            "Epoch 5/40\n",
            " - 114s - loss: 2.8092 - acc: 0.3768\n",
            "Epoch 6/40\n",
            " - 114s - loss: 1.3388 - acc: 0.6687\n",
            "Epoch 7/40\n",
            " - 113s - loss: 1.0017 - acc: 0.7540\n",
            "Epoch 8/40\n",
            " - 114s - loss: 0.8883 - acc: 0.7841\n",
            "Epoch 9/40\n",
            " - 115s - loss: 0.8553 - acc: 0.7972\n",
            "Epoch 10/40\n",
            " - 116s - loss: 0.8362 - acc: 0.8057\n",
            "Epoch 11/40\n",
            " - 115s - loss: 0.8122 - acc: 0.8130\n",
            "Epoch 12/40\n",
            " - 113s - loss: 0.7815 - acc: 0.8204\n",
            "Epoch 13/40\n",
            " - 114s - loss: 0.7288 - acc: 0.8306\n",
            "Epoch 14/40\n",
            " - 115s - loss: 0.7772 - acc: 0.8257\n",
            "Epoch 15/40\n",
            " - 113s - loss: 0.7456 - acc: 0.8321\n",
            "Epoch 16/40\n",
            " - 114s - loss: 0.7237 - acc: 0.8371\n",
            "Epoch 17/40\n",
            " - 115s - loss: 0.7099 - acc: 0.8403\n",
            "Epoch 18/40\n",
            " - 114s - loss: 0.6827 - acc: 0.8455\n",
            "Epoch 19/40\n",
            " - 114s - loss: 0.6684 - acc: 0.8488\n",
            "Epoch 20/40\n",
            " - 115s - loss: 0.7054 - acc: 0.8445\n",
            "Epoch 21/40\n",
            " - 113s - loss: 0.6889 - acc: 0.8484\n",
            "Epoch 22/40\n",
            " - 114s - loss: 0.6675 - acc: 0.8517\n",
            "Epoch 23/40\n",
            " - 115s - loss: 0.6557 - acc: 0.8546\n",
            "Epoch 24/40\n",
            " - 115s - loss: 0.6735 - acc: 0.8528\n",
            "Epoch 25/40\n",
            " - 113s - loss: 0.6782 - acc: 0.8527\n",
            "Epoch 26/40\n",
            " - 113s - loss: 0.6347 - acc: 0.8598\n",
            "Epoch 27/40\n",
            " - 114s - loss: 0.6528 - acc: 0.8576\n",
            "Epoch 28/40\n",
            " - 114s - loss: 0.6324 - acc: 0.8621\n",
            "Epoch 29/40\n",
            " - 114s - loss: 0.6207 - acc: 0.8643\n",
            "Epoch 30/40\n",
            " - 114s - loss: 0.6153 - acc: 0.8654\n",
            "Epoch 31/40\n",
            " - 114s - loss: 0.6156 - acc: 0.8663\n",
            "Epoch 32/40\n",
            " - 114s - loss: 0.6019 - acc: 0.8684\n",
            "Epoch 33/40\n",
            " - 115s - loss: 0.5746 - acc: 0.8728\n",
            "Epoch 34/40\n",
            " - 113s - loss: 0.6431 - acc: 0.8642\n",
            "Epoch 35/40\n",
            " - 114s - loss: 0.6040 - acc: 0.8700\n",
            "Epoch 36/40\n",
            " - 114s - loss: 0.5904 - acc: 0.8719\n",
            "Epoch 37/40\n",
            " - 114s - loss: 0.6129 - acc: 0.8698\n",
            "Epoch 38/40\n",
            " - 114s - loss: 0.6101 - acc: 0.8705\n",
            "Epoch 39/40\n",
            " - 113s - loss: 0.5802 - acc: 0.8753\n",
            "Epoch 40/40\n",
            " - 114s - loss: 0.5997 - acc: 0.8731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96f1904438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "xslhrK9sUKPM",
        "colab_type": "code",
        "outputId": "48bc161c-e140-43f7-d98c-a45c027e6a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gutenberg_data/models/02/py_code.json', 'w') as fout:\n",
        "    json.dump({\n",
        "        'chars': ''.join(py_chars),\n",
        "        'char_to_idx': py_char_to_idx,\n",
        "        'chunk_size': 160,\n",
        "    }, fout)\n",
        "py_model.save('/content/gutenberg_data/models/02/py_code.h5')\n",
        "py_model.save_weights('/content/gutenberg_data/models/02/py_code_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cu4hlM_TgFXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "상기 TPU 예제와 동일하게, \n",
        "\n",
        "고정 chunk_size 모델의 weight을 저장, chunk_size=None 로  rnn 을 생성 후 저장된 weight 만 로드 하였다.\n",
        "\n",
        "\n",
        "현재 본 colab 상에 python 라이브러리가 몇개 없어서인지 결과는 생각보다는 별로다."
      ]
    },
    {
      "metadata": {
        "id": "S365Jny5rBrF",
        "colab_type": "code",
        "outputId": "664661bf-3555-47ae-b256-527349c49d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "prediction_model = char_rnn_model(None, len(py_chars), num_layers=2, num_nodes=640, dropout=0) \n",
        "prediction_model.load_weights('/content/gutenberg_data/models/02/py_code_weights.h5')\n",
        "prediction_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 97)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_21 (TimeDis (None, None, 97)          62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lKQ9LYD_3ay5",
        "colab_type": "code",
        "outputId": "49b89e4d-3800-4d1e-ec0b-58bdd431f0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3412
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_code(model, start_with='\\ndef ', end_with='\\n\\n', diversity=1.0):\n",
        "    generated = start_with\n",
        "    yield generated\n",
        "    for i in range(2000):\n",
        "        x = np.zeros((1, len(generated), len(py_chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, py_char_to_idx[char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        \n",
        "        preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        next_index = np.argmax(probas)        \n",
        "        next_char = py_chars[next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "        if generated.endswith(end_with):\n",
        "            break\n",
        "\n",
        "st = ''\n",
        "for i in range(20):\n",
        "    for ch in generate_code(prediction_model):\n",
        "        sys.stdout.write(ch)\n",
        "        st += ch\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "def itemgee2_array(*xerr = hashlib.m = heap[action]\n",
            "    alias_gc                                       \n",
            "    raise AssertionError(\"MSG\"\n",
            "GE = byte_inde = re.compile(r'[a-zA-M]\")\n",
            "except Impo =errormsg:\n",
            "    standardMset = {}\n",
            "\n",
            "\n",
            "\n",
            "def masterchr - cnf={MSGt\n",
            "    def window(name):\n",
            "        \"\"\"MSG\"\"\"'%s' % iConn and '    SinNodetUs els ' + nazename\n",
            "    else:\n",
            "        names = names.split(\".\")\n",
            "    rematedihten = 1\n",
            "\n",
            "\n",
            "\n",
            "def sav i 2):\n",
            "retur sorted(x for x in x + 1):\n",
            "    rc = sre (int(word))*1000)\n",
            "    return t.lower()\n",
            "\n",
            "\n",
            "\n",
            "def _rmdefault_t prog.1set_pa <Peerhas(p as Ftpcp2, p2c2(peermcos, s, end decode_q.CRED_ERROR)\n",
            "\n",
            "\n",
            "\n",
            "def r <= 1:\n",
            "    reason = b'H' not in mode\n",
            "    file_or_future.cail(result or [] i = is_finalizer(cha modname)\n",
            "fro(q, data):\n",
            "    attr, va_w_items = html.entities()\n",
            "    if default_header_map is not None and cpu_noop():\n",
            "        new_comps.append(tosize)\n",
            "        code = locale_aliaf\n",
            "        loadfile_l = filename\n",
            "    else:\n",
            "        compiler_type = 'mi */x + theye' | th)\n",
            "        target_vexup = ''.join([test+6, test])\n",
            "    d = {\"MimeFolume\"), -tagl, r\"M\n",
            "    ~opcod modeL =r (modelimitrrict + r\"(-.%s%s\" % (x, y)) feetureed = ge .deno = pensize\n",
            "    names . bindingroup_bytes = len(byte_lin\n",
            "    for i                unicode_filterfj[\"escape] = _de except Att = ehlowedError\n",
            "\n",
            "\n",
            "\n",
            "def _c is not None:\n",
            "    _platform_cache.clear()\n",
            "\n",
            "\n",
            "\n",
            "def top():\n",
            "    for tok in w.handle\n",
            "logid() as value:\n",
            "        yield\n",
            "        tup = action for opt, value in options.items()r\\n'\n",
            "  MIMENon = '\\n'\n",
            "n       \n",
            "    else:\n",
            "            item = '\n",
            "\n",
            "\n",
            "\n",
            "def datetime_result(date):\n",
            "    \"\"\"MSG\"\"\"\n",
            "        missing = []\n",
            "        if match:\n",
            "\n",
            "\n",
            "\n",
            "def urlsafe_t S_lsG(url.replace(data, size) + b'\\n')\n",
            "\n",
            "\n",
            "\n",
            "def _list_from_string(s[-1]):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    \n",
            "    \n",
            "    dummy = f\n",
            "    send_down(*possibilities, n, Man=Nonop)\n",
            "    if not deep oo n == 0:ib is not None:\n",
            "      url ==  \n",
            "       args = cookie\n",
            "    reposing   root is not \\\n",
            "          hisname and n == 1 or  'n' in func.__name__\n",
            "                        and s('.(%s)' % \n",
            "                 _new_value' Any <<len(starts) - 1):\n",
            "      or ''\n",
            "        return tuple([_nute)] == e ):\n",
            "      __)\n",
            "   nametpd_type = typecapt\n",
            "    _inter.__Encoding__\"abildentialnamed = 0\n",
            "\n",
            "\n",
            "\n",
            "def class_(*argsByta0, **kw): % (self.currentLineItem,\n",
            "                     self.sca   (len(self.color) + \" \" + command)\n",
            "        self.commands[bu)] = None\n",
            "        return self.undobufferentries(*str(content))\n",
            "\n",
            "\n",
            "\n",
            "def _parse_fla <<=5D(data):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    f.seek(pos)\n",
            "    dfe =r in zip(fdict for f in r_name(body)_i = b''.join(chars)\n",
            "    for dat in data:\n",
            "        i = b'0'*\"\\\\\"\n",
            "        flag = '&'\n",
            "        dumparm = can=os.read(address,swafl_filters)\n",
            "        return buf +\n",
            "            .firstweekday)\n",
            "def ListCompbyx = newparams\n",
            "\n",
            "\n",
            "\n",
            "def splitext(p):\n",
            "    p = os.path.normpath(path)\n",
            "    words = []\n",
            "    ph = bytes(writing)\n",
            "raw_data_menu = ctypes.we Sequence(SMTP)\n",
            "SetProxyType = type([]).vumtext.__no_type_check__()\n",
            "\n",
            "\n",
            "\n",
            "def expr'Mor2ms, None):\n",
            "    \"\"\"MSG\"\"\"\n",
            " templatenamd = \"tuple\" % dict(\n",
            "        ppr for p in groupind long               \n",
            "    return pred\n",
            "\n",
            "\n",
            "\n",
            "def _irsdecoded_words(a, b):\n",
            "    \"MSG\"\n",
            "    a tagged_a()  \n",
            "    return base and b\"\" or nr\n",
            "\n",
            "\n",
            "\n",
            "def  parsed_entities(src, name, path, fallbach=factory=False):\n",
            "    \"\"\"MS'\"({'norm\n",
            "        if s == b'e':\n",
            "  ftper = b'0R;'\n",
            "    else:\n",
            "        raise ValueError(msg % top 'rlue)\n",
            "\n",
            "\n",
            "\n",
            "def swapcolo2aX(name, lst=0):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    \n",
            "    if not condtype.__dict__:\n",
            "        _type(u %s %s\" get   format(text))\n",
            "    return t.\n",
            "\n",
            "\n",
            "\n",
            "def py_match(random, sendmailbox):\n",
            "    \"\"\"MSG\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "def _remove_ugvariate(filename, fname, ctx=None):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    if not isinstlist:Menubutton(MAPversion, \n",
            "    di':  \n",
            "    def tk_ver =0  = \"\"error\"\":\n",
            "        \"\"\"MSG\"\"\"\n",
            "    try:\n",
            "        j = file.read(2)\n",
            "        if len(t) !=                params = buf[148024]\n",
            "    \n",
            " o = -alphanum\n",
            "    reo[-1] = quoted\n",
            "    e = [4*5])\n",
            "    for i not in pat:\n",
            "        return p[:0]\n",
            "    return col\n",
            "\n",
            "\n",
            "\n",
            "def _posixsubprocess_shutdown(\n",
            "    \n",
            "    ):\n",
            "    \n",
            "    \n",
            "    max_workers = {}\n",
            "    stashed = None\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kft-cxL23azP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
