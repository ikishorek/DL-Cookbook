{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.1 Generating Text in the Style of an Example Text - inference.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7yWqdplWgoFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "colab 에서 gutenberg 패키지를 사용하기 위해서는, libdb5.3-dev 선행 설치가 필요하다."
      ]
    },
    {
      "metadata": {
        "id": "PGLS_ZOe5Vnf",
        "colab_type": "code",
        "outputId": "bd85a309-beda-411f-e904-b64aed658e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "!sudo apt install libdb5.3-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  db5.3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libdb5.3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 762 kB of archives.\n",
            "After this operation, 3,146 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdb5.3-dev amd64 5.3.28-13.1ubuntu1 [762 kB]\n",
            "Fetched 762 kB in 1s (670 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdb5.3-dev.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../libdb5.3-dev_5.3.28-13.1ubuntu1_amd64.deb ...\n",
            "Unpacking libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n",
            "Setting up libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiCi2V2G3bUq",
        "colab_type": "code",
        "outputId": "1d7bd1fc-74ee-4c97-87f9-b04f7d8ad712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install gutenberg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (40.8.0)\n",
            "Collecting gutenberg\n",
            "  Downloading https://files.pythonhosted.org/packages/14/b1/6e99867c38e70d46366966a0a861c580377f38312cf9dbad38b82ed1823d/Gutenberg-0.7.0.tar.gz\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/fc/ebfbd4de236b493f9ece156f816c21df0ae87ccc22604c5f9b664efef1b9/bsddb3-6.2.6.tar.gz (239kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (0.16.0)\n",
            "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/a2/bc580a51ac1f9680aa04da4b6e96d499903d6e606d2f78f02e73527799da/rdflib_sqlalchemy-0.3.8-py3-none-any.whl\n",
            "Collecting rdflib>=4.2.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 24.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (2.18.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (40.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (1.11.0)\n",
            "Collecting alembic>=0.8.8 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bb/ec1e21f2e303689ad2170eb47fc67df9ad4199ade6759a99474c4d3535c8/alembic-1.0.8.tar.gz (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.1.4 in /usr/local/lib/python3.6/dist-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.18)\n",
            "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.2.0->gutenberg) (2.3.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (1.22)\n",
            "Collecting Mako (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f3/67579bb486517c0d49547f9697e36582cd19dafb5df9e687ed8e22de57fa/Mako-1.0.7.tar.gz (564kB)\n",
            "\u001b[K    100% |████████████████████████████████| 573kB 25.9MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3 (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.1.1)\n",
            "Building wheels for collected packages: gutenberg, bsddb3, alembic, Mako\n",
            "  Building wheel for gutenberg (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8e/cd/75/4bc6f16541a1b7a69b02168da567695b2271c23ac4a0a0a453\n",
            "  Building wheel for bsddb3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/11/b8/b3/fa84db10bf8c563e4ba1a72837a0946d123f12adb34b164bf5\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/bc/74/834fa0c75c4ae6d6718db5e65187d508623ee291dead032156\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/35/25/dbcb848832ccb1a4b4ad23f529badfd3bce9bf88017f7ca510\n",
            "Successfully built gutenberg bsddb3 alembic Mako\n",
            "Installing collected packages: bsddb3, Mako, python-editor, alembic, isodate, rdflib, rdflib-sqlalchemy, gutenberg\n",
            "Successfully installed Mako-1.0.7 alembic-1.0.8 bsddb3-6.2.6 gutenberg-0.7.0 isodate-0.6.0 python-editor-1.0.4 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_WCvUA0zQGpd",
        "colab_type": "code",
        "outputId": "a91e4bb1-d886-429d-ce87-0c894c7a7b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-AJE6QUWg4AE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "시간 절약을 위해, gutenberg db file 들을 미리 다운로드 받아놓고, google drive 를 mount 하여 연결한다.\n",
        "\n",
        "* gutenberg db file 다운로드: https://drive.google.com/drive/folders/1Y6nMqJ-srDfflTuoniVdEzdWfkrShe4T\n",
        "\n",
        "*   gutenberg db file  기본 경로 : ~/gutenberg_data/metadata/metadata.db\n",
        "\n",
        "colab 상에서는, /content/gutenberg_data/metadata/metadata.db 에 위치하면 됨"
      ]
    },
    {
      "metadata": {
        "id": "fVrbBx3IQvqa",
        "colab_type": "code",
        "outputId": "64e69341-e487-494e-d1a3-b633a370f221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lx3_jzJdR-W4",
        "colab_type": "code",
        "outputId": "abe89b2a-2d4c-437d-ba83-54e1330b2b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# unmount 를 원할 시 실행\n",
        "\n",
        "#!fusermount -u gdrive\n",
        "#!rmdir gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fusermount: failed to unmount /content/gutenberg_data: Invalid argument\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCz5DhOmg_8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "/content/gdrive 에 google drive 를 mount 하고,\n",
        "\n",
        " 구글 드라이브 상의 /gutenberg/gutenberg_data 를 \n",
        " /content/gutenberg_data 위치로 symbolic link 연결한다\n"
      ]
    },
    {
      "metadata": {
        "id": "gZcDcY0xWPXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/gutenberg/gutenberg_data /content/gutenberg_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36ZYgHb_VjKx",
        "colab_type": "code",
        "outputId": "7e80f2d2-06e4-4079-d603-bbcc9ced0673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al /content/gutenberg_data/metadata/metadata.db"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3697408\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  contexts\n",
            "-rw------- 1 root root 909377536 Mar  2 03:05 'c^o^s^p^'\n",
            "-rw------- 1 root root 942211072 Mar  2 03:05 'c^p^o^s^'\n",
            "-rw------- 1 root root 915693568 Mar  2 03:06 'c^s^p^o^'\n",
            "-rw------- 1 root root    999424 Mar  2 03:05  __db.001\n",
            "-rw------- 1 root root   1441792 Mar  2 03:06  __db.002\n",
            "-rw------- 1 root root  65544192 Mar  2 03:06  __db.003\n",
            "-rw------- 1 root root 285048832 Mar  2 03:06  i2k\n",
            "-rw------- 1 root root 665780224 Mar  2 03:06  k2i\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  namespace\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  prefix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rU6lpK_33ayM",
        "colab_type": "code",
        "outputId": "24caa878-2967-41f1-ccb8-d11af46d5fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    GUTENBERG = True\n",
        "    from gutenberg.acquire import load_etext\n",
        "    from gutenberg.query import get_etexts, get_metadata\n",
        "    from gutenberg.acquire import get_metadata_cache\n",
        "    from gutenberg.acquire.text import UnknownDownloadUriException\n",
        "    from gutenberg.cleanup import strip_headers\n",
        "    from gutenberg._domain_model.exceptions import CacheAlreadyExistsException\n",
        "except ImportError:\n",
        "    GUTENBERG = False\n",
        "    print('Gutenberg is not installed. See instructions at https://pypi.python.org/pypi/Gutenberg')\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import tensorflow.keras.callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "import scipy.misc\n",
        "import json\n",
        "\n",
        "import os, sys\n",
        "import re\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "try:\n",
        "    from io import BytesIO\n",
        "except ImportError:\n",
        "    from StringIO import StringIO as BytesIO"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXsvn2uLhJoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "미리 db 데이터를 넣어 놓아도,  cache.populate() 시 메타 데이타 땡겨오는데 시간이 꽤 걸린다.\n",
        "\n",
        "꼼수가 있는데, 애로 코드블럭을 run 하고 바로 stop 하면 메타데이타(get_metadata) 검색은 잘 안되지만,\n",
        "\n",
        "\n",
        "실제 텍스트 데이터 로드(load_etext) 는 잘 실행된다."
      ]
    },
    {
      "metadata": {
        "id": "fnVGqiNJ3ayP",
        "colab_type": "code",
        "outputId": "ff21d25a-e9d6-44bc-f3ee-34c3cdea2cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1065
        }
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    cache = get_metadata_cache()\n",
        "    try:\n",
        "        cache.populate()\n",
        "    except CacheAlreadyExistsException as e:\n",
        "        print(e)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-70d9b2c78a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metadata_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCacheAlreadyExistsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gutenberg/acquire/metadata.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_metadata_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_metadata_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gutenberg/acquire/metadata.py\u001b[0m in \u001b[0;36m_download_metadata_archive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qQqO7VR3hNxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "서적 ID를 기반으로 실제 서적 데이터를 땡겨온다.\n",
        "\n",
        "\n",
        "100 : 세익스피어 전집(희곡)\n",
        "\n",
        "2981 : 카사노바 회고집(에세이)\n",
        "\n",
        "[9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398] : clarissa (여러 편의 편지 형태)\n",
        "\n",
        "\n",
        "학습 텍스트는 원본에서 목차 등 불필요한 텍스트를 발라내게 된다. (text.split 부분)"
      ]
    },
    {
      "metadata": {
        "id": "8n00slgP6YGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shakespeare_id = 100\n",
        "casanova_id = 2981 \n",
        "clarissa_ids = [9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398]\n",
        "\n",
        "def load_etext_from(ids, filter_func):\n",
        "  etext = '\\n'.join([filter_func(strip_headers(load_etext(id))) \\\n",
        "                     for id in ids])\n",
        "  return etext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h43THvuk-F84",
        "colab_type": "code",
        "outputId": "6cc95da7-e656-44d0-ca48-350e92ec5c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "shakespeare = load_etext_from([shakespeare_id], lambda text: text.split('\\nTHE END', 1)[-1])\n",
        "print(len(shakespeare))\n",
        "shakespeare[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5528070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\nALL’S WELL THAT ENDS WELL\\n\\n\\nby William Shakespeare\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Rossillon. A room in the Countess’s palace.\\nScene II. Paris. A room in the King’s palace.\\nScene III. Rossillon. A Room in the Palace.\\n\\n\\nACT II\\nScene I. Paris. A room in the King’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Paris. The King’s palace.\\nScene IV. Paris. The King’s palace.\\nScene V. Another room in the same.\\n\\n\\nACT III\\nScene I. Florence. A room in the Duke’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Florence. Before the Duke’s palace.\\nScene IV. Rossillon. A room in the Countess’s palace.\\nScene V. Without the walls of Florence.\\nScene VI. Camp before Florence.\\nScene VII. Florence. A room in the Widow’s house.\\n\\n\\nACT IV\\nScene I. Without the Florentine camp.\\nScene II. Florence. A room in the Widow’s house.\\nScene III. The Florentine camp.\\nScene IV. Florence. A room in the Widow’s house.\\nScene V. Rossillon. A room in the Countess’s palace.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "nvAMCBz1Ci9Q",
        "colab_type": "code",
        "outputId": "2b7746d5-3a2b-4c5e-fecc-f41c79b97986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "casanova = load_etext_from([casanova_id], lambda text: text.split('\\nCASANOVA AT DUX', 1)[-1]) # from main contents\n",
        "print(len(casanova))\n",
        "casanova[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6685264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n An Unpublished Chapter of History, By Arthur Symons\\n\\n I\\n The Memoirs of Casanova, though they have enjoyed the popularity of a bad reputation, have never had justice done to them by serious students of literature, of life, and of history. One English writer, indeed, Mr. Havelock Ellis, has realised that ‘there are few more delightful books in the world,’ and he has analysed them in an essay on Casanova, published in Affirmations, with extreme care and remarkable subtlety. But this essay stands alone, at all events in English, as an attempt to take Casanova seriously, to show him in his relation to his time, and in his relation to human problems. And yet these Memoirs are perhaps the most valuable document which we possess on the society of the eighteenth century; they are the history of a unique life, a unique personality, one of the greatest of autobiographies; as a record of adventures, they are more entertaining than Gil Blas, or Monte Cristo, or any of the imaginary travels, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "Zbur6dJQCjH6",
        "colab_type": "code",
        "outputId": "d48b5211-d92c-404a-f337-1d735c30244f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "clarissa = load_etext_from(clarissa_ids, lambda text: text.split('\\nTHE HISTORY OF CLARISSA HARLOWE', 1)[-1]) # from main contents\n",
        "print(len(clarissa))\n",
        "clarissa[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5173348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\nLETTER I\\n\\nMISS ANNA HOWE, TO MISS CLARISSA HARLOWE JAN 10.\\n\\n\\nI am extremely concerned, my dearest friend, for the disturbances that\\nhave happened in your family. I know how it must hurt you to become\\nthe subject of the public talk: and yet, upon an occasion so generally\\nknown, it is impossible but that whatever relates to a young lady, whose\\ndistinguished merits have made her the public care, should engage every\\nbody's attention. I long to have the particulars from yourself; and of\\nthe usage I am told you receive upon an accident you could not help; and\\nin which, as far as I can learn, the sufferer was the aggressor.\\n\\nMr. Diggs, the surgeon, whom I sent for at the first hearing of the\\nrencounter, to inquire, for your sake, how your brother was, told me,\\nthat there was no danger from the wound, if there were none from the\\nfever; which it seems has been increased by the perturbation of his\\nspirits.\\n\\nMr. Wyerley drank tea with us yesterday; and though he is far from being\\npartial to \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "rgleo2CnLhT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_global_chars_index(etexts):\n",
        "  chars = list(sorted(set(\"\\n\".join(etexts))))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "gchar, gchar_to_idx = get_global_chars_index([shakespeare,casanova,clarissa])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0nqz37H3ayW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_chars_index(etext):\n",
        "  chars = list(sorted(set(etext)))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "def generate_meta_from(etext, model_name, chunk_size=160):\n",
        "  etext_meta = {}\n",
        "  etext_meta['model_name'] = model_name\n",
        "  etext_meta['char'], etext_meta['char_to_idx'] = gchar, gchar_to_idx\n",
        "  etext_meta['chunk_size'] = chunk_size\n",
        "  return etext_meta\n",
        "\n",
        "\n",
        "shakespeare_meta = generate_meta_from(shakespeare, 'shakespeare')\n",
        "casanova_meta = generate_meta_from(casanova, 'casanova')\n",
        "clarissa_meta = generate_meta_from(clarissa, 'clarissa')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIG2LwgW3ayY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def char_rnn_model(chunk_size, num_chars, num_layers, num_nodes=512, dropout=0.1):\n",
        "    input = Input(shape=(chunk_size, num_chars), name='input')\n",
        "    prev = input\n",
        "    for i in range(num_layers):\n",
        "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
        "        if dropout:\n",
        "            prev = Dropout(dropout)(lstm)\n",
        "        else:\n",
        "            prev = lstm\n",
        "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
        "    model = Model(inputs=[input], outputs=[dense])\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33BNza2wwZHX",
        "colab_type": "code",
        "outputId": "0561a2ab-c029-41e3-e05c-4894b312239c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "# prediction from saved weights\n",
        "!ls -al /content/gutenberg_data/models/rel\n",
        "\n",
        "basepath = '/content/gutenberg_data/models/rel/'\n",
        "\n",
        "def load_model(basepath, model_meta):\n",
        "  model = char_rnn_model(None, len(model_meta['char']), num_layers=2, num_nodes=640, dropout=0) \n",
        "  model.load_weights(basepath + \"\" + model_meta['model_name'] + '_weights.h5')\n",
        "  return model\n",
        " \n",
        "prediction_models = {\n",
        "    'shakespeare' : load_model(basepath, shakespeare_meta),\n",
        "    'casanova' : load_model(basepath, casanova_meta),\n",
        "    'clarissa' : load_model(basepath, clarissa_meta),\n",
        "}\n",
        "#prediction_model = load_model(basepath, shakespeare_meta)  \n",
        "#prediction_model = load_model(basepath, casanova_meta)\n",
        "#prediction_model = load_model(basepath, clarissa_meta)\n",
        "prediction_models['shakespeare'].summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 123384\n",
            "-rw------- 1 root root 21057072 Mar 13 07:57 casanova.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 casanova.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:57 casanova_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 04:23 clarissa.h5\n",
            "-rw------- 1 root root     1293 Mar 13 04:23 clarissa.json\n",
            "-rw------- 1 root root 21055728 Mar 13 04:24 clarissa_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 07:56 shakespeare.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 shakespeare.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:56 shakespeare_weights.h5\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 106)         0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1912320   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, None, 106)         67946     \n",
            "=================================================================\n",
            "Total params: 5,259,626\n",
            "Trainable params: 5,259,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ffQ8tgF3ayj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_output(model, training_text, model_meta, start_index=None, diversity=None, amount=400):\n",
        "    CHUNK_SIZE = model_meta['chunk_size']\n",
        "    if start_index is None:\n",
        "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
        "    print(\"start_index : %s\" % start_index)\n",
        "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
        "    yield generated + '#'\n",
        "    for i in range(amount):\n",
        "        x = np.zeros((1, len(generated), len(model_meta['char'])))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, model_meta['char_to_idx'][char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        if diversity is None:\n",
        "            next_index = np.argmax(preds[len(generated) - 1])\n",
        "        else:\n",
        "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            next_index = np.argmax(probas)     \n",
        "        next_char = model_meta['char'][next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "    return generated\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1kawo928ZwD",
        "colab_type": "code",
        "outputId": "f4c1655d-b6ab-4a38-fddb-415e25ed86a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['casanova'], casanova, model_meta=casanova_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. #“When you are a man of an excellent sovereign is the prophet are the same.”\n",
            "\n",
            "“I am sorry to hear it, but it is a principle to the contrary.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrsOPjPcy1R_",
        "colab_type": "code",
        "outputId": "e7c90891-8e87-4c2b-8050-e4f50ca45d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['shakespeare'], casanova, model_meta=shakespeare_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. #I know his\n",
            "report that have been in love to a full assault.\n",
            "\n",
            "SIR ANDREW.\n",
            "I’ faith, sir, if you will not love her, that I might say\n",
            "that he comes to see your lordship to be the wiser man.\n",
            "\n",
            "FIRST LORD.\n",
            "Why, then, you are a fool to this end of a woman.\n",
            "\n",
            "HAMLET.\n",
            "I would I had to speak of thee; thou art a gentleman of a constancy; there is no\n",
            "that said ‘As e’er as doth the death of the distinction of her eye, in pure end\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TFabEnaTDv0s",
        "colab_type": "code",
        "outputId": "e72b3fe9-62d6-49d2-c840-74f57f4d0c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['clarissa'], casanova, model_meta=clarissa_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. # I wish they were in a concert with\n",
            "the contents of the letters I had left with her.\n",
            "\n",
            "The women went up to Hampstead and the widow Bevis, in the same moment thou hast read.\n",
            "\n",
            "The widow Bevis, in her letter to the Colonel, what a devil had she to do with the\n",
            "women and the lady could hardly stand in the same pacified; and that she had not been\n",
            "able to show it to her.\n",
            "\n",
            "But the people at Hampstead, she in the same moment to her chair, and begged her\n",
            "pardon; and which she could not be satisfied, that she was always the most\n",
            "injured of her sex, and of her own conveniencies for suffering the matter to a man so often intended.\n",
            "\n",
            "You see, my dear, I have not the least doubt but that you will not think me a\n",
            "disapprobation of my contrivance, and the persons of the second patterns as if to observe the design\n",
            "of the family, and to the pure we have been of late to be engaged for her.\n",
            "\n",
            "The widow Bevis, as in the morning, I think the man has a man of honour to any\n",
            "body.  He was so ill on Wednesday night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JinhZH01zIDx",
        "colab_type": "code",
        "outputId": "1429a472-f946-492a-e89c-6bee1a54381a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l /content/gutenberg_data/models/rel/ \n",
        "\n",
        "import json\n",
        "py_code_meta = json.loads(open(basepath + 'py_code.json').read())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 164287\n",
            "-rw------- 1 root root 21057072 Mar 13 07:57 casanova.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 casanova.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:57 casanova_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 04:23 clarissa.h5\n",
            "-rw------- 1 root root     1293 Mar 13 04:23 clarissa.json\n",
            "-rw------- 1 root root 21055728 Mar 13 04:24 clarissa_weights.h5\n",
            "-rw------- 1 root root 20941868 Mar 13 11:21 py_code.h5\n",
            "-rw------- 1 root root     1019 Mar 13 11:21 py_code.json\n",
            "-rw------- 1 root root 20940532 Mar 13 11:21 py_code_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 07:56 shakespeare.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 shakespeare.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:56 shakespeare_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B6GQF56xyser",
        "colab_type": "code",
        "outputId": "a32fcd01-945c-4a18-ff71-01d64e0edac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = char_rnn_model(None, len(py_code_meta['chars']), num_layers=2, num_nodes=640, dropout=0) \n",
        "py_model.load_weights('/content/gutenberg_data/models/rel/py_code_weights.h5')\n",
        "py_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 97)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, None, 97)          62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OdT3K2yW1cCJ",
        "colab_type": "code",
        "outputId": "907ebba1-80ff-4db7-cc61-0bcef91e419d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3695
        }
      },
      "cell_type": "code",
      "source": [
        "py_chars = py_code_meta['chars']\n",
        "\n",
        "def generate_code(model, start_with='\\ndef ', end_with='\\n\\n', diversity=1.0):\n",
        "    generated = start_with\n",
        "    yield generated\n",
        "    for i in range(2000):\n",
        "        x = np.zeros((1, len(generated), len(py_chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, py_char_to_idx[char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        \n",
        "        preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        next_index = np.argmax(probas)        \n",
        "        next_char = py_chars[next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "        if generated.endswith(end_with):\n",
        "            break\n",
        "\n",
        "st = ''\n",
        "for i in range(20):\n",
        "    for ch in generate_code(py_model):\n",
        "        sys.stdout.write(ch)\n",
        "        st += ch\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "def get_loader(source):\n",
            "    if source.file:\n",
            "     '\\n'\n",
            "        'MSG'\n",
            "       ma = arg.__get__(other)\n",
            "    if args is Npn\n",
            "    _setmode = fr'et and '__booleanumd'\n",
            "\n",
            "\n",
            "\n",
            "def _exc_info():\n",
            "    if type(exc_value._exc_value) in types_map.items():\n",
            "        support.TestCases(.need_dir, method_na)\n",
            "\n",
            "\n",
            "\n",
            "def execvpe(file, args, env):\n",
            "    \n",
            "  8  if getattem\" ='      \n",
            "    return (Gvi.)\n",
            "\n",
            "\n",
            "\n",
            "def _recursive_match_tests(suite):\n",
            "    \"\" sep' + \" \".join(_int * _normal))\n",
            "\n",
            "\n",
            "\n",
            "def pow (text=None,\n",
            "        indent=None, s = None):\n",
            "  _valid =LTYPE = _textange(text)\n",
            "    if width > 1:\n",
            "options.cateaddrags(tt)\n",
            "    recosf\n",
            "    re\n",
            "    if len(options) == 1 and opt[-1] == '\\n':\n",
            "        _compile(source, new\n",
            "\n",
            "\n",
            "\n",
            "def _checkvar(b):\n",
            "    ret.appeGentries()\n",
            "\n",
            "\n",
            "\n",
            "def _perm1(found_dif >unitialspace): \n",
            "    \"link:event\n",
            "else: (\"Acture\", cntu, key)\n",
            "      except    \n",
            "    n = len(a)\n",
            " asr\" = _dep(u\"\\n\", \" \", \"< \" or \")\n",
            "\n",
            "\n",
            "\n",
            "def t raise  StopIteration(int, floatcasI(0):    \n",
            "   osname = \" \"\n",
            "    for _name in __EYPO : FAS_USE_Eq | _dict_  = _UNDONleace\n",
            "\n",
            "\n",
            "\n",
            "def show_member_(caelense, the_module.handler):\n",
            "    return _call(\"font\", \"transfercmd\", \n",
            "    \" at xml\"    : self._start(th = attror.se0)\n",
            "    at: 0 = 25\n",
            "    return _date,sohe.Fp, nextTest\n",
            "\n",
            "\n",
            "\n",
            "def below( io: 0, 0)\n",
            "        try:\n",
            "            uid = _get_sup (u - (-1),            ), repr(root) - id)\n",
            "    V'                             'kz20:        \n",
            "    'mn_c ':     , 'c + 'cp1fAque'|noc7nifi : 'h:'}\n",
            "        if h.startswith('_'):\n",
            "        base = _resource_sharer.readlines( all_callees)\n",
            " self._cache._depcache.u_Code = codepcode\n",
            "        emit(FAILURE)\n",
            "\n",
            "\n",
            "\n",
            "def setlocale(cre0, 1, file=None) :\n",
            "    \"\"le \"==\"\\n\"\\n\"\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "def _is_gdict_entry():\n",
            "    frist = f\n",
            "    i = 0\n",
            "pythonou = 255\n",
            "events.pop('extensionAsynceop'))\n",
            "\n",
            "\n",
            "\n",
            "def _all_tasks - 1 = n\n",
            "    date_fset = HUNKS\n",
            "    for inf in monthname:\n",
            "  .         dayfiee = min(len, ben=tlan + aof) * self._decr(d)\n",
            "            self._datalength = self._dact[key]\n",
            "            del self._obj[openen:]\n",
            "        return buf\n",
            "    msg = \"MSG\"\n",
            "    raise ReadError(msg)\n",
            "\n",
            "\n",
            "\n",
            "def ioro = 0\n",
            "    for c in \"spacing\":\n",
            "        if c == \",\":\n",
            "     except UnicodeEet in     subprogy_mappen = syrScriptGet\n",
            "\n",
            "\n",
            "\n",
            "def _format_time(pt / 1024):\n",
            "    prefix = []\n",
            " notlib.rel_e[0] = value\n",
            "\n",
            "\n",
            "\n",
            "def failfast(m = Fahall(['g', '],', ')', ]\n",
            "\n",
            "\n",
            "\n",
            "def split(pathnametover):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    nametorol = None\n",
            "    \n",
            "    n:\n",
            "        host, setUp = selection_manager\n",
            " not =  (houndary, '\\n'.ppa for fn in contents.\n",
            "        whenName = \"Dict\"\n",
            "        total_sent = self._effectleft\n",
            "string = ma.__ftper b \n",
            "\n",
            "\n",
            "\n",
            "def _ch i'                           _flags_qection[0]\n",
            "                        extra += 3\n",
            "                   update_tryorder\n",
            "                    cocher_name += ascii_key\n",
            "    ]\n",
            "                if chn:\n",
            "               MMLSocket  = exc        \n",
            "            except Exception amb d =  \n",
            "            else:\n",
            "   .            if rend in handlerL: regl_te = False\n",
            "                rle = s\n",
            "            if e:\n",
            "     column_names = start.strip()\n",
            "        if use\" and count != 0:\n",
            "            retu )\n",
            "\n",
            "\n",
            "\n",
            "def _format_lif\n",
            "pasMa\n",
            "import bzb, cepta\n",
            "  alias = uni2top[0]\n",
            "    if isinstance(b, (bytes, bytearray)ither):\n",
            "        do('ug', 'ptarfile=dest'ram.pedbound1udd2530',\n",
            "        encode = encode + eol_encoding.encoding\n",
            "  . SimpleXMLR = ()\n",
            "    assert_(\"PAIR if v       \n",
            "    _HRENU_CON.__dict__['__getitem__'] == 'INTROSPECTaTX':\n",
            "    '='   STAR '            |uff Slgin len(entry)-1\n",
            "    ReferdictedTrans  elif context.Etiny() - len(respo + 5 < self.minute):\n",
            "        \n",
            "        self.pos = (len(start), pid))\n",
            " extra:\n",
            "  av = ANYaddL (tkinse_t (label, txn))\n",
            "    pl = p            \n",
            "    p = []\n",
            "    p2c = '/'\n",
            "    arb = _bootstrap._limbo TypeTaVent(__version__)\n",
            "eboordir = _global_emc \n",
            "\n",
            "\n",
            "\n",
            "def _coerce_result(*amount):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    import gdaln else dirs\n",
            "    if not os.pa.exp and hasatt and\n",
            "            not enable_gooc and hasattr(result, 'replace') and\n",
            "             types.nuistines.get(elts) == len(exte base, ext - tp,tr = 1M keywords.netscr = ARCHIVE_FORMATS[format][0]\n",
            "        arg = opcode.argin oragr = memo.getopt\n",
            "        oldvalue = token\n",
            "        if vand == 0:\n",
            "            \n",
            "            element.msg( 'MSG' % type_num)\n",
            "            token, vrelease, version = sys.ved or value\n",
            "        unexpected.append(e)\n",
            "    te = got0.readlines()\n",
            "    r xoff = 0\n",
            "    result = []\n",
            "    done, gettext = t._get_unixfrom (obj)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4aXCikNh3ay7",
        "colab_type": "code",
        "outputId": "a36ac25c-954a-49ca-b604-ee2f4f1110d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 512\n",
        "\n",
        "# flat_model = char_rnn_model(160, len(py_chars), num_layers=1, num_nodes=512, dropout=0)\n",
        "# flat_model = tf.contrib.tpu.keras_to_tpu_model(flat_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#     tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
        "\n",
        "# early = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "#                               min_delta=0.03,\n",
        "#                               patience=3,\n",
        "#                               verbose=0, mode='auto')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.65.67.106:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11502498185262799519)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11588889170251908459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15484636259604477500)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6188258309789351709)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17618968535966954990)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1368695216754022451)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 896554020254126233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12401001441761719750)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5652057927253700336)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17090350113839668390)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 17711765655739407088)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(64,), dtype=tf.int32, name='core_id_70'), TensorSpec(shape=(64, 160, 97), dtype=tf.float32, name='input_190'), TensorSpec(shape=(64, 160, 97), dtype=tf.float32, name='time_distributed_9_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 21.852776288986206 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 93s - loss: 3.4410 - acc: 0.3380\n",
            "Epoch 2/40\n",
            " - 51s - loss: 2.0958 - acc: 0.5072\n",
            "Epoch 3/40\n",
            " - 53s - loss: 1.2896 - acc: 0.6872\n",
            "Epoch 4/40\n",
            " - 51s - loss: 1.0748 - acc: 0.7407\n",
            "Epoch 5/40\n",
            " - 52s - loss: 0.9810 - acc: 0.7633\n",
            "Epoch 6/40\n",
            " - 51s - loss: 0.9108 - acc: 0.7794\n",
            "Epoch 7/40\n",
            " - 52s - loss: 0.9775 - acc: 0.7741\n",
            "Epoch 8/40\n",
            " - 52s - loss: 0.8688 - acc: 0.7925\n",
            "Epoch 9/40\n",
            " - 51s - loss: 0.8615 - acc: 0.7959\n",
            "Epoch 10/40\n",
            " - 52s - loss: 0.8670 - acc: 0.7973\n",
            "Epoch 11/40\n",
            " - 51s - loss: 0.8627 - acc: 0.7993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96e767fcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "qYPG1iuyooEP",
        "colab_type": "code",
        "outputId": "da923fb3-17e4-4ac2-e32f-37dbfc040f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "cell_type": "code",
      "source": [
        "# flat_model.fit_generator(\n",
        "#     data_generator(python_code, py_char_to_idx, batch_size=BATCH_SIZE, chunk_size=160),\n",
        "#     epochs=40,\n",
        "#     callbacks=[early,],\n",
        "#     steps_per_epoch=int(2 * len(python_code) / (BATCH_SIZE * 160)),\n",
        "#     verbose=2\n",
        "# )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " - 53s - loss: 0.8566 - acc: 0.8016\n",
            "Epoch 2/40\n",
            " - 52s - loss: 0.8543 - acc: 0.8025\n",
            "Epoch 3/40\n",
            " - 52s - loss: 0.8442 - acc: 0.8046\n",
            "Epoch 4/40\n",
            " - 52s - loss: 0.8321 - acc: 0.8079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96e89b8940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "Nt_fTNfv3ay9",
        "colab_type": "code",
        "outputId": "8d272dca-6a5a-4845-cc3e-ae7d7099049c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "example_code = 'if a == 2:\\n    b=1\\nelse:\\n    b=2\\n'\n",
        "#example_code = 'a=(2 * 3)\\nb=(4 * 6 + 7)\\nreturn C'\n",
        "\n",
        "py_char_to_idx = py_code_meta['char_to_idx']\n",
        "\n",
        "def activations(model, code):\n",
        "    x = np.zeros((1, len(code), len(py_char_to_idx)))\n",
        "    for t, char in enumerate(code):\n",
        "        x[0, t, py_char_to_idx[char]] = 1.\n",
        "    output = model.get_layer('lstm_layer_1').output\n",
        "    f = K.function([model.input], [output])\n",
        "    return f([x])[0][0]\n",
        "\n",
        "act = activations(py_model, example_code)\n",
        "act.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33, 640)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "oUXD8JDI3ay-",
        "colab_type": "code",
        "outputId": "1c3f8c4e-2555-4ea1-aa3f-6311afd4d5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "def interesting_neurons(act):\n",
        "    res = []\n",
        "    for n in np.argmax(act, axis=1):\n",
        "        if not n in res:\n",
        "            res.append(n)\n",
        "    return res\n",
        "\n",
        "neurons = interesting_neurons(act)\n",
        "len(neurons)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "Mh2pwk-Z3azA",
        "colab_type": "code",
        "outputId": "2b371241-b92f-4680-9bd2-bed3882beab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "cell_type": "code",
      "source": [
        "def visualize_neurons(neurons, code, act, cell_size=12):\n",
        "    img = np.full((len(neurons) + 1, len(code), 3), 128)\n",
        "    scores = (act[:, neurons].T + 1) / 2\n",
        "    img[1:, :, 0] = 255 * (1 - scores)\n",
        "    img[1:, :, 1] = 255 * scores\n",
        "\n",
        "    f = BytesIO()\n",
        "    img = scipy.misc.imresize(img, float(cell_size), interp='nearest')\n",
        "    pil_img = PIL.Image.fromarray(img)\n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    for idx, ch in enumerate(code):\n",
        "        draw.text((idx * cell_size + 2, 0), ch)\n",
        "    pil_img.save(f, 'png')\n",
        "    return Image(data=f.getvalue())\n",
        "\n",
        "img = visualize_neurons(neurons, example_code, act)\n",
        "display(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAE4CAIAAADHAFN+AAAO0UlEQVR4nO3dW4xddR3F8Tmd07l1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\naJQUgGiUFIBo/wXa3mx1qKTosAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BR6lL2qY3azC",
        "colab_type": "code",
        "outputId": "975bd1a3-6c4b-4732-95fd-55d725b14045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "cell_type": "code",
      "source": [
        "def image_for_code(code):\n",
        "    act = activations(py_model, code)\n",
        "    neurons = interesting_neurons(act)\n",
        "    return visualize_neurons(neurons, code, act)\n",
        "\n",
        "display(image_for_code('if (a == 2) and ((b == 1) or (c==2)):'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAGMCAIAAAAuqIAlAAAUkklEQVR4nO3dW4xcB3kH8Bnv7Hrt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\n/wBt6QAuvGF4wwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HWcagIrR3azF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "code = 'if (a == 2) and ((b == 1) or (c==2)):'\n",
        "mask = '   ________     ____________________ '\n",
        "act = activations(py_model, code)\n",
        "positive = [idx for idx, ch in enumerate(mask) if ch == '_']\n",
        "negative = [idx for idx, ch in enumerate(mask) if ch != '_']\n",
        "\n",
        "neurons = np.argsort(act[positive].sum(axis=0) - act[negative].sum(axis=0))[-5:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SO9Uey363azG",
        "colab_type": "code",
        "outputId": "8bcaef08-2a94-4e54-814a-6754aaeba0d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "img = visualize_neurons(neurons, code, act)\n",
        "display(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAABICAIAAAACptczAAAFWElEQVR4nO3d34tUZRzH8dndmR0W\nW9NFzJKoTBZXDLFQAyOLMAyiHxfdBEEE/RVxbmbov4iILrrpxiJCkiijopSKylpZFi3Cfois6Srb\nruvgxdIwu+p8Pt89z549s7xft/vZOc853/N85zjrM09fo9GoAAA8/as9AADoJYuaZpZlWZZ1/4Xs\nf/mPneRFSsW5OCU864Jr2iWTahjF68W6m8oz8lTzK+cZLWqazWaz2WzKg8mYI8uy/C9SKu2LU+la\nlWazWZ67cEGqQjg1XZN1dwpawro7ylOvhPMrZy2qnQNqv+Ito+1M5+iXJ/RukOpA8rxyHiv0u/nv\nRXl9Fg4hi5VqJid8h097gxVTd//UCqh7ZXFfWPYRk8zTVLVIPr+WXYtq54C6XyNnBqbSWfg895nz\ni2lPp5hLZF6fdt+8XebmCVYGvVj3Yjh1b/+0svKVNccjX8evRfEt6OYfVVf62MvTWfg8inziqBRY\n0VTXZ60quO7ltHBSxZya+U+ZnLUobH51P0pJm2aq98kinziWjHlFS1vOJ8TyWKtPmqXV5Z5PVYsi\n51d3/e0BdX5kecupKAM+84SXfIraE8yhJil5/uuz5GOZPNc5YZ9a9VkR4sydtpwn1Vkv828P+Q+X\nczxpJZxf8o/etztW3yr+5/b8H4r3orV91iv6AXxP692z7q2RF3AHrmbTBICew4ogAAiotlo6NLVR\nZ2brOnP33zrj+OSQzmyf1JmpEZ3pN67P2LjOXKvpTO2azgz9pzN/bdGZx77UmS8e15ktRk0HjGvo\n+Ga/zoxO6My5rTrTMh4ndpzWmVTqczrzz2ad+XWnzjz1qc60BnTm3Vd0ZvePOjMzpDP3/KkzG/7V\nmfXTOsOTJgAE0DQBIICmCQABNE0ACKBpAkAATRMAAmiaABBA0wSAAJomAATQNAEggKYJAAHWlxCP\nXNQZZ71zKk8f05nxHTrz80M6MzKlMw//oDPOmvEzD+jMtrM646zx/+hZnRkz1laf2Kszjn0ndebR\nb3Xm+z06c95Yo735vM4468Edzr3q1OIuY8xHXtCZoRmdcWqx38j8fp/O7DuhMxsu6Yzjyjqd4UkT\nAAJomgAQQNMEgACaJgAE0DQBIICmCQABNE0ACKBpAkAATRMAAmiaABBA0wSAgL6s0ZAhZ9/qqrFn\n94VNxuvM60x9VmcmRnVm2NjjeOs5nTm1S2d2/6Qzjg+e05nnP0xzLMfHz+iMs5bZ4ewzfvygzlw1\n1hfvMb5P4LSxZvyl93WmZtzznz2hM09+rjNTG3XmvZd1xrk+B77WmUFjLjvm6mlex8GTJgAE0DQB\nIICmCQABNE0ACKBpAkAATRMAAmiaABBA0wSAAJomAATQNAEggKYJAAHWvueO+ZrOOGvPz2zTGWff\n85mhNMe644rOjI/pjLNu2tEy3uaGL+vM9Pr8Y6lUKpVdp3Rm3rjLZo21w85+7vf+oTMD13VmbFxn\nzhr71Pcb39vgmNyuM87a85GLOnPwuM78dr/OOI4d0hlnLjt7ozvfgTBo7GXPkyYABNA0ASCApgkA\nATRNAAigaQJAAE0TAAJomgAQQNMEgACaJgAE0DQBIICmCQAB1tpzZ09zZ+25sy7Y2U/ZWTftrGU+\n8JXO3HlJZ2rG9XnnVZ3Ze1JnXjyiM871+WWnzoxO6MzcoM4463md7yVwOOvKnfvQGbNzb7z9ms44\n30vw+ls6M/mgzgxP68ymCzpz9LDOHD6qM498pzN1Y2/0N9/QmXVXdcbZp54nTQAIoGkCQABNEwAC\naJoAEEDTBIAAmiYABNA0ASCApgkAATRNAAigaQJAAE0TAAJuAEL/NJWYasOmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jkkrYn1O3azI",
        "colab_type": "code",
        "outputId": "bd8dfd74-41fe-4b27-db77-df300f11b5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "neurons"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([237, 117,  52, 197, 209])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "ty2yBySh3azK",
        "colab_type": "code",
        "outputId": "154655bd-55cf-4fd8-f634-2a8971ceb50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "act[negative, 108].sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03596913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "vwu-uQFW3azM",
        "colab_type": "code",
        "outputId": "e6682f3f-114d-4a21-929a-be822958559f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "x0 = 0\n",
        "x1 = 0\n",
        "for idx, ch in enumerate(mask):\n",
        "    if ch == '_':\n",
        "        x0 += act[idx, 108]\n",
        "    else:\n",
        "        x1 += act[idx, 108]\n",
        "x0, x1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3232587524689734, 0.035969129181467)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "kft-cxL23azP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}