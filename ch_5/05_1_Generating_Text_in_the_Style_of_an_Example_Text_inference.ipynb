{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.1 Generating Text in the Style of an Example Text - inference.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7yWqdplWgoFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "colab 에서 gutenberg 패키지를 사용하기 위해서는, libdb5.3-dev 선행 설치가 필요하다."
      ]
    },
    {
      "metadata": {
        "id": "PGLS_ZOe5Vnf",
        "colab_type": "code",
        "outputId": "3e140a6b-f6f8-4fe5-b2bc-6fedf9a91d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "!sudo apt install libdb5.3-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  db5.3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libdb5.3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 762 kB of archives.\n",
            "After this operation, 3,146 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdb5.3-dev amd64 5.3.28-13.1ubuntu1 [762 kB]\n",
            "Fetched 762 kB in 3s (261 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdb5.3-dev.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../libdb5.3-dev_5.3.28-13.1ubuntu1_amd64.deb ...\n",
            "Unpacking libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n",
            "Setting up libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiCi2V2G3bUq",
        "colab_type": "code",
        "outputId": "6299f3c6-fdec-4dcc-836b-8ce5e82a70cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install gutenberg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (40.8.0)\n",
            "Collecting gutenberg\n",
            "  Downloading https://files.pythonhosted.org/packages/14/b1/6e99867c38e70d46366966a0a861c580377f38312cf9dbad38b82ed1823d/Gutenberg-0.7.0.tar.gz\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/fc/ebfbd4de236b493f9ece156f816c21df0ae87ccc22604c5f9b664efef1b9/bsddb3-6.2.6.tar.gz (239kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (0.16.0)\n",
            "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/a2/bc580a51ac1f9680aa04da4b6e96d499903d6e606d2f78f02e73527799da/rdflib_sqlalchemy-0.3.8-py3-none-any.whl\n",
            "Collecting rdflib>=4.2.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 25.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (2.18.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (40.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (1.11.0)\n",
            "Collecting alembic>=0.8.8 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bb/ec1e21f2e303689ad2170eb47fc67df9ad4199ade6759a99474c4d3535c8/alembic-1.0.8.tar.gz (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.1.4 in /usr/local/lib/python3.6/dist-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.18)\n",
            "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.2.0->gutenberg) (2.3.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2018.11.29)\n",
            "Collecting Mako (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f3/67579bb486517c0d49547f9697e36582cd19dafb5df9e687ed8e22de57fa/Mako-1.0.7.tar.gz (564kB)\n",
            "\u001b[K    100% |████████████████████████████████| 573kB 26.2MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3 (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.1.1)\n",
            "Building wheels for collected packages: gutenberg, bsddb3, alembic, Mako\n",
            "  Building wheel for gutenberg (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8e/cd/75/4bc6f16541a1b7a69b02168da567695b2271c23ac4a0a0a453\n",
            "  Building wheel for bsddb3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/11/b8/b3/fa84db10bf8c563e4ba1a72837a0946d123f12adb34b164bf5\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/bc/74/834fa0c75c4ae6d6718db5e65187d508623ee291dead032156\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/35/25/dbcb848832ccb1a4b4ad23f529badfd3bce9bf88017f7ca510\n",
            "Successfully built gutenberg bsddb3 alembic Mako\n",
            "Installing collected packages: bsddb3, Mako, python-editor, alembic, isodate, rdflib, rdflib-sqlalchemy, gutenberg\n",
            "Successfully installed Mako-1.0.7 alembic-1.0.8 bsddb3-6.2.6 gutenberg-0.7.0 isodate-0.6.0 python-editor-1.0.4 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_WCvUA0zQGpd",
        "colab_type": "code",
        "outputId": "bb1f2263-d3f2-45b6-c8e6-6a40c0d73c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-AJE6QUWg4AE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "시간 절약을 위해, gutenberg db file 들을 미리 다운로드 받아놓고, google drive 를 mount 하여 연결한다.\n",
        "\n",
        "* gutenberg db file 다운로드: https://drive.google.com/drive/folders/1Y6nMqJ-srDfflTuoniVdEzdWfkrShe4T\n",
        "\n",
        "*   gutenberg db file  기본 경로 : ~/gutenberg_data/metadata/metadata.db\n",
        "\n",
        "colab 상에서는, /content/gutenberg_data/metadata/metadata.db 에 위치하면 됨"
      ]
    },
    {
      "metadata": {
        "id": "fVrbBx3IQvqa",
        "colab_type": "code",
        "outputId": "12f5a05d-15b1-4bf2-ac15-47ab235fe407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lx3_jzJdR-W4",
        "colab_type": "code",
        "outputId": "abe89b2a-2d4c-437d-ba83-54e1330b2b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# unmount 를 원할 시 실행\n",
        "\n",
        "#!fusermount -u gdrive\n",
        "#!rmdir gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fusermount: failed to unmount /content/gutenberg_data: Invalid argument\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCz5DhOmg_8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "/content/gdrive 에 google drive 를 mount 하고,\n",
        "\n",
        " 구글 드라이브 상의 /gutenberg/gutenberg_data 를 \n",
        " /content/gutenberg_data 위치로 symbolic link 연결한다\n"
      ]
    },
    {
      "metadata": {
        "id": "gZcDcY0xWPXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4888d04-85c0-4935-8153-ba199986d146"
      },
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/gutenberg/gutenberg_data /content/gutenberg_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ln: failed to create symbolic link '/content/gutenberg_data/gutenberg_data': Function not implemented\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "36ZYgHb_VjKx",
        "colab_type": "code",
        "outputId": "58e4d589-105b-4c91-9a93-22fc04461e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al /content/gutenberg_data/metadata/metadata.db"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3697408\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  contexts\n",
            "-rw------- 1 root root 909377536 Mar  2 03:05 'c^o^s^p^'\n",
            "-rw------- 1 root root 942211072 Mar  2 03:05 'c^p^o^s^'\n",
            "-rw------- 1 root root 915693568 Mar  2 03:06 'c^s^p^o^'\n",
            "-rw------- 1 root root    999424 Mar  2 03:05  __db.001\n",
            "-rw------- 1 root root   1441792 Mar  2 03:06  __db.002\n",
            "-rw------- 1 root root  65544192 Mar  2 03:06  __db.003\n",
            "-rw------- 1 root root 285048832 Mar  2 03:06  i2k\n",
            "-rw------- 1 root root 665780224 Mar  2 03:06  k2i\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  namespace\n",
            "-rw------- 1 root root     16384 Mar  2 03:05  prefix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rU6lpK_33ayM",
        "colab_type": "code",
        "outputId": "95b06aa8-8f73-4262-e198-f79b250e9488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    GUTENBERG = True\n",
        "    from gutenberg.acquire import load_etext\n",
        "    from gutenberg.query import get_etexts, get_metadata\n",
        "    from gutenberg.acquire import get_metadata_cache\n",
        "    from gutenberg.acquire.text import UnknownDownloadUriException\n",
        "    from gutenberg.cleanup import strip_headers\n",
        "    from gutenberg._domain_model.exceptions import CacheAlreadyExistsException\n",
        "except ImportError:\n",
        "    GUTENBERG = False\n",
        "    print('Gutenberg is not installed. See instructions at https://pypi.python.org/pypi/Gutenberg')\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import tensorflow.keras.callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "import scipy.misc\n",
        "import json\n",
        "\n",
        "import os, sys\n",
        "import re\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "try:\n",
        "    from io import BytesIO\n",
        "except ImportError:\n",
        "    from StringIO import StringIO as BytesIO"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXsvn2uLhJoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "미리 db 데이터를 넣어 놓아도,  cache.populate() 시 메타 데이타 땡겨오는데 시간이 꽤 걸린다.\n",
        "\n",
        "꼼수가 있는데, 애로 코드블럭을 run 하고 바로 stop 하면 메타데이타(get_metadata) 검색은 잘 안되지만,\n",
        "\n",
        "\n",
        "실제 텍스트 데이터 로드(load_etext) 는 잘 실행된다."
      ]
    },
    {
      "metadata": {
        "id": "fnVGqiNJ3ayP",
        "colab_type": "code",
        "outputId": "d64b91c3-a5c1-4722-c92d-3b33b93e6683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1065
        }
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    cache = get_metadata_cache()\n",
        "    try:\n",
        "        cache.populate()\n",
        "    except CacheAlreadyExistsException as e:\n",
        "        print(e)\n",
        "        pass"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-70d9b2c78a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metadata_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCacheAlreadyExistsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gutenberg/acquire/metadata.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_metadata_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_metadata_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gutenberg/acquire/metadata.py\u001b[0m in \u001b[0;36m_download_metadata_archive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mmetadata_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qQqO7VR3hNxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "서적 ID를 기반으로 실제 서적 데이터를 땡겨온다.\n",
        "\n",
        "\n",
        "100 : 세익스피어 전집(희곡)\n",
        "\n",
        "2981 : 카사노바 회고집(에세이)\n",
        "\n",
        "[9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398] : clarissa (여러 편의 편지 형태)\n",
        "\n",
        "\n",
        "학습 텍스트는 원본에서 목차 등 불필요한 텍스트를 발라내게 된다. (text.split 부분)"
      ]
    },
    {
      "metadata": {
        "id": "8n00slgP6YGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shakespeare_id = 100\n",
        "casanova_id = 2981 \n",
        "clarissa_ids = [9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398]\n",
        "\n",
        "def load_etext_from(ids, filter_func):\n",
        "  etext = '\\n'.join([filter_func(strip_headers(load_etext(id))) \\\n",
        "                     for id in ids])\n",
        "  return etext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h43THvuk-F84",
        "colab_type": "code",
        "outputId": "cace9037-7366-4003-b464-272409e4bf26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "shakespeare = load_etext_from([shakespeare_id], lambda text: text.split('\\nTHE END', 1)[-1])\n",
        "print(len(shakespeare))\n",
        "shakespeare[:1000]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5528070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\nALL’S WELL THAT ENDS WELL\\n\\n\\nby William Shakespeare\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Rossillon. A room in the Countess’s palace.\\nScene II. Paris. A room in the King’s palace.\\nScene III. Rossillon. A Room in the Palace.\\n\\n\\nACT II\\nScene I. Paris. A room in the King’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Paris. The King’s palace.\\nScene IV. Paris. The King’s palace.\\nScene V. Another room in the same.\\n\\n\\nACT III\\nScene I. Florence. A room in the Duke’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Florence. Before the Duke’s palace.\\nScene IV. Rossillon. A room in the Countess’s palace.\\nScene V. Without the walls of Florence.\\nScene VI. Camp before Florence.\\nScene VII. Florence. A room in the Widow’s house.\\n\\n\\nACT IV\\nScene I. Without the Florentine camp.\\nScene II. Florence. A room in the Widow’s house.\\nScene III. The Florentine camp.\\nScene IV. Florence. A room in the Widow’s house.\\nScene V. Rossillon. A room in the Countess’s palace.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "nvAMCBz1Ci9Q",
        "colab_type": "code",
        "outputId": "cb82cdd2-f39c-48e7-9ef6-4db18721dae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "casanova = load_etext_from([casanova_id], lambda text: text.split('\\nCASANOVA AT DUX', 1)[-1]) # from main contents\n",
        "print(len(casanova))\n",
        "casanova[:1000]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6685264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n An Unpublished Chapter of History, By Arthur Symons\\n\\n I\\n The Memoirs of Casanova, though they have enjoyed the popularity of a bad reputation, have never had justice done to them by serious students of literature, of life, and of history. One English writer, indeed, Mr. Havelock Ellis, has realised that ‘there are few more delightful books in the world,’ and he has analysed them in an essay on Casanova, published in Affirmations, with extreme care and remarkable subtlety. But this essay stands alone, at all events in English, as an attempt to take Casanova seriously, to show him in his relation to his time, and in his relation to human problems. And yet these Memoirs are perhaps the most valuable document which we possess on the society of the eighteenth century; they are the history of a unique life, a unique personality, one of the greatest of autobiographies; as a record of adventures, they are more entertaining than Gil Blas, or Monte Cristo, or any of the imaginary travels, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Zbur6dJQCjH6",
        "colab_type": "code",
        "outputId": "3d6415a7-5c5f-453c-c81a-b137dbf27a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "clarissa = load_etext_from(clarissa_ids, lambda text: text.split('\\nTHE HISTORY OF CLARISSA HARLOWE', 1)[-1]) # from main contents\n",
        "print(len(clarissa))\n",
        "clarissa[:1000]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5173348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\nLETTER I\\n\\nMISS ANNA HOWE, TO MISS CLARISSA HARLOWE JAN 10.\\n\\n\\nI am extremely concerned, my dearest friend, for the disturbances that\\nhave happened in your family. I know how it must hurt you to become\\nthe subject of the public talk: and yet, upon an occasion so generally\\nknown, it is impossible but that whatever relates to a young lady, whose\\ndistinguished merits have made her the public care, should engage every\\nbody's attention. I long to have the particulars from yourself; and of\\nthe usage I am told you receive upon an accident you could not help; and\\nin which, as far as I can learn, the sufferer was the aggressor.\\n\\nMr. Diggs, the surgeon, whom I sent for at the first hearing of the\\nrencounter, to inquire, for your sake, how your brother was, told me,\\nthat there was no danger from the wound, if there were none from the\\nfever; which it seems has been increased by the perturbation of his\\nspirits.\\n\\nMr. Wyerley drank tea with us yesterday; and though he is far from being\\npartial to \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "rgleo2CnLhT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_global_chars_index(etexts):\n",
        "  chars = list(sorted(set(\"\\n\".join(etexts))))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "gchar, gchar_to_idx = get_global_chars_index([shakespeare,casanova,clarissa])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0nqz37H3ayW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_chars_index(etext):\n",
        "  chars = list(sorted(set(etext)))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "def generate_meta_from(etext, model_name, chunk_size=160):\n",
        "  etext_meta = {}\n",
        "  etext_meta['model_name'] = model_name\n",
        "  etext_meta['char'], etext_meta['char_to_idx'] = gchar, gchar_to_idx\n",
        "  etext_meta['chunk_size'] = chunk_size\n",
        "  return etext_meta\n",
        "\n",
        "\n",
        "shakespeare_meta = generate_meta_from(shakespeare, 'shakespeare')\n",
        "casanova_meta = generate_meta_from(casanova, 'casanova')\n",
        "clarissa_meta = generate_meta_from(clarissa, 'clarissa')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIG2LwgW3ayY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def char_rnn_model(chunk_size, num_chars, num_layers, num_nodes=512, dropout=0.1):\n",
        "    input = Input(shape=(chunk_size, num_chars), name='input')\n",
        "    prev = input\n",
        "    for i in range(num_layers):\n",
        "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
        "        if dropout:\n",
        "            prev = Dropout(dropout)(lstm)\n",
        "        else:\n",
        "            prev = lstm\n",
        "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
        "    model = Model(inputs=[input], outputs=[dense])\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33BNza2wwZHX",
        "colab_type": "code",
        "outputId": "f5ee82c7-9a92-4128-f5a3-38a847cb3f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "cell_type": "code",
      "source": [
        "# prediction from saved weights\n",
        "!ls -al /content/gutenberg_data/models/rel\n",
        "\n",
        "basepath = '/content/gutenberg_data/models/rel/'\n",
        "\n",
        "def load_model(basepath, model_meta):\n",
        "  model = char_rnn_model(None, len(model_meta['char']), num_layers=2, num_nodes=640, dropout=0) \n",
        "  model.load_weights(basepath + \"\" + model_meta['model_name'] + '_weights.h5')\n",
        "  return model\n",
        " \n",
        "prediction_models = {\n",
        "    'shakespeare' : load_model(basepath, shakespeare_meta),\n",
        "    'casanova' : load_model(basepath, casanova_meta),\n",
        "    'clarissa' : load_model(basepath, clarissa_meta),\n",
        "}\n",
        "#prediction_model = load_model(basepath, shakespeare_meta)  \n",
        "#prediction_model = load_model(basepath, casanova_meta)\n",
        "#prediction_model = load_model(basepath, clarissa_meta)\n",
        "prediction_models['shakespeare'].summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 164287\n",
            "-rw------- 1 root root 21057072 Mar 13 07:57 casanova.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 casanova.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:57 casanova_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 04:23 clarissa.h5\n",
            "-rw------- 1 root root     1293 Mar 13 04:23 clarissa.json\n",
            "-rw------- 1 root root 21055728 Mar 13 04:24 clarissa_weights.h5\n",
            "-rw------- 1 root root 20941836 Mar 14 07:06 py_code.h5\n",
            "-rw------- 1 root root     1019 Mar 14 07:06 py_code.json\n",
            "-rw------- 1 root root 20940492 Mar 14 07:06 py_code_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 07:56 shakespeare.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 shakespeare.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:56 shakespeare_weights.h5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 106)         0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1912320   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 106)         67946     \n",
            "=================================================================\n",
            "Total params: 5,259,626\n",
            "Trainable params: 5,259,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ffQ8tgF3ayj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_output(model, training_text, model_meta, start_index=None, diversity=None, amount=400):\n",
        "    CHUNK_SIZE = model_meta['chunk_size']\n",
        "    if start_index is None:\n",
        "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
        "    print(\"start_index : %s\" % start_index)\n",
        "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
        "    yield generated + '#'\n",
        "    for i in range(amount):\n",
        "        x = np.zeros((1, len(generated), len(model_meta['char'])))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, model_meta['char_to_idx'][char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        if diversity is None:\n",
        "            next_index = np.argmax(preds[len(generated) - 1])\n",
        "        else:\n",
        "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            next_index = np.argmax(probas)     \n",
        "        next_char = model_meta['char'][next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "    return generated\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8LYTrzwafNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "동일한 시작 문장을 각기 다른 모델에 부여하여 어투의 차이를 확인해 본다.\n",
        "\n",
        "(diversity 값 존재시 random pick 개념이 들어가 있어서, 이를 큰 값을 줄 수록 아무말 하는 경향이 생긴다)"
      ]
    },
    {
      "metadata": {
        "id": "G1kawo928ZwD",
        "colab_type": "code",
        "outputId": "f4c1655d-b6ab-4a38-fddb-415e25ed86a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['casanova'], casanova, model_meta=casanova_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. #“When you are a man of an excellent sovereign is the prophet are the same.”\n",
            "\n",
            "“I am sorry to hear it, but it is a principle to the contrary.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hear it, but I have never seen her before.”\n",
            "\n",
            "“I am sorry to hea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrsOPjPcy1R_",
        "colab_type": "code",
        "outputId": "e7c90891-8e87-4c2b-8050-e4f50ca45d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['shakespeare'], casanova, model_meta=shakespeare_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. #I know his\n",
            "report that have been in love to a full assault.\n",
            "\n",
            "SIR ANDREW.\n",
            "I’ faith, sir, if you will not love her, that I might say\n",
            "that he comes to see your lordship to be the wiser man.\n",
            "\n",
            "FIRST LORD.\n",
            "Why, then, you are a fool to this end of a woman.\n",
            "\n",
            "HAMLET.\n",
            "I would I had to speak of thee; thou art a gentleman of a constancy; there is no\n",
            "that said ‘As e’er as doth the death of the distinction of her eye, in pure end\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TFabEnaTDv0s",
        "colab_type": "code",
        "outputId": "e72b3fe9-62d6-49d2-c840-74f57f4d0c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['clarissa'], casanova, model_meta=clarissa_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. # I wish they were in a concert with\n",
            "the contents of the letters I had left with her.\n",
            "\n",
            "The women went up to Hampstead and the widow Bevis, in the same moment thou hast read.\n",
            "\n",
            "The widow Bevis, in her letter to the Colonel, what a devil had she to do with the\n",
            "women and the lady could hardly stand in the same pacified; and that she had not been\n",
            "able to show it to her.\n",
            "\n",
            "But the people at Hampstead, she in the same moment to her chair, and begged her\n",
            "pardon; and which she could not be satisfied, that she was always the most\n",
            "injured of her sex, and of her own conveniencies for suffering the matter to a man so often intended.\n",
            "\n",
            "You see, my dear, I have not the least doubt but that you will not think me a\n",
            "disapprobation of my contrivance, and the persons of the second patterns as if to observe the design\n",
            "of the family, and to the pure we have been of late to be engaged for her.\n",
            "\n",
            "The widow Bevis, as in the morning, I think the man has a man of honour to any\n",
            "body.  He was so ill on Wednesday night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JinhZH01zIDx",
        "colab_type": "code",
        "outputId": "adeaf987-a309-420b-a6d1-0af20362149d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l /content/gutenberg_data/models/rel/ \n",
        "\n",
        "\n",
        "basepath = '/content/gutenberg_data/models/rel/'\n",
        "\n",
        "import json\n",
        "py_code_meta = json.loads(open(basepath + 'py_code.json').read())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 164287\n",
            "-rw------- 1 root root 21057072 Mar 13 07:57 casanova.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 casanova.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:57 casanova_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 04:23 clarissa.h5\n",
            "-rw------- 1 root root     1293 Mar 13 04:23 clarissa.json\n",
            "-rw------- 1 root root 21055728 Mar 13 04:24 clarissa_weights.h5\n",
            "-rw------- 1 root root 20941836 Mar 14 08:04 py_code.h5\n",
            "-rw------- 1 root root     1019 Mar 14 08:04 py_code.json\n",
            "-rw------- 1 root root 20940492 Mar 14 08:04 py_code_weights.h5\n",
            "-rw------- 1 root root 21057072 Mar 13 07:56 shakespeare.h5\n",
            "-rw------- 1 root root     1293 Mar 13 07:56 shakespeare.json\n",
            "-rw------- 1 root root 21055728 Mar 13 07:56 shakespeare_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gL6OZjjOBsL3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "상기 TPU 예제와 동일하게, \n",
        "고정 chunk_size 모델의 weight을 저장, chunk_size=None 로  rnn 을 생성 후 저장된 weight 만 로드 하였다.\n",
        "\n",
        "(결과가 썩 좋지 않다)"
      ]
    },
    {
      "metadata": {
        "id": "B6GQF56xyser",
        "colab_type": "code",
        "outputId": "4e070537-a693-4711-ebcc-86e080299405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = char_rnn_model(None, len(py_code_meta['chars']), num_layers=2, num_nodes=640, dropout=0) \n",
        "py_model.load_weights(basepath + 'py_code_weights.h5')\n",
        "py_model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 97)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, None, 97)          62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OdT3K2yW1cCJ",
        "colab_type": "code",
        "outputId": "fbe7991b-581e-435c-ca0d-17a35f0f3a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4060
        }
      },
      "cell_type": "code",
      "source": [
        "py_chars = py_code_meta['chars']\n",
        "py_char_to_idx = py_code_meta['char_to_idx']\n",
        "\n",
        "def generate_code(model, start_with='\\ndef ', end_with='\\n\\n', diversity=1.0):\n",
        "    generated = start_with\n",
        "    yield generated\n",
        "    for i in range(2000):\n",
        "        x = np.zeros((1, len(generated), len(py_chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, py_char_to_idx[char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        \n",
        "        preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        next_index = np.argmax(probas)        \n",
        "        next_char = py_chars[next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "        if generated.endswith(end_with):\n",
        "            break\n",
        "\n",
        "st = ''\n",
        "for i in range(20):\n",
        "    for ch in generate_code(py_model):\n",
        "        sys.stdout.write(ch)\n",
        "        st += ch\n",
        "    print()\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "def _quoted_task bster, image, listformo in _sanit_results(os.fs             export_symbols=extra_args, context=True):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    tr = tk              \n",
            "            _func_flags = func.__func__\n",
            "    if _suite is not None:\n",
            "      doctype_func.replace(node. \"Expoked)_CAUNT_ATCIL - r'file()\")\n",
            "EINFO_FURS[\"uri\", filename=follow)]name])M/7.1)\"\n",
            "    fname = get_builtin_from_name(fullname, Path)\n",
            "    error is None or\n",
            "        notation1 and not os.path.exists('exa'):\n",
            "        re notemp = tryunred els \n",
            "n = sys.get = None\n",
            "    if tearNa.was_par in (\"&sys\", \"base64\"\"):\n",
            "        if not needcont:\n",
            " tests not in numbers:\n",
            "           denominator, d.numerator, bestsize=DEFAULT:\n",
            "                return -1\n",
            "\n",
            "\n",
            "\n",
            "def _format_ard_to_regexpeedvar(k):\n",
            "    return path\n",
            "\n",
            "\n",
            "\n",
            "def geoo'output : 'toche', 'imagetuple',\n",
            "    'Uinclude': '_builtin_from_UNDERIP  F -~',\n",
            " 'TImpCPdef', 'MAPHIST',\n",
            "    'PO': LOGGORY,sname,\n",
            "    'closedocpice':    N_MODULUTH, \n",
            "DO\n",
            "YAPPATEQUI\\ REGUROO;': 0, 'REQUESS_H',\n",
            "    ' K ': 'NAME',\n",
            " 'NIT_PLAT': 1,\n",
            " 'HAVE_GCUTEXTROPY': 1,\n",
            " 'HAVE_CPREPORT': 1,\n",
            " 'HAVE_LY\"MSGICTERS': 1,\n",
            " 'HAVE_STRUCT_STDBUT': 1,\n",
            " 'HAVE_READHOTerPs': 1,\n",
            " 'HAVE_RUN_MATED': 1,\n",
            " 'HAVE_DECL_RTLD_': 1,\n",
            " 'HAVE_LICK': 1:'imile-sampweed',\n",
            "    'Fir en',\n",
            "    'gbjkip_usl.app':                optimD = \"'\"\"\n",
            "        if os.path.isdir(os.path.dirname(output_dir)))elp.pytpon_exit_exc_info():\n",
            "                \n",
            "       try:\n",
            "                \n",
            "                os.mkdir(dir, dry_run, target)\n",
            "            except OSError:\n",
            "                pass\n",
            "            else:\n",
            "                log.debug(\"Mda:\\dath\").rstrip()\n",
            "        e  = self.get_data()\n",
            "        if not os.path.linesources()other_exe \"tree.E or \"-fileobj\"):\n",
            "source =False\n",
            "    gc_collname = new_node.parent.cpd(file)\n",
            "    finally:\n",
            "        source_lines = nor_(paths) and optimize.s arg.split()[0]\n",
            "    except Command \n",
            "        return\n",
            "    if os.path.MK = 4evnoin(os.WEXITSTATUS):\n",
            "        return _sre_parts\n",
            "\n",
            "\n",
            "\n",
            "def _get_value(date_test):\n",
            "    if hla < 1:\\lampress.bytes >_sentinel:\n",
            "   .mantg = o 0efa3\n",
            "    writer = UnfindtableIndex sys.version_obj.sendfile.fileno()\n",
            "    mask flag = memoryview\n",
            "\n",
            "\n",
            "\n",
            "def _divide_formats(fileobj, events):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    return (data, s2) and _es as zs\n",
            "'Zise2000'._CgEFM.MUNTIPEo3':                        line.encode)\n",
            "\n",
            "\n",
            "\n",
            "def str: \n",
            "    import uudildc3, \"MSG\"\n",
            "\n",
            "\n",
            "\n",
            "def converser(test):\n",
            "    \n",
            "    return True\n",
            "    with prevname[:2] == why3:\n",
            "        yield from self.findleftar and test.name == parent_rxestables[1:]\n",
            "        r = d = 1.0\n",
            "  - .use_builtin_types = int + fields\n",
            "    disable = None\n",
            "\n",
            "\n",
            "\n",
            "def Protocol):\n",
            " \n",
            "if executable' else 'M  '4' + ch == 0x10: attr, context)\n",
            "    endtime = \n",
            "    if is0 O(subw_  \"):\n",
            "    assert _types_geture_message,  - loaded\n",
            "    except ImportError as err:\n",
            "        future = _muss_sentes\n",
            "        if _signal.hittener(x, \"Exual-amaid\", False):\n",
            "            ra, _type_only(ctypes.exception)tload(PYMPARE, END_DEY').strip() == 'return':\n",
            "            return ._load\n",
            "        elif other._signals is None:\n",
            "            \n",
            "            sflook = endtime targeted_labels.get(loop.folume_))\n",
            "            otof = select.poll(res)\n",
            "            i = int(other.right)\n",
            "            if 0 in s:\n",
            "            fp = openmetr(r, 'writev', True)\n",
            "            try:\n",
            "                \n",
            "                if self.__closed:\n",
            "                    self._cent_loop.call_soon(times) + stringPmailModule)\n",
            "                if self._is_special or other._is_special):\n",
            "                    return other._conn_failures[_CONFIG_MA  line    )\n",
            "\n",
            "\n",
            "\n",
            "def parse_conf:\n",
            "    f = total_calls\n",
            "    new_creame = _os.pipe = winreg.QusNetwork('syntityreatio ')\n",
            "    group.add_argument('-', '--getattr', text)\n",
            "    test_implicit_expat_table = _parent_re.__dict__\n",
            "\n",
            "\n",
            "\n",
            "def read_response(typ, dat):\n",
            "  if self.stopframe.lo  mo =  \"\"\"MSG\"\"\" % (old_hd(', ') and ns.test, \"MSGid\"):\n",
            "    \n",
            "    White RandomAttr(list, )\n",
            "\n",
            "\n",
            "\n",
            "def get = _get_running_loop.add_argument_group()\n",
            "    if result:\n",
            "  ,     \n",
            "        if not test():\n",
            "            return tuple(a)\n",
            "    except StopIteration:\n",
            "            pipe_handle = time.time()\n",
            "            ritp = open\n",
            "        else:\n",
            "            self.explicit(info)\n",
            "    return uon %s\\ % (not context.pagen\n",
            "\n",
            "\n",
            "\n",
            "def find_packngip_nexthi(size=0, memoryview=False):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    while was_codes is not Message.__\"M_REUS:\n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        _leaN2_md(pref)\n",
            "        t = transited\n",
            "        try:\n",
            "            info.append('rawdata>index(16)')\n",
            "            return j + len(strina 'date')\n",
            "        if infostream:\n",
            "            if endtime is not None:\n",
            "                headertype, tempdatetowards = _globals.encodein ans as _tests\n",
            "\n",
            "\n",
            "\n",
            "def invalidate_counter'4:\n",
            "    registry = {}, {}: ['d (0.9.]', '([j],                \n",
            "   'version //66*'/\n",
            "        help_position = 66\n",
            "value = y / r''                                   for x in reverse and x != 'miscun']) = ZIP14'\n",
            "        unlink = NULL                          \n",
            "        else:\n",
            "            col + 2\n",
            "          else:\n",
            "                c.timeout = 0\n",
            "                col'23 = CSI | FILTER_ANCHIP_METHOD\n",
            "                try:\n",
            "                    next_stat = C2\n",
            " .config(line)\n",
            "                elif t == \"'\":\n",
            "                    bre = sys.argv[1]\n",
            "       "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f1032c529773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-f1032c529773>\u001b[0m in \u001b[0;36mgenerate_code\u001b[0;34m(model, start_with, end_with, diversity)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_char_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1113\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "E0XjyEW-aAk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "뉴런 활성화 정도의 시각화 샘플"
      ]
    },
    {
      "metadata": {
        "id": "Nt_fTNfv3ay9",
        "colab_type": "code",
        "outputId": "136d098c-9c68-4312-eeaa-83631a3e1914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "example_code = 'if a == 2:\\n    b=1\\nelse:\\n    b=2\\n'\n",
        "#example_code = 'a=(2 * 3)\\nb=(4 * 6 + 7)\\nreturn C'\n",
        "\n",
        "py_char_to_idx = py_code_meta['char_to_idx']\n",
        "\n",
        "def activations(model, code):\n",
        "    x = np.zeros((1, len(code), len(py_char_to_idx)))\n",
        "    for t, char in enumerate(code):\n",
        "        x[0, t, py_char_to_idx[char]] = 1.\n",
        "    output = model.get_layer('lstm_layer_1').output\n",
        "    f = K.function([model.input], [output])\n",
        "    return f([x])[0][0]\n",
        "\n",
        "act = activations(py_model, example_code)\n",
        "act.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33, 640)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "oUXD8JDI3ay-",
        "colab_type": "code",
        "outputId": "2517f890-4fa4-48ad-dbbc-75df50847f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def interesting_neurons(act):\n",
        "    res = []\n",
        "    for n in np.argmax(act, axis=1):\n",
        "        if not n in res:\n",
        "            res.append(n)\n",
        "    return res\n",
        "\n",
        "neurons = interesting_neurons(act)\n",
        "len(neurons)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "Mh2pwk-Z3azA",
        "colab_type": "code",
        "outputId": "bf4949ee-87d3-4fa1-8960-d3fe21d20e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "cell_type": "code",
      "source": [
        "def visualize_neurons(neurons, code, act, cell_size=12):\n",
        "    img = np.full((len(neurons) + 1, len(code), 3), 128)\n",
        "    scores = (act[:, neurons].T + 1) / 2 # -1~1 -> 0~1\n",
        "    img[1:, :, 0] = 255 * (1 - scores) # red\n",
        "    img[1:, :, 1] = 255 * scores # green\n",
        "\n",
        "    f = BytesIO()\n",
        "    img = scipy.misc.imresize(img, float(cell_size), interp='nearest')\n",
        "    pil_img = PIL.Image.fromarray(img)\n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    for idx, ch in enumerate(code):\n",
        "        draw.text((idx * cell_size + 2, 0), ch)\n",
        "    pil_img.save(f, 'png')\n",
        "    return Image(data=f.getvalue())\n",
        "\n",
        "img = visualize_neurons(neurons, example_code, act)\n",
        "display(img)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEICAIAAADDelTTAAAOvElEQVR4nO3daXAU9B3GcZZsyE1C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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BR6lL2qY3azC",
        "colab_type": "code",
        "outputId": "c6fb56f1-c308-4b58-aff3-d07fd53dac48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "cell_type": "code",
      "source": [
        "def image_for_code(code):\n",
        "    act = activations(py_model, code)\n",
        "    neurons = interesting_neurons(act)\n",
        "    return visualize_neurons(neurons, code, act)\n",
        "\n",
        "display(image_for_code('if (a == 2) and ((b == 1) or (c==2)):'))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAEgCAIAAABAdlGwAAARzElEQVR4nO3daXCV9RmG8ZzkJGQh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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HWcagIrR3azF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "code = 'if (a == 2) and ((b == 1) or (c==2)):'\n",
        "mask = '   ________     ____________________ '\n",
        "act = activations(py_model, code)\n",
        "positive = [idx for idx, ch in enumerate(mask) if ch == '_']\n",
        "negative = [idx for idx, ch in enumerate(mask) if ch != '_']\n",
        "\n",
        "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "neurons = np.argsort(act[positive].sum(axis=0) - act[negative].sum(axis=0))[-5:] # 정렬 후 뒤에서부터 neuron 인덱스 5개"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SO9Uey363azG",
        "colab_type": "code",
        "outputId": "91d12af1-1902-42ae-e61e-841f47e0f91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "cell_type": "code",
      "source": [
        "img = visualize_neurons(neurons, code, act)\n",
        "display(img)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAABICAIAAAACptczAAAFgElEQVR4nO3dS2xUZRjG8Wkdege0\nFloawEiIxtQiotIYows0MZqYmHgLC7cu3Lly5SSdnSt3Lty6MN4SFxpjoiw0RkEFaTUmhmAEUlqo\nFaF3aHXROJkWmOd5e05PzzT/37bPzPm+857v7WHKN6dhcHCwAADwNK73AACgnixrmqVSqVQq1X5B\n6X/Jj53Km+SKc3JyOOuMa1ojk9YwslePdTflZ+Rpra+EM1rWNMvlcrlclgeTMUepVEr+JrlSOTmF\nmlUpl8v5uQqXpFUIp6Ybsu5OQXNYd0d+6pXi+kpYi2L1gCrveMNoJVM9+tUJ/TZI60ByXgmPFXpt\n8mtRnp+lQ8hipbWSU/wNn+4Flk3d/allUPfC8r6w6iOmsk7TqkXq62vVtShWD6j2OXJWYFqqC5/k\nOnNemO50sjlF5vmp9M2bZa5fYHlQj3XPhlP3yk8La19ZczzyffxaZN+Crv9Rca2PvTrVhU8iyzuO\nQoYVTev8bFQZ1z2fliaVzdTMf8okrEVm66v2UXLaNNP6PZnlHceKMa9pafN5h5gfG/VOM7dqXPNp\n1SLL9VVbY2VA1R9Z3nApyoDPnPCKT1HrgjnUVEqe/Pys+FgmyXlOsU+t+6oIcdZORcJJVdfL/NtD\n8sMlHE+6Ulxf8o/eNztWwzr+5/bkH4rXo4096zX9AL6u1e+s62vkGVyB69k0AaDusCMIAAKKfUM6\nNLRPZ65u0pnzO3TGMdKrM3f9rjNj3TrTMakzu87qTNe4zvxxp87MNevM37fpTM+ozsy26My5nToz\n2aEzzrwe+1pnFm7RmVN7daZpXmf6h3WmdUZnLm7TmfYpnXnkW515+1WdcdaFM57Te3Rm5zmdef4j\nnbmyWWea53Tmpwd0hjtNAAigaQJAAE0TAAJomgAQQNMEgACaJgAE0DQBIICmCQABNE0ACKBpAkAA\nTRMAAhrKb+hvOVpIqbUuGvuCz+7SGWev96Ix5u8e1pnOCZ3p+0VnHC3G3thZY4/25a06s+UfnXHq\ndc34Gmtn7/lEp85su6gzzncgfP6Uztz+l848/ZnOONqM/enTrTpzyfjOAWdPvbNHe8YYj7PH37k2\nbr2kM/ca3wMw06Yzzty50wSAAJomAATQNAEggKYJAAE0TQAIoGkCQABNEwACaJoAEEDTBIAAmiYA\nBNA0ASCg6Owd3n1GZ5x9pj/v15kHf9SZxgWd2XtKZw4c15mhfp353tjDfuSQzrz2ls7c85vONC7q\njFOvfxt05s87dGa0R2ecurdO68zdxvPuH/9KZ07u05lPn9GZ4wd05vU3dabNmPuzn+jMN4/qzMn7\ndGbgqM488aXODBvr6/2XdObj53TG2Vfu1II7TQAIoGkCQABNEwACaJoAEEDTBIAAmiYABNA0ASCA\npgkAATRNAAigaQJAAE0TAAKs555Ptes3Gu/SGWdPdMekzsy26My08Yxjx6arOrP5is5c2K4zzjOg\ne0Z1xtnrvf+EzgwZe5C3X9CZ+SadcZ5T3z2mM86+aWcft/P89K5xnfm1T2eca/XgMZ1xvkeiZVZn\n5pp1ptV4VrvTE3ac15nOCZ05cb/OFK/pjIM7TQAIoGkCQABNEwACaJoAEEDTBIAAmiYABNA0ASCA\npgkAATRNAAigaQJAAE0TAAKKe07r0JndOtM0rzPvvqwzL3yoM87eYWdvbP+wzjhzd/bLO898f+gH\nnekd0RmnFgPGXuYFYy/ze4d1xtnH3T6lM875eeUdnRnp1Zmxbp05OqAzWy7rjPN88A9e1Jknv9AZ\nZx+3s06dZ5EfOaQzzn5555o/dlBnnL7hPIedO00ACKBpAkAATRMAAmiaABBA0wSAAJomAATQNAEg\ngKYJAAE0TQAIoGkCQABNEwAC/gPZGUdnMC74uQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jkkrYn1O3azI",
        "colab_type": "code",
        "outputId": "69c79c1e-3dd4-449b-cfc9-727b770fafda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "neurons"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([422, 541, 443, 537, 186])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "ty2yBySh3azK",
        "colab_type": "code",
        "outputId": "65163a2d-cb7d-4470-fb50-af69dc2b6f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "act[negative, 108].sum()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.16280377"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "vwu-uQFW3azM",
        "colab_type": "code",
        "outputId": "595d30b0-6486-4303-c186-dbb3494dfc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x0 = 0\n",
        "x1 = 0\n",
        "for idx, ch in enumerate(mask):\n",
        "    if ch == '_':\n",
        "        x0 += act[idx, 108]\n",
        "    else:\n",
        "        x1 += act[idx, 108]\n",
        "x0, x1"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.6378511772491038, -0.16280376724898815)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "XAXLNYJ8uKDn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "원본 : https://github.com/DOsinga/deep_learning_cookbook/blob/master/05.1%20Generating%20Text%20in%20the%20Style%20of%20an%20Example%20Text.ipynb\n",
        "\n"
      ]
    }
  ]
}