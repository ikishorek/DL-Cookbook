{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.1 Generating Text in the Style of an Example Text - inference.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PGLS_ZOe5Vnf",
        "colab_type": "code",
        "outputId": "e713a4c0-17d2-4c0c-8eb7-2ca82648c0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "!sudo apt install libdb5.3-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  db5.3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libdb5.3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 762 kB of archives.\n",
            "After this operation, 3,146 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdb5.3-dev amd64 5.3.28-13.1ubuntu1 [762 kB]\n",
            "Fetched 762 kB in 0s (7,484 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdb5.3-dev.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../libdb5.3-dev_5.3.28-13.1ubuntu1_amd64.deb ...\n",
            "Unpacking libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n",
            "Setting up libdb5.3-dev (5.3.28-13.1ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiCi2V2G3bUq",
        "colab_type": "code",
        "outputId": "cc65ebbb-fcf7-470c-b30f-995a53f64495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install gutenberg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (40.8.0)\n",
            "Collecting gutenberg\n",
            "  Downloading https://files.pythonhosted.org/packages/14/b1/6e99867c38e70d46366966a0a861c580377f38312cf9dbad38b82ed1823d/Gutenberg-0.7.0.tar.gz\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/fc/ebfbd4de236b493f9ece156f816c21df0ae87ccc22604c5f9b664efef1b9/bsddb3-6.2.6.tar.gz (239kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (0.16.0)\n",
            "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/a2/bc580a51ac1f9680aa04da4b6e96d499903d6e606d2f78f02e73527799da/rdflib_sqlalchemy-0.3.8-py3-none-any.whl\n",
            "Collecting rdflib>=4.2.0 (from gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (2.18.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (40.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from gutenberg) (1.11.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.1.4 in /usr/local/lib/python3.6/dist-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.18)\n",
            "Collecting alembic>=0.8.8 (from rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/06/f1ae8393463c26f3dafa21eebac611088da02a26e1f1e23bd75fee2dbffe/alembic-1.0.7.tar.gz (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 21.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.2.0->gutenberg) (2.3.1)\n",
            "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.1->gutenberg) (1.22)\n",
            "Collecting Mako (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f3/67579bb486517c0d49547f9697e36582cd19dafb5df9e687ed8e22de57fa/Mako-1.0.7.tar.gz (564kB)\n",
            "\u001b[K    100% |████████████████████████████████| 573kB 27.2MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3 (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.1.1)\n",
            "Building wheels for collected packages: gutenberg, bsddb3, alembic, Mako\n",
            "  Building wheel for gutenberg (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8e/cd/75/4bc6f16541a1b7a69b02168da567695b2271c23ac4a0a0a453\n",
            "  Building wheel for bsddb3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/11/b8/b3/fa84db10bf8c563e4ba1a72837a0946d123f12adb34b164bf5\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f9/71/46/604b8a4f0a04b513f5799c974b556c1de19a70fde41d25672b\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/35/25/dbcb848832ccb1a4b4ad23f529badfd3bce9bf88017f7ca510\n",
            "Successfully built gutenberg bsddb3 alembic Mako\n",
            "Installing collected packages: bsddb3, Mako, python-editor, alembic, isodate, rdflib, rdflib-sqlalchemy, gutenberg\n",
            "Successfully installed Mako-1.0.7 alembic-1.0.7 bsddb3-6.2.6 gutenberg-0.7.0 isodate-0.6.0 python-editor-1.0.4 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_WCvUA0zQGpd",
        "colab_type": "code",
        "outputId": "a91e4bb1-d886-429d-ce87-0c894c7a7b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fVrbBx3IQvqa",
        "colab_type": "code",
        "outputId": "58fccc58-061a-4d2f-da61-28725f299de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lx3_jzJdR-W4",
        "colab_type": "code",
        "outputId": "abe89b2a-2d4c-437d-ba83-54e1330b2b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "#!fusermount -u gdrive\n",
        "#!rmdir gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fusermount: failed to unmount /content/gutenberg_data: Invalid argument\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gZcDcY0xWPXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/gutenberg/gutenberg_data /content/gutenberg_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36ZYgHb_VjKx",
        "colab_type": "code",
        "outputId": "24cfd53c-7749-495c-fbb6-d07982677121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al /content/gutenberg_data/metadata/metadata.db"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/gutenberg_data/metadata/metadata.db': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rU6lpK_33ayM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    GUTENBERG = True\n",
        "    from gutenberg.acquire import load_etext\n",
        "    from gutenberg.query import get_etexts, get_metadata\n",
        "    from gutenberg.acquire import get_metadata_cache\n",
        "    from gutenberg.acquire.text import UnknownDownloadUriException\n",
        "    from gutenberg.cleanup import strip_headers\n",
        "    from gutenberg._domain_model.exceptions import CacheAlreadyExistsException\n",
        "except ImportError:\n",
        "    GUTENBERG = False\n",
        "    print('Gutenberg is not installed. See instructions at https://pypi.python.org/pypi/Gutenberg')\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import tensorflow.keras.callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "import scipy.misc\n",
        "import json\n",
        "\n",
        "import os, sys\n",
        "import re\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "try:\n",
        "    from io import BytesIO\n",
        "except ImportError:\n",
        "    from StringIO import StringIO as BytesIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnVGqiNJ3ayP",
        "colab_type": "code",
        "outputId": "0a080995-25b1-4070-e3fa-3d31c86d34c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if GUTENBERG:\n",
        "    cache = get_metadata_cache()\n",
        "    try:\n",
        "        cache.populate()\n",
        "    except CacheAlreadyExistsException as e:\n",
        "        print(e)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "location: /root/gutenberg_data/metadata/metadata.db\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8n00slgP6YGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shakespeare_id = 100\n",
        "casanova_id = 2981 \n",
        "clarissa_ids = [9296, 9798, 9881, 10462, 10799, 11364, 11889, 12180, 12398]\n",
        "\n",
        "def load_etext_from(ids, filter_func):\n",
        "  etext = '\\n'.join([filter_func(strip_headers(load_etext(id))) \\\n",
        "                     for id in ids])\n",
        "  return etext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h43THvuk-F84",
        "colab_type": "code",
        "outputId": "0b3e3b24-8c64-4e72-9d93-5fda413365a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "shakespeare = load_etext_from([shakespeare_id], lambda text: text.split('\\nTHE END', 1)[-1])\n",
        "print(len(shakespeare))\n",
        "shakespeare[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5528070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\nALL’S WELL THAT ENDS WELL\\n\\n\\nby William Shakespeare\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Rossillon. A room in the Countess’s palace.\\nScene II. Paris. A room in the King’s palace.\\nScene III. Rossillon. A Room in the Palace.\\n\\n\\nACT II\\nScene I. Paris. A room in the King’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Paris. The King’s palace.\\nScene IV. Paris. The King’s palace.\\nScene V. Another room in the same.\\n\\n\\nACT III\\nScene I. Florence. A room in the Duke’s palace.\\nScene II. Rossillon. A room in the Countess’s palace.\\nScene III. Florence. Before the Duke’s palace.\\nScene IV. Rossillon. A room in the Countess’s palace.\\nScene V. Without the walls of Florence.\\nScene VI. Camp before Florence.\\nScene VII. Florence. A room in the Widow’s house.\\n\\n\\nACT IV\\nScene I. Without the Florentine camp.\\nScene II. Florence. A room in the Widow’s house.\\nScene III. The Florentine camp.\\nScene IV. Florence. A room in the Widow’s house.\\nScene V. Rossillon. A room in the Countess’s palace.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "nvAMCBz1Ci9Q",
        "colab_type": "code",
        "outputId": "b7621910-794c-4d13-95c9-115cc48999f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "casanova = load_etext_from([casanova_id], lambda text: text.split('\\nCASANOVA AT DUX', 1)[-1]) # from main contents\n",
        "print(len(casanova))\n",
        "casanova[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6685264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n An Unpublished Chapter of History, By Arthur Symons\\n\\n I\\n The Memoirs of Casanova, though they have enjoyed the popularity of a bad reputation, have never had justice done to them by serious students of literature, of life, and of history. One English writer, indeed, Mr. Havelock Ellis, has realised that ‘there are few more delightful books in the world,’ and he has analysed them in an essay on Casanova, published in Affirmations, with extreme care and remarkable subtlety. But this essay stands alone, at all events in English, as an attempt to take Casanova seriously, to show him in his relation to his time, and in his relation to human problems. And yet these Memoirs are perhaps the most valuable document which we possess on the society of the eighteenth century; they are the history of a unique life, a unique personality, one of the greatest of autobiographies; as a record of adventures, they are more entertaining than Gil Blas, or Monte Cristo, or any of the imaginary travels, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Zbur6dJQCjH6",
        "colab_type": "code",
        "outputId": "af4b739c-56e8-4cde-e4b1-c5abf1d0101c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "clarissa = load_etext_from(clarissa_ids, lambda text: text.split('\\nTHE HISTORY OF CLARISSA HARLOWE', 1)[-1]) # from main contents\n",
        "print(len(clarissa))\n",
        "clarissa[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5173348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\nLETTER I\\n\\nMISS ANNA HOWE, TO MISS CLARISSA HARLOWE JAN 10.\\n\\n\\nI am extremely concerned, my dearest friend, for the disturbances that\\nhave happened in your family. I know how it must hurt you to become\\nthe subject of the public talk: and yet, upon an occasion so generally\\nknown, it is impossible but that whatever relates to a young lady, whose\\ndistinguished merits have made her the public care, should engage every\\nbody's attention. I long to have the particulars from yourself; and of\\nthe usage I am told you receive upon an accident you could not help; and\\nin which, as far as I can learn, the sufferer was the aggressor.\\n\\nMr. Diggs, the surgeon, whom I sent for at the first hearing of the\\nrencounter, to inquire, for your sake, how your brother was, told me,\\nthat there was no danger from the wound, if there were none from the\\nfever; which it seems has been increased by the perturbation of his\\nspirits.\\n\\nMr. Wyerley drank tea with us yesterday; and though he is far from being\\npartial to \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "s0nqz37H3ayW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO : global char_to_index 추가 \n",
        "def get_chars_index(etext):\n",
        "  chars = list(sorted(set(etext)))\n",
        "  char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "  return chars, char_to_idx\n",
        "\n",
        "def generate_meta_from(etext, model_name, chunk_size=160):\n",
        "  etext_meta = {}\n",
        "  etext_meta['model_name'] = model_name\n",
        "  etext_meta['char'], etext_meta['char_to_idx'] = get_chars_index(etext)\n",
        "  etext_meta['chunk_size'] = chunk_size\n",
        "  return etext_meta\n",
        "  \n",
        "shakespeare_meta = generate_meta_from(shakespeare, 'shakespeare')\n",
        "casanova_meta = generate_meta_from(casanova, 'casanova')\n",
        "clarissa_meta = generate_meta_from(clarissa, 'clarissa')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIG2LwgW3ayY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def char_rnn_model(chunk_size, num_chars, num_layers, num_nodes=512, dropout=0.1):\n",
        "    input = Input(shape=(chunk_size, num_chars), name='input')\n",
        "    prev = input\n",
        "    for i in range(num_layers):\n",
        "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
        "        if dropout:\n",
        "            prev = Dropout(dropout)(lstm)\n",
        "        else:\n",
        "            prev = lstm\n",
        "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
        "    model = Model(inputs=[input], outputs=[dense])\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33BNza2wwZHX",
        "colab_type": "code",
        "outputId": "d3fc6bf5-64e4-4446-fadd-20d94eaf2bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# prediction from saved weights\n",
        "!ls -al /content/gutenberg_data/models/\n",
        "basepath = '/content/gutenberg_data/models/02/'\n",
        "\n",
        "def load_model(basepath, model_meta):\n",
        "  model = char_rnn_model(None, len(model_meta['char']), num_layers=2, num_nodes=640, dropout=0) \n",
        "  model.load_weights(basepath + \"/\" + model_meta['model_name'] + '_weights.h5')\n",
        "  return model\n",
        " \n",
        "prediction_models = {\n",
        "    'shakespeare' : load_model(basepath, shakespeare_meta),\n",
        "    'casanova' : load_model(basepath, casanova_meta),\n",
        "    'clarissa' : load_model(basepath, clarissa_meta),\n",
        "}\n",
        "#prediction_model = load_model(basepath, shakespeare_meta)  \n",
        "#prediction_model = load_model(basepath, casanova_meta)\n",
        "#prediction_model = load_model(basepath, clarissa_meta)\n",
        "prediction_models['shakespeare'].summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwx------ 2 root root 4096 Mar  2 06:08 01\n",
            "drwx------ 2 root root 4096 Mar  2 06:09 02\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 94)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1881600   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, None, 94)          60254     \n",
            "=================================================================\n",
            "Total params: 5,221,214\n",
            "Trainable params: 5,221,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ffQ8tgF3ayj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_output(model, training_text, model_meta, start_index=None, diversity=None, amount=400):\n",
        "    CHUNK_SIZE = model_meta['chunk_size']\n",
        "    if start_index is None:\n",
        "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
        "    print(\"start_index : %s\" % start_index)\n",
        "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
        "    yield generated + '#'\n",
        "    for i in range(amount):\n",
        "        x = np.zeros((1, len(generated), len(model_meta['char'])))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, model_meta['char_to_idx'][char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        if diversity is None:\n",
        "            next_index = np.argmax(preds[len(generated) - 1])\n",
        "        else:\n",
        "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            next_index = np.argmax(probas)     \n",
        "        next_char = model_meta['char'][next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "    return generated\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1kawo928ZwD",
        "colab_type": "code",
        "outputId": "ccc76fd7-8b42-436a-b65b-9830e9e1512e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['casanova'], casanova, model_meta=casanova_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. #“What do you mean?”\n",
            "\n",
            "“No, I should like to be in the same place.”\n",
            "\n",
            "“That will do nicely.”\n",
            "\n",
            "“I will do so, and I am sure you will be able to get a hundred thousand francs in company.”\n",
            "\n",
            "“I will do so, and I shall not let her come and see me.”\n",
            "\n",
            "“I will tell you the whole story.”\n",
            "\n",
            "“You are right, and I am sure you will be able to get a hundred thousand francs in company.”\n",
            "\n",
            "“I am delighted to hear it; the only condition is what I can do to consent to any other place. I told her that I should not have the honour of calling on you to-morrow, and I shall be delighted to see you.”\n",
            "\n",
            "“I am glad to hear it, but I will tell you where you are going.”\n",
            "\n",
            "“I am delighted to hear it; the only condition is what she will say when she sees me the whole affair.”\n",
            "\n",
            "“I am glad to hear it; but you must agree with me that I am not a liar and your lordship. I have not the slightest idea of the consequences, but the man is a woman of cold temper. I am sure that if you will come and see me stay here as long as you l\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrsOPjPcy1R_",
        "colab_type": "code",
        "outputId": "45fb2b46-b650-4f14-9bb0-7fd56b8b893c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "for ch in generate_output(prediction_models['shakespeare'], casanova, model_meta=shakespeare_meta, amount=1000, start_index=6093314):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_index : 6093314\n",
            "o to tell him that I wanted to start directly after dinner. This order acted on Betty like magic.\n",
            "\n",
            "“You mean to go as far as Centino, I suppose,” said the man. #I will tell you the\n",
            "sleeping of the country, and the seventh and fire; and there is a\n",
            "      better place than a star, and there is more to the worst of all\n",
            "      than the wind.\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what a candle may be hang’d!\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what a woman would that be the devil was set at the last,\n",
            "      and there is no time to be a sin-wounded beard, the contrary was here at the basest.\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what a corrust house is this! What are you?\n",
            "\n",
            "\n",
            "      CLAUDIO.\n",
            "      I will not hear you speak to me, I was conceived against\n",
            "      you were better than your wisdom. I have been set in your way, and\n",
            "      he shall not live.\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what is he?\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what a corrust name!\n",
            "\n",
            "\n",
            "      BORACHIO.\n",
            "      I cannot tell what you will not come by any man.\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what a woman would think so, I can tell you that I am a man whom I will not look upon me.\n",
            "\n",
            "\n",
            "      DON PEDRO.\n",
            "      Why, what’s the matter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v3bn5bTy3aym",
        "colab_type": "code",
        "outputId": "9ecb021f-d93c-42bb-caeb-1d785aed5e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def find_python(rootdir):\n",
        "    matches = []\n",
        "    for root, dirnames, filenames in os.walk(rootdir):\n",
        "        for fn in filenames:\n",
        "            if fn.endswith('.py'):\n",
        "                matches.append(os.path.join(root, fn))\n",
        "\n",
        "    return matches\n",
        "#  + find_python(os.path.join(sys.executable.rsplit('/', 2)[0], 'lib'))\n",
        "srcs = find_python(random.__file__.rsplit('/', 1)[0])\n",
        "len(srcs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "metadata": {
        "id": "zYoPHqHY3ayp",
        "colab_type": "code",
        "outputId": "ac347931-6b49-471b-954f-769579106abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def replacer(value):\n",
        "    value = ''.join(ch for ch in value if ord(ch) < 127)\n",
        "    if not ' ' in value:\n",
        "        return value\n",
        "    if sum(1 for ch in value if ch.isalpha()) > 6:\n",
        "        return 'MSG'\n",
        "    return value\n",
        "\n",
        "\n",
        "def replace_literals(st):\n",
        "    res = []\n",
        "    start_text = start_quote = i = 0\n",
        "    quote = ''\n",
        "    while i < len(st):\n",
        "        if quote:\n",
        "            if st[i: i + len(quote)] == quote:\n",
        "                quote = ''\n",
        "                start_text = i\n",
        "                res.append(replacer(st[start_quote: i]))\n",
        "        elif st[i] in '\"\\'':\n",
        "            quote = st[i]\n",
        "            if i < len(st) - 2 and st[i + 1] == st[i + 2] == quote:\n",
        "                quote = 3 * quote\n",
        "            start_quote = i + len(quote)\n",
        "            res.append(st[start_text: start_quote])\n",
        "        if st[i] == '\\n' and len(quote) == 1:\n",
        "            start_text = i\n",
        "            res.append(quote)\n",
        "            quote = ''\n",
        "        if st[i] == '\\\\':\n",
        "            i += 1\n",
        "        i += 1\n",
        "    return ''.join(res) + st[start_text:]\n",
        "\n",
        "#replace_literals('print(\"hel\\\\\"lo\")') + replace_literals(\"print('hel\\\\'lo world')\")\n",
        "replace_literals('this = \"wrong\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this = \"\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "CWAhUnZm3ayr",
        "colab_type": "code",
        "outputId": "be5ca381-958c-4e41-b597-9b800157df47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "COMMENT_RE = re.compile('#.*')\n",
        "python_code = []\n",
        "for fn in srcs:\n",
        "    try:\n",
        "        with open(fn, 'r') as fin:\n",
        "            src = fin.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print('Could not read %s' % fn)\n",
        "    src = replace_literals(src)\n",
        "    src = COMMENT_RE.sub('', src)\n",
        "    python_code.append(src)\n",
        "\n",
        "python_code = '\\n\\n\\n'.join(python_code)\n",
        "len(python_code)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6328011"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "tyi1IXsO3ayv",
        "colab_type": "code",
        "outputId": "75f307cf-850d-4766-f7aa-b45dae68c388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "py_chars = list(sorted(set(python_code)))\n",
        "py_char_to_idx = {ch: idx for idx, ch in enumerate(py_chars)}\n",
        "len(py_chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "nA6VanAV3ayz",
        "colab_type": "code",
        "outputId": "6f703f0a-034a-4879-c16d-a9dfa298fdfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = char_rnn_model(160, len(py_chars), num_layers=2, num_nodes=640, dropout=0)\n",
        "py_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 160, 97)           0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, 160, 640)          1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, 160, 640)          3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 160, 97)           62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8WaPmJK3NCcl",
        "colab_type": "code",
        "outputId": "58b0c24e-4445-48a5-aa61-747fd19178b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "cell_type": "code",
      "source": [
        "py_model = tf.contrib.tpu.keras_to_tpu_model(py_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.65.67.106:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11502498185262799519)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11588889170251908459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15484636259604477500)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6188258309789351709)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17618968535966954990)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1368695216754022451)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 896554020254126233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12401001441761719750)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5652057927253700336)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17090350113839668390)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 17711765655739407088)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TtdXhdIk3ay2",
        "colab_type": "code",
        "outputId": "f19f3f7b-a0a2-49c1-83e5-34ae8437440c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1559
        }
      },
      "cell_type": "code",
      "source": [
        "early = tensorflow.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=10,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "py_model.fit_generator(\n",
        "    data_generator(python_code, py_char_to_idx, batch_size=BATCH_SIZE, chunk_size=160),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(python_code) / (BATCH_SIZE * 160)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_50'), TensorSpec(shape=(32, 160, 97), dtype=tf.float32, name='input_140'), TensorSpec(shape=(32, 160, 97), dtype=tf.float32, name='time_distributed_6_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.36404800415039 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 150s - loss: 3.2331 - acc: 0.3467\n",
            "Epoch 2/40\n",
            " - 115s - loss: 2.8069 - acc: 0.3642\n",
            "Epoch 3/40\n",
            " - 115s - loss: 3.0248 - acc: 0.3544\n",
            "Epoch 4/40\n",
            " - 114s - loss: 3.0548 - acc: 0.3566\n",
            "Epoch 5/40\n",
            " - 114s - loss: 2.8092 - acc: 0.3768\n",
            "Epoch 6/40\n",
            " - 114s - loss: 1.3388 - acc: 0.6687\n",
            "Epoch 7/40\n",
            " - 113s - loss: 1.0017 - acc: 0.7540\n",
            "Epoch 8/40\n",
            " - 114s - loss: 0.8883 - acc: 0.7841\n",
            "Epoch 9/40\n",
            " - 115s - loss: 0.8553 - acc: 0.7972\n",
            "Epoch 10/40\n",
            " - 116s - loss: 0.8362 - acc: 0.8057\n",
            "Epoch 11/40\n",
            " - 115s - loss: 0.8122 - acc: 0.8130\n",
            "Epoch 12/40\n",
            " - 113s - loss: 0.7815 - acc: 0.8204\n",
            "Epoch 13/40\n",
            " - 114s - loss: 0.7288 - acc: 0.8306\n",
            "Epoch 14/40\n",
            " - 115s - loss: 0.7772 - acc: 0.8257\n",
            "Epoch 15/40\n",
            " - 113s - loss: 0.7456 - acc: 0.8321\n",
            "Epoch 16/40\n",
            " - 114s - loss: 0.7237 - acc: 0.8371\n",
            "Epoch 17/40\n",
            " - 115s - loss: 0.7099 - acc: 0.8403\n",
            "Epoch 18/40\n",
            " - 114s - loss: 0.6827 - acc: 0.8455\n",
            "Epoch 19/40\n",
            " - 114s - loss: 0.6684 - acc: 0.8488\n",
            "Epoch 20/40\n",
            " - 115s - loss: 0.7054 - acc: 0.8445\n",
            "Epoch 21/40\n",
            " - 113s - loss: 0.6889 - acc: 0.8484\n",
            "Epoch 22/40\n",
            " - 114s - loss: 0.6675 - acc: 0.8517\n",
            "Epoch 23/40\n",
            " - 115s - loss: 0.6557 - acc: 0.8546\n",
            "Epoch 24/40\n",
            " - 115s - loss: 0.6735 - acc: 0.8528\n",
            "Epoch 25/40\n",
            " - 113s - loss: 0.6782 - acc: 0.8527\n",
            "Epoch 26/40\n",
            " - 113s - loss: 0.6347 - acc: 0.8598\n",
            "Epoch 27/40\n",
            " - 114s - loss: 0.6528 - acc: 0.8576\n",
            "Epoch 28/40\n",
            " - 114s - loss: 0.6324 - acc: 0.8621\n",
            "Epoch 29/40\n",
            " - 114s - loss: 0.6207 - acc: 0.8643\n",
            "Epoch 30/40\n",
            " - 114s - loss: 0.6153 - acc: 0.8654\n",
            "Epoch 31/40\n",
            " - 114s - loss: 0.6156 - acc: 0.8663\n",
            "Epoch 32/40\n",
            " - 114s - loss: 0.6019 - acc: 0.8684\n",
            "Epoch 33/40\n",
            " - 115s - loss: 0.5746 - acc: 0.8728\n",
            "Epoch 34/40\n",
            " - 113s - loss: 0.6431 - acc: 0.8642\n",
            "Epoch 35/40\n",
            " - 114s - loss: 0.6040 - acc: 0.8700\n",
            "Epoch 36/40\n",
            " - 114s - loss: 0.5904 - acc: 0.8719\n",
            "Epoch 37/40\n",
            " - 114s - loss: 0.6129 - acc: 0.8698\n",
            "Epoch 38/40\n",
            " - 114s - loss: 0.6101 - acc: 0.8705\n",
            "Epoch 39/40\n",
            " - 113s - loss: 0.5802 - acc: 0.8753\n",
            "Epoch 40/40\n",
            " - 114s - loss: 0.5997 - acc: 0.8731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96f1904438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "xslhrK9sUKPM",
        "colab_type": "code",
        "outputId": "48bc161c-e140-43f7-d98c-a45c027e6a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gutenberg_data/models/02/py_code.json', 'w') as fout:\n",
        "    json.dump({\n",
        "        'chars': ''.join(py_chars),\n",
        "        'char_to_idx': py_char_to_idx,\n",
        "        'chunk_size': 160,\n",
        "    }, fout)\n",
        "py_model.save('/content/gutenberg_data/models/02/py_code.h5')\n",
        "py_model.save_weights('/content/gutenberg_data/models/02/py_code_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S365Jny5rBrF",
        "colab_type": "code",
        "outputId": "664661bf-3555-47ae-b256-527349c49d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "prediction_model = char_rnn_model(None, len(py_chars), num_layers=2, num_nodes=640, dropout=0) \n",
        "prediction_model.load_weights('/content/gutenberg_data/models/02/py_code_weights.h5')\n",
        "prediction_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, 97)          0         \n",
            "_________________________________________________________________\n",
            "lstm_layer_1 (LSTM)          (None, None, 640)         1889280   \n",
            "_________________________________________________________________\n",
            "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
            "_________________________________________________________________\n",
            "time_distributed_21 (TimeDis (None, None, 97)          62177     \n",
            "=================================================================\n",
            "Total params: 5,230,817\n",
            "Trainable params: 5,230,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lKQ9LYD_3ay5",
        "colab_type": "code",
        "outputId": "49b89e4d-3800-4d1e-ec0b-58bdd431f0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3412
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_code(model, start_with='\\ndef ', end_with='\\n\\n', diversity=1.0):\n",
        "    generated = start_with\n",
        "    yield generated\n",
        "    for i in range(2000):\n",
        "        x = np.zeros((1, len(generated), len(py_chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, py_char_to_idx[char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        \n",
        "        preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        next_index = np.argmax(probas)        \n",
        "        next_char = py_chars[next_index]\n",
        "        yield next_char\n",
        "\n",
        "        generated += next_char\n",
        "        if generated.endswith(end_with):\n",
        "            break\n",
        "\n",
        "st = ''\n",
        "for i in range(20):\n",
        "    for ch in generate_code(prediction_model):\n",
        "        sys.stdout.write(ch)\n",
        "        st += ch\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "def itemgee2_array(*xerr = hashlib.m = heap[action]\n",
            "    alias_gc                                       \n",
            "    raise AssertionError(\"MSG\"\n",
            "GE = byte_inde = re.compile(r'[a-zA-M]\")\n",
            "except Impo =errormsg:\n",
            "    standardMset = {}\n",
            "\n",
            "\n",
            "\n",
            "def masterchr - cnf={MSGt\n",
            "    def window(name):\n",
            "        \"\"\"MSG\"\"\"'%s' % iConn and '    SinNodetUs els ' + nazename\n",
            "    else:\n",
            "        names = names.split(\".\")\n",
            "    rematedihten = 1\n",
            "\n",
            "\n",
            "\n",
            "def sav i 2):\n",
            "retur sorted(x for x in x + 1):\n",
            "    rc = sre (int(word))*1000)\n",
            "    return t.lower()\n",
            "\n",
            "\n",
            "\n",
            "def _rmdefault_t prog.1set_pa <Peerhas(p as Ftpcp2, p2c2(peermcos, s, end decode_q.CRED_ERROR)\n",
            "\n",
            "\n",
            "\n",
            "def r <= 1:\n",
            "    reason = b'H' not in mode\n",
            "    file_or_future.cail(result or [] i = is_finalizer(cha modname)\n",
            "fro(q, data):\n",
            "    attr, va_w_items = html.entities()\n",
            "    if default_header_map is not None and cpu_noop():\n",
            "        new_comps.append(tosize)\n",
            "        code = locale_aliaf\n",
            "        loadfile_l = filename\n",
            "    else:\n",
            "        compiler_type = 'mi */x + theye' | th)\n",
            "        target_vexup = ''.join([test+6, test])\n",
            "    d = {\"MimeFolume\"), -tagl, r\"M\n",
            "    ~opcod modeL =r (modelimitrrict + r\"(-.%s%s\" % (x, y)) feetureed = ge .deno = pensize\n",
            "    names . bindingroup_bytes = len(byte_lin\n",
            "    for i                unicode_filterfj[\"escape] = _de except Att = ehlowedError\n",
            "\n",
            "\n",
            "\n",
            "def _c is not None:\n",
            "    _platform_cache.clear()\n",
            "\n",
            "\n",
            "\n",
            "def top():\n",
            "    for tok in w.handle\n",
            "logid() as value:\n",
            "        yield\n",
            "        tup = action for opt, value in options.items()r\\n'\n",
            "  MIMENon = '\\n'\n",
            "n       \n",
            "    else:\n",
            "            item = '\n",
            "\n",
            "\n",
            "\n",
            "def datetime_result(date):\n",
            "    \"\"\"MSG\"\"\"\n",
            "        missing = []\n",
            "        if match:\n",
            "\n",
            "\n",
            "\n",
            "def urlsafe_t S_lsG(url.replace(data, size) + b'\\n')\n",
            "\n",
            "\n",
            "\n",
            "def _list_from_string(s[-1]):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    \n",
            "    \n",
            "    dummy = f\n",
            "    send_down(*possibilities, n, Man=Nonop)\n",
            "    if not deep oo n == 0:ib is not None:\n",
            "      url ==  \n",
            "       args = cookie\n",
            "    reposing   root is not \\\n",
            "          hisname and n == 1 or  'n' in func.__name__\n",
            "                        and s('.(%s)' % \n",
            "                 _new_value' Any <<len(starts) - 1):\n",
            "      or ''\n",
            "        return tuple([_nute)] == e ):\n",
            "      __)\n",
            "   nametpd_type = typecapt\n",
            "    _inter.__Encoding__\"abildentialnamed = 0\n",
            "\n",
            "\n",
            "\n",
            "def class_(*argsByta0, **kw): % (self.currentLineItem,\n",
            "                     self.sca   (len(self.color) + \" \" + command)\n",
            "        self.commands[bu)] = None\n",
            "        return self.undobufferentries(*str(content))\n",
            "\n",
            "\n",
            "\n",
            "def _parse_fla <<=5D(data):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    f.seek(pos)\n",
            "    dfe =r in zip(fdict for f in r_name(body)_i = b''.join(chars)\n",
            "    for dat in data:\n",
            "        i = b'0'*\"\\\\\"\n",
            "        flag = '&'\n",
            "        dumparm = can=os.read(address,swafl_filters)\n",
            "        return buf +\n",
            "            .firstweekday)\n",
            "def ListCompbyx = newparams\n",
            "\n",
            "\n",
            "\n",
            "def splitext(p):\n",
            "    p = os.path.normpath(path)\n",
            "    words = []\n",
            "    ph = bytes(writing)\n",
            "raw_data_menu = ctypes.we Sequence(SMTP)\n",
            "SetProxyType = type([]).vumtext.__no_type_check__()\n",
            "\n",
            "\n",
            "\n",
            "def expr'Mor2ms, None):\n",
            "    \"\"\"MSG\"\"\"\n",
            " templatenamd = \"tuple\" % dict(\n",
            "        ppr for p in groupind long               \n",
            "    return pred\n",
            "\n",
            "\n",
            "\n",
            "def _irsdecoded_words(a, b):\n",
            "    \"MSG\"\n",
            "    a tagged_a()  \n",
            "    return base and b\"\" or nr\n",
            "\n",
            "\n",
            "\n",
            "def  parsed_entities(src, name, path, fallbach=factory=False):\n",
            "    \"\"\"MS'\"({'norm\n",
            "        if s == b'e':\n",
            "  ftper = b'0R;'\n",
            "    else:\n",
            "        raise ValueError(msg % top 'rlue)\n",
            "\n",
            "\n",
            "\n",
            "def swapcolo2aX(name, lst=0):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    \n",
            "    if not condtype.__dict__:\n",
            "        _type(u %s %s\" get   format(text))\n",
            "    return t.\n",
            "\n",
            "\n",
            "\n",
            "def py_match(random, sendmailbox):\n",
            "    \"\"\"MSG\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "def _remove_ugvariate(filename, fname, ctx=None):\n",
            "    \"\"\"MSG\"\"\"\n",
            "    if not isinstlist:Menubutton(MAPversion, \n",
            "    di':  \n",
            "    def tk_ver =0  = \"\"error\"\":\n",
            "        \"\"\"MSG\"\"\"\n",
            "    try:\n",
            "        j = file.read(2)\n",
            "        if len(t) !=                params = buf[148024]\n",
            "    \n",
            " o = -alphanum\n",
            "    reo[-1] = quoted\n",
            "    e = [4*5])\n",
            "    for i not in pat:\n",
            "        return p[:0]\n",
            "    return col\n",
            "\n",
            "\n",
            "\n",
            "def _posixsubprocess_shutdown(\n",
            "    \n",
            "    ):\n",
            "    \n",
            "    \n",
            "    max_workers = {}\n",
            "    stashed = None\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4aXCikNh3ay7",
        "colab_type": "code",
        "outputId": "a36ac25c-954a-49ca-b604-ee2f4f1110d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "flat_model = char_rnn_model(160, len(py_chars), num_layers=1, num_nodes=512, dropout=0)\n",
        "flat_model = tf.contrib.tpu.keras_to_tpu_model(flat_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                              min_delta=0.03,\n",
        "                              patience=3,\n",
        "                              verbose=0, mode='auto')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.65.67.106:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11502498185262799519)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11588889170251908459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15484636259604477500)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6188258309789351709)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17618968535966954990)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1368695216754022451)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 896554020254126233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12401001441761719750)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5652057927253700336)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17090350113839668390)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 17711765655739407088)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/40\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(64,), dtype=tf.int32, name='core_id_70'), TensorSpec(shape=(64, 160, 97), dtype=tf.float32, name='input_190'), TensorSpec(shape=(64, 160, 97), dtype=tf.float32, name='time_distributed_9_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 21.852776288986206 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " - 93s - loss: 3.4410 - acc: 0.3380\n",
            "Epoch 2/40\n",
            " - 51s - loss: 2.0958 - acc: 0.5072\n",
            "Epoch 3/40\n",
            " - 53s - loss: 1.2896 - acc: 0.6872\n",
            "Epoch 4/40\n",
            " - 51s - loss: 1.0748 - acc: 0.7407\n",
            "Epoch 5/40\n",
            " - 52s - loss: 0.9810 - acc: 0.7633\n",
            "Epoch 6/40\n",
            " - 51s - loss: 0.9108 - acc: 0.7794\n",
            "Epoch 7/40\n",
            " - 52s - loss: 0.9775 - acc: 0.7741\n",
            "Epoch 8/40\n",
            " - 52s - loss: 0.8688 - acc: 0.7925\n",
            "Epoch 9/40\n",
            " - 51s - loss: 0.8615 - acc: 0.7959\n",
            "Epoch 10/40\n",
            " - 52s - loss: 0.8670 - acc: 0.7973\n",
            "Epoch 11/40\n",
            " - 51s - loss: 0.8627 - acc: 0.7993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96e767fcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "qYPG1iuyooEP",
        "colab_type": "code",
        "outputId": "da923fb3-17e4-4ac2-e32f-37dbfc040f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "cell_type": "code",
      "source": [
        "flat_model.fit_generator(\n",
        "    data_generator(python_code, py_char_to_idx, batch_size=BATCH_SIZE, chunk_size=160),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=int(2 * len(python_code) / (BATCH_SIZE * 160)),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " - 53s - loss: 0.8566 - acc: 0.8016\n",
            "Epoch 2/40\n",
            " - 52s - loss: 0.8543 - acc: 0.8025\n",
            "Epoch 3/40\n",
            " - 52s - loss: 0.8442 - acc: 0.8046\n",
            "Epoch 4/40\n",
            " - 52s - loss: 0.8321 - acc: 0.8079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96e89b8940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "Nt_fTNfv3ay9",
        "colab_type": "code",
        "outputId": "24931d84-2520-4c9a-ed01-c74cca2cd003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "example_code = 'if a == 2:\\n    b=1\\nelse:\\n    b=2\\n'\n",
        "#example_code = 'a=(2 * 3)\\nb=(4 * 6 + 7)\\nreturn C'\n",
        "\n",
        "def activations(model, code):\n",
        "    x = np.zeros((1, len(code), len(py_char_to_idx)))\n",
        "    for t, char in enumerate(code):\n",
        "        x[0, t, py_char_to_idx[char]] = 1.\n",
        "    output = model.get_layer('lstm_layer_1').output\n",
        "    f = K.function([model.input], [output])\n",
        "    return f([x])[0][0]\n",
        "\n",
        "act = activations(flat_model, example_code)\n",
        "act.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "metadata": {
        "id": "oUXD8JDI3ay-",
        "colab_type": "code",
        "outputId": "b5e5c86e-62b9-4b58-febf-d6b33a6d0c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def interesting_neurons(act):\n",
        "    res = []\n",
        "    for n in np.argmax(act, axis=1):\n",
        "        if not n in res:\n",
        "            res.append(n)\n",
        "    return res\n",
        "\n",
        "neurons = interesting_neurons(act)\n",
        "len(neurons)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "metadata": {
        "id": "Mh2pwk-Z3azA",
        "colab_type": "code",
        "outputId": "a7e79b85-4fff-4bb1-f286-e7e04f4eaf39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "def visualize_neurons(neurons, code, act, cell_size=12):\n",
        "    img = np.full((len(neurons) + 1, len(code), 3), 128)\n",
        "    scores = (act[:, neurons].T + 1) / 2\n",
        "    img[1:, :, 0] = 255 * (1 - scores)\n",
        "    img[1:, :, 1] = 255 * scores\n",
        "\n",
        "    f = BytesIO()\n",
        "    img = scipy.misc.imresize(img, float(cell_size), interp='nearest')\n",
        "    pil_img = PIL.Image.fromarray(img)\n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    for idx, ch in enumerate(code):\n",
        "        draw.text((idx * cell_size + 2, 0), ch)\n",
        "    pil_img.save(f, 'png')\n",
        "    return Image(data=f.getvalue())\n",
        "\n",
        "img = visualize_neurons(neurons, example_code, act)\n",
        "display(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAADYCAIAAAB3M0NIAAAHoElEQVR4nO3d0VEcVxQEUHBtCigI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\n/0YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BR6lL2qY3azC",
        "colab_type": "code",
        "outputId": "119d5ff8-cc9e-4349-aef8-8b8b76db8934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "def image_for_code(code):\n",
        "    act = activations(flat_model, code)\n",
        "    neurons = interesting_neurons(act)\n",
        "    return visualize_neurons(neurons, code, act)\n",
        "\n",
        "display(image_for_code('if (a == 2) and ((b == 1) or (c==2)):'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAD8CAIAAACD/YZQAAAJgklEQVR4nO3d3VUc1xoEUPCaFFAQ\nEAQk4SQmHwjiOolREBAESsIPWpeFZJuu0nw66tHa+1WH/jvdpTZy9bl+fHy8AiDzx68+AIBL8k1o\nHo/H4/H48Q8c/+/8fY9sZFeSi7PDs148px+MmTqM9S5x3kP7OfKp5+vMM/omNJ+enp6enjZ3tjks\ncTwez9/IrrxdnKsPZ+Xp6Wk/d+FXUxORzOlvOe/JhO5w3hP7ma/B5+vMuTi8P6C3Lf7r0Lcx74/+\nx1R/G0ztaPO8ztxX9bPn34ub1+frLjYna+pJHvwbfvYGWzPv+aktmPerb3Phh/c48pxOzcX48/XD\nc3F4f0AfX6PkCZzyfuLPuc+SH5w9nTWXKLw+b7n5X2P++YDtwSXO+xrJvL/96dXPn9nweDa3k8/F\n+gj65x8dfva+f8z7iT/HyjeOq4UzOnV9fleL532fvp7UmlML/1PmzLlY9nx9vJedhubU35Mr3zi+\nO+afOrX7fEPcj9/1TXO3Prjnp+Zi5fP1sT/eDuj9ryz/9VHcHJALT/i736JehPBQR6b8/Ovz3a9l\nzrnOgzn1y5+KSvLsvDnzpN7PV/hvD+fv7szjmTX4fG3+o/d/7ev6F/7P7ef/UvwS/d5n/VN/AX/R\nLvesL+vIF9yBvzI0AS6ORhBA4XC6Wbezu5ftMTev22O+fNoe8xqc15eF5z61r9vgGn76MrOv5Bre\nPW+PuQmOJ7k+z3fbYxLJvpIxyVxc4r4ePm+PSZ7TZL6mnovo3IPcSHjTBCgITYCC0AQoCE2AgtAE\nKAhNgILQBCgITYCC0AQoCE2AgtAEKEQfIZ7qhyZd5mRMIulfJ73pqb7q1Hn9rpJ++lTPPfES9KZX\nfidh5f18G2wn8XCa2c5Ur3zqOnvTBCgITYCC0AQoCE2AgtAEKAhNgILQBCgITYCC0AQoCE2AgtAE\nKByStcifb7fHTPXTk/5ssp5ydMz3wZiFXd2p9a+Tc08kx5N0daf64InTw/aYlWuRJ5J9vQRzmqxX\nntwbSac+mdOkv7/ymwzJvZrwpglQEJoABaEJUBCaAAWhCVAQmgAFoQlQEJoABaEJUBCaAAWhCVC4\n/vN/jyMbmlqbOOm97s1Ut3qqv59YuT74lL3dGyvnfWV/f2rN9ylJx3/ls+NNE6AgNAEKQhOgIDQB\nCkIToCA0AQpCE6AgNAEKQhOgIDQBCkIToBCte54YW2s76KcnptZu3lsffOp4VnbGE8l8TX27YOo6\nr+yDJ8bOK7iGU2u+J5L1yqfWNE940wQoCE2AgtAEKAhNgILQBCgITYCC0AQoCE2AgtAEKAhNgILQ\nBCgcVu5sbJ3ooX76Sit75cm+9tabnrLy3Kf6zlPznmxnal8ru94rvQbXx5smQEFoAhSEJkBBaAIU\nhCZAQWgCFIQmQEFoAhSEJkBBaAIUhCZA4ZB0LadE3dihXnmynbE11ne2fvpKU3N6+3z+sVxd7W89\n95VWfk/gdL89Zur5mro3EnfBeu7eNAEKQhOgIDQBCkIToCA0AQpCE6AgNAEKQhOgIDQBCkIToCA0\nAQqHu6DXGfW4gw7ybdDr/PR5e8zz7czxJOcVdfODvvP9aXvMS3Be0TUc6iAn1zk5nqtkTGCqv5/M\n6dS+kt50sq+p7xtM7WvK1POVXOekV37zuj3GmyZAQWgCFIQmQEFoAhSEJkBBaAIUhCZAQWgCFIQm\nQEFoAhSEJkDhMLWhqb7qynXYpyTH/NefM/uaWkt6paTPu7LvnJg6nql12JN5T67z1P2z8j5Mnq/X\nh+0xp2BMwpsmQEFoAhSEJkBBaAIUhCZAQWgCFIQmQEFoAhSEJkBBaAIUhCZAIeqeT60Tnaw7PNU9\nv8Q11qes7O8/nLbHTM17cp2T9eWTdeH3Nu9Jrzy5578E557MVyI55sRz0N9fuea7N02AgtAEKAhN\ngILQBCgITYCC0AQoCE2AgtAEKAhNgILQBCgITYDCIel1rrSyQzplqvM71WVOutVT/fTkmKf2tXJO\n97YOe2Lq2UnmK7nHEsl1Tr5vkKwvP3Ve3jQBCkIToCA0AQpCE6AgNAEKQhOgIDQBCkIToCA0AQpC\nE6AgNAEK0brnUz3TZC3pKVM93GRM0nu9fQ72tXBt9Kl+evLtgpXfN0jOK1mPO5mLZE6T7xK8JGus\nD90byXb+Gpqvu+D6JN3zqW9NTH1PwJsmQEFoAhSEJkBBaAIUhCZAQWgCFIQmQEFoAhSEJkBBaAIU\nhCZA4ZD0OhNTPe6kq5uY6pkmx3x/mtlX0mWe6uomffmV66dfotPD9pikf530wZN7bOpZnurmT5nK\nhERyP3vTBCgITYCC0AQoCE2AgtAEKAhNgILQBCgITYCC0AQoCE2AgtAEKFw/Hh9HNpR0UZNe51Tf\neWzd8+C8kq7uVGd8ZQ935TVcaaovn9yr0Xr3U53xhd9bmDJ1b0x9tyHhTROgIDQBCkIToCA0AQpC\nE6AgNAEKQhOgIDQBCkIToCA0AQpCE6BwSAYlnc3nYB3tva2xPiW5Pit7wSvXRp+SdIenjmfq+wbJ\nmJuF3zdIrs/e1qmf6u/fBd9kmFrP3ZsmQEFoAhSEJkBBaAIUhCZAQWgCFIQmQEFoAhSEJkBBaAIU\nhCZA4ZB0LVf2yqdMdb0TK3vcU+toR335ofXcE1F3eKhTn5jqjCdjVvbBk653Mu+3wXytzITT/cx2\nkvPypglQEJoABaEJUBCaAAWhCVAQmgAFoQlQEJoABaEJUBCaAAWhCVCI1j2/C/rOl9hPn5L0i8f2\nNdWtHlqrfep4ku7w1Prgial9Td0byTcHEsk3EKJO/c6+7TB1nT9b9xxgltAEKAhNgILQBCgITYCC\n0AQoCE2AgtAEKAhNgILQBCgITYDCYWpt66l+emKqEz3W4x7qIK/sTScd20vsVidWfisgkaxFnnTG\nk2uYjJl6Li7xWxPJ9fGmCVAQmgAFoQlQEJoABaEJUBCaAAWhCVAQmgAFoQlQEJoABaEJUIjWPZ9a\ndzjZzt3L9pjXoV75VAc56Wjfn2b2lfSUp74nkPSdk7l4uZ05nsTKvnNy/6zsek/NVyL51sTenq+p\nY/amCVAQmgAFoQlQEJoABaEJUBCaAAWhCVAQmgAFoQlQEJoABaEJULh+PD5uDppaZ3xKsn76ynXP\nV66jvXIt8sRUz33ltwKmrOyVJ6b6+1P7Sr45kJia96R3n3zbwZsmQEFoAhSEJkBBaAIUhCZAQWgC\nFIQmQEFoAhSEJkBBaAIUhCZA4W9jZlhdg2TypgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HWcagIrR3azF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "code = 'if (a == 2) and ((b == 1) or (c==2)):'\n",
        "mask = '   ________     ____________________ '\n",
        "act = activations(flat_model, code)\n",
        "positive = [idx for idx, ch in enumerate(mask) if ch == '_']\n",
        "negative = [idx for idx, ch in enumerate(mask) if ch != '_']\n",
        "\n",
        "neurons = np.argsort(act[positive].sum(axis=0) - act[negative].sum(axis=0))[-5:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SO9Uey363azG",
        "colab_type": "code",
        "outputId": "8dae4443-31a5-4619-a40a-bea5c3ebca5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "img = visualize_neurons(neurons, code, act)\n",
        "display(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAABICAIAAAACptczAAADoUlEQVR4nO3d21HcQBAF0IUiCUiD\nNHAaygmlYdIgDUjDH5S3ltfqNmoGiTrnl0HzaM1FXjzo4v7+/gBA5vKnBwCwJ69Cc5qmaZrOf8P0\n3/q+Wy6yKcnibHDWg2t6pk3XMMbbY91D2xl51/5aOaNXoTnP8zzPi50tNktM07T+IptyXJzD2arM\n87ydu/BFVyGSmv7KuicF3WDdE9upV+P+WlmLq9MBHa/4YdNjm9PRf03pp0FXR4vzWtlX6XvX34uL\n6/PSxWKxunZy40/43htsTN3zqQ2o++F1Lny5x5Z92lWL9v315VpcnQ7o/BolO7DLaeHX3GfJN/ZO\nZ8wShetzzM3P2rzfYFuwx7qPkdT9+NXD91c2HM/idfJajI+g91+6+u6+v+a08GuMfOI4DKxo1/r8\nVoPrvk0vkxoztfCfMitrMWx/ne9lo6HZ9XNy5BPHmzF/a2m3+YS4Hb/1SXOzztzzXbUYub/OuzwO\n6PQjyw+34mKDXDjhN5+i7kI41JaSr1+fNx/LrFnnxpz68V1Rkuydo5WTOq1X+LuH9d2tHE+vxv21\n+Evvz/q6+MH/3L7+Q/E9+t2z/tYP4Hdtv7Pe18gH3IE/GZoAu+NEEEDB1eFxudHzTU9nT9f76+v6\nqec6d397rpPM6+Z5uc3j7XKbZO5duuqe6FrD5Dp/HpbbJEbez11G1vQ2yLGkTbIvPGkCFAhNgAKh\nCVAgNAEKhCZAgdAEKBCaAAVCE6BAaAIUCE2AAqEJUBD9EeKu89eJ5Hxocg63y8jz6cm8kvVJ3A0c\n88gzyF26zqd39ZXouje6xjMyNxLJvJIxe9IEKBCaAAVCE6BAaAIUCE2AAqEJUCA0AQqEJkCB0AQo\nEJoABUIToCA6e951BjnRdZZ55HWSNk3Hgnd5jjs5z9v1nvGtrU/yHu2RRv4thURbbtz1XCfhSROg\nQGgCFAhNgAKhCVAgNAEKhCZAgdAEKBCaAAVCE6BAaAIUCE2Agrb3nieSc7hbe3dzciY6WZ+ueXWN\nJ7lOUq/o3H1wna57bGuSunfdqyP/lkIyr2Q8I/dX15g9aQIUCE2AAqEJUCA0AQqEJkCB0AQoEJoA\nBUIToEBoAhQITYACoQlQEJ097zqnnBj5buuu9zsn65NI+oreWx2s4UNwnZHv7H4eeI8lus6DJ7Xo\nehf5SF21SPZOV5sunjQBCoQmQIHQBCgQmgAFQhOgQGgCFAhNgAKhCVAgNAEKhCZAgdAEKPgH5Wr9\nFRnfEToAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jkkrYn1O3azI",
        "colab_type": "code",
        "outputId": "d1310151-a1fc-4fe1-fa02-4567bca7262f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "neurons"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([239, 247,  18,   7,   8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "ty2yBySh3azK",
        "colab_type": "code",
        "outputId": "0b60c740-8a81-412b-8e87-ed6ac9b5615a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "act[negative, 108].sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06521046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "vwu-uQFW3azM",
        "colab_type": "code",
        "outputId": "91137e53-b395-45c2-9733-b87601d69df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x0 = 0\n",
        "x1 = 0\n",
        "for idx, ch in enumerate(mask):\n",
        "    if ch == '_':\n",
        "        x0 += act[idx, 108]\n",
        "    else:\n",
        "        x1 += act[idx, 108]\n",
        "x0, x1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19161548878764734, 0.06521046429406852)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "metadata": {
        "id": "kft-cxL23azP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}